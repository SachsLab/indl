{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Recurrent Layers\n",
    "\n",
    "A bit of a mess. Also check out tfp_utils, lfads_utils, and lfads_complex_cell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GenerativeRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IN = 8\n",
    "N_UNITS = 24\n",
    "N_OUT_TIMESTEPS = 115\n",
    "cell = tfkl.GRUCell  # LSTMCell or GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "#  Test regular RNN with zeros input\n",
    "reg_rnn_layer = tfkl.RNN(cell(N_UNITS), return_state=True, return_sequences=True)\n",
    "in_ = tf.zeros((1, 115, 16))\n",
    "x_ = reg_rnn_layer(in_)\n",
    "print(K.any(x_[0]))  # Just to remind myself that input zeros and state zeros will yield output zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test placeholder tensor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1, 8)]       0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 8)]          0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ZerosLike (TensorFl [(None, 8)]          0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1, 8)]       0           tf_op_layer_ZerosLike[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 8)]     0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 115, 8)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "generative_rnn (GenerativeRNN)  [(None, 115, 24), (N 2448        tf_op_layer_concat[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,448\n",
      "Trainable params: 2,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test placeholder tensor with no timesteps\n",
    "K.clear_session()\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS)\n",
    "in_ = tfkl.Input(shape=(N_IN,))\n",
    "x_, cell_state_ = gen_rnn_layer(in_)\n",
    "print(\"Test placeholder tensor\")\n",
    "model = tf.keras.Model(inputs=in_, outputs=x_)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test placeholder tensor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None,)]            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1)]          0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ZerosLike (TensorFl [(None, 1)]          0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1, 1)]       0           tf_op_layer_ZerosLike[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 1)]          0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ZerosLike_1 (Tensor [(None, 1)]          0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 1, 1)]       0           tf_op_layer_ZerosLike_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 1)]     0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 115, 1)]     0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "generative_rnn (GenerativeRNN)  [(None, 115, 24), (N 1944        tf_op_layer_concat[0][0]         \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,944\n",
      "Trainable params: 1,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test placeholder tensor with no timesteps as initial state\n",
    "K.clear_session()\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS)\n",
    "in_ = tfkl.Input(shape=(N_UNITS,))\n",
    "x_, cell_state_ = gen_rnn_layer(None, initial_state=in_)\n",
    "print(\"Test placeholder tensor\")\n",
    "model = tf.keras.Model(inputs=in_, outputs=x_)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test None input\n",
      "(1, 115, 24) (1, 24)\n",
      "tf.Tensor(False, shape=(), dtype=bool) tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Test None input --> uses zeros\n",
    "K.clear_session()\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS)\n",
    "print(\"Test None input\")\n",
    "x_, cell_state_ = gen_rnn_layer()\n",
    "print(x_.shape, cell_state_.shape)\n",
    "print(K.any(x_), K.any(cell_state_))  # <- any non-zero values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test zeros input\n",
      "(1, 115, 24) (1, 24)\n",
      "tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Test random input\n",
    "K.clear_session()\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS)\n",
    "in_ = tf.random.uniform((1, 8, N_UNITS), minval=-1.0, maxval=1.0)\n",
    "print(\"Test zeros input\")\n",
    "x_, cell_state_ = gen_rnn_layer(in_)\n",
    "print(x_.shape, cell_state_.shape)\n",
    "print(K.any(x_), K.any(cell_state_))  # <- any non-zero values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([None, 115, 24]), TensorShape([None, 24])]\n",
      "(1, 115, 24) (1, 24)\n",
      "tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Test random states\n",
    "K.clear_session()\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS)\n",
    "print(gen_rnn_layer.compute_output_shape())\n",
    "init_states = [tf.random.uniform((1, N_UNITS), minval=-1.0, maxval=1.0) for _ in range(1)]\n",
    "x_, cell_states_ = gen_rnn_layer(initial_state=init_states)\n",
    "print(x_.shape, cell_state_.shape)\n",
    "print(K.any(x_), K.any(cell_state_))  # <- any non-zero values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 115, 24) (5, 24)\n",
      "tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Test masking\n",
    "K.clear_session()\n",
    "\n",
    "tmp = tf.range(N_OUT_TIMESTEPS)[tf.newaxis, :, tf.newaxis]\n",
    "mask = tf.math.logical_or(tmp < 5, tmp > 100)\n",
    "gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True,\n",
    "                              timesteps=N_OUT_TIMESTEPS, tile_input=True)\n",
    "in_ = tf.random.uniform((5, N_OUT_TIMESTEPS, N_UNITS), minval=-1.0, maxval=1.0)\n",
    "x_, cell_state_ = gen_rnn_layer(in_, mask=mask)\n",
    "print(x_.shape, cell_state_.shape)\n",
    "print(K.any(x_), K.any(cell_state_))  # <- any non-zero values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbage code I don't want to throw out yet.\n",
    "if False:\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n",
    "        assert(mask is None), \"mask not supported.\"\n",
    "        # First part copied from super call()\n",
    "        \n",
    "        # The input should be dense, padded with zeros. If a ragged input is fed\n",
    "        # into the layer, it is padded and the row lengths are used for masking.\n",
    "        inputs, row_lengths = K.convert_inputs_if_ragged(inputs)\n",
    "        is_ragged_input = (row_lengths is not None)\n",
    "        self._validate_args_if_ragged(is_ragged_input, mask)\n",
    "\n",
    "        # Get initial_state. Merge provided initial_state and preserved if self.stateful,\n",
    "        # otherwise use provided or zeros if provided is None.\n",
    "        inputs, initial_state, constants = self._process_inputs(\n",
    "            inputs, initial_state, constants)\n",
    "\n",
    "        self._maybe_reset_cell_dropout_mask(self.cell)\n",
    "        if isinstance(self.cell, tfkl.StackedRNNCells):\n",
    "            for cell in self.cell.cells:\n",
    "                self._maybe_reset_cell_dropout_mask(cell)\n",
    "\n",
    "        kwargs = {}\n",
    "        if generic_utils.has_arg(self.cell.call, 'training'):\n",
    "            kwargs['training'] = training\n",
    "\n",
    "        # TF RNN cells expect single tensor as state instead of list wrapped tensor.\n",
    "        is_tf_rnn_cell = getattr(self.cell, '_is_tf_rnn_cell', None) is not None\n",
    "        if constants:\n",
    "            if not generic_utils.has_arg(self.cell.call, 'constants'):\n",
    "                raise ValueError('RNN cell does not support constants')\n",
    "\n",
    "            def step(inputs, states):\n",
    "                constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n",
    "                states = states[:-self._num_constants]  # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "                states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n",
    "                output, new_states = self.cell.call(\n",
    "                    inputs, states, constants=constants, **kwargs)\n",
    "                if not nest.is_sequence(new_states):\n",
    "                    new_states = [new_states]\n",
    "                return output, new_states\n",
    "        else:\n",
    "\n",
    "            def step(inputs, states):\n",
    "                states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n",
    "                output, new_states = self.cell.call(inputs, states, **kwargs)\n",
    "                if not nest.is_sequence(new_states):\n",
    "                    new_states = [new_states]\n",
    "                return output, new_states\n",
    "\n",
    "        # Begin deviation from super call() #\n",
    "        #####################################\n",
    "        # We do not do K.rnn because it does not support feeding the output back as the input to the next step.\n",
    "        def _process_single_input_t(input_t):\n",
    "            input_t = tf.unstack(input_t, axis=-2)  # unstack for time_step dim\n",
    "            if self.go_backwards:\n",
    "                input_t.reverse()\n",
    "            return input_t\n",
    "\n",
    "        if nest.is_sequence(inputs):\n",
    "            processed_input = nest.map_structure(_process_single_input_t, inputs)\n",
    "        else:\n",
    "            processed_input = (_process_single_input_t(inputs),)\n",
    "        cell_input = nest.pack_sequence_as(inputs, [_[0] for _ in processed_input])\n",
    "            \n",
    "        cell_state = tuple(initial_state)\n",
    "        \n",
    "        out_states = []\n",
    "        out_inputs = []\n",
    "        for step_ix in range(self.timesteps):\n",
    "            cell_input, new_states = step(cell_input, cell_state)\n",
    "            flat_new_states = nest.flatten(new_states)\n",
    "            cell_state = nest.pack_sequence_as(cell_state, flat_new_states)\n",
    "            out_states.append(cell_state)\n",
    "            out_inputs.append(cell_input)\n",
    "\n",
    "        out_inputs = tf.stack(out_inputs, axis=-2)\n",
    "        # if cell outputs a distribution, then we might do the following, but base class\n",
    "        # would have to change.\n",
    "        if False:\n",
    "            if hasattr(out_inputs[0], 'parameters') and 'distribution' in out_inputs[0].parameters:\n",
    "                dist0_parms = out_inputs[0].parameters['distribution'].parameters\n",
    "                coll_parms = {}\n",
    "                for p_name, p_val in dist0_parms.items():\n",
    "                    if K.tensor_util.is_tensor(p_val):\n",
    "                        coll_parms[p_name] = []\n",
    "                for dist in out_inputs:\n",
    "                    for p_name in coll_parms.keys():\n",
    "                        coll_parms[p_name].append(dist.parameters['distribution'].parameters[p_name])\n",
    "                for p_name in coll_parms.keys():\n",
    "                    coll_parms[p_name] = tf.stack(coll_parms[p_name], axis=-2)\n",
    "                dist_class = out_inputs[0].parameters['distribution'].__class__\n",
    "                out_inputs = dist_class(**coll_parms)\n",
    "                # Warning! time dimension lost in batch with None\n",
    "                out_inputs = tfp.distributions.Independent(out_inputs, reinterpreted_batch_ndims=1)\n",
    "        \n",
    "        out_states = tf.stack(out_states, axis=-2)\n",
    "        out_states = tf.unstack(out_states, axis=0)\n",
    "        if not hasattr(self.cell.state_size, '__len__'):\n",
    "            out_states = out_states[0]\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            out_inputs = out_inputs[..., -1, :]\n",
    "            out_states = [_[..., -1, :] for _ in out_states] if isinstance(out_states, list) else out_states[..., -1, :]\n",
    "        if self.return_state:\n",
    "            return out_inputs, out_states\n",
    "        return out_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}