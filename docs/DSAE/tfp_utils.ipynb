{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow Probability Utilities\n",
    "\n",
    "This notebook is a bit of a mess after the refactor.\n",
    "Its code has all been moved to `indl.model.tfp` and `indl.model.tfp.dsae`\n",
    "\n",
    "Many of the tests have been moved to the unit tests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfb = tfp.bijectors\n",
    "scale_shift = np.log(np.exp(1) - 1).astype(np.float32)\n",
    "from indl.model.tfp.devae import *\n",
    "from indl.model.tfp import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how this would work in a variational autoencoder.\n",
    "N_TIMES = 10\n",
    "N_SENSORS = 8\n",
    "N_SAMPLES = 2\n",
    "N_HIDDEN = 5\n",
    "KL_WEIGHT = 0.05\n",
    "\n",
    "t_vec = tf.range(N_TIMES, dtype=tf.float32) / N_TIMES\n",
    "sig_vec = 1 + tf.exp(-10*(t_vec - 0.5))\n",
    "\n",
    "\n",
    "def make_model(prior):\n",
    "    input_ = tfkl.Input(shape=(LATENT_SIZE,))\n",
    "\n",
    "    # Encoder\n",
    "    make_latent_dist_fn, latent_params = make_mvn_dist_fn(\n",
    "        input_, LATENT_SIZE, offdiag=True, loc_name=\"latent_loc\")\n",
    "    q_latent = tfpl.DistributionLambda(\n",
    "        name=\"q_latent\",\n",
    "        make_distribution_fn=make_latent_dist_fn,\n",
    "        convert_to_tensor_fn=lambda s: s.sample(N_SAMPLES),\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior,\n",
    "                                                          use_exact_kl=True,\n",
    "                                                          weight=KL_WEIGHT)\n",
    "    )(latent_params)\n",
    "\n",
    "    # Decoder\n",
    "    y_ = q_latent[..., tf.newaxis, :] / sig_vec[:, tf.newaxis]\n",
    "    # broadcast-add zeros to restore timesteps\n",
    "    #y_ = q_latent[..., tf.newaxis, :] + tf.zeros([N_TIMES, 1])\n",
    "    #y_ = tf.reshape(y_, [-1, N_TIMES, LATENT_SIZE])\n",
    "    #y_ = tfkl.LSTM(N_HIDDEN, return_sequences=True)(y_)\n",
    "    #y_ = tf.reshape(y_, [N_SAMPLES, -1, N_TIMES, N_HIDDEN])\n",
    "    make_out_dist_fn, out_dist_params = make_mvn_dist_fn(y_, N_SENSORS, loc_name=\"out_loc\")\n",
    "    p_out = tfpl.DistributionLambda(\n",
    "        make_distribution_fn=make_out_dist_fn, name=\"p_out\")(out_dist_params)\n",
    "    # no prior on the output.\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Model(inputs=input_, outputs=[q_latent, p_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake dataset to train the model.\n",
    "LATENT_SIZE = 4\n",
    "BATCH_SIZE = 6\n",
    "# The latents are sampled from a distribution with known parameters.\n",
    "true_dist = tfd.MultivariateNormalDiag(\n",
    "    loc=[-1., 1., 5, -5],  # must have length == LATENT_SIZE\n",
    "    scale_diag=[0.5, 0.5, 0.9, 0.2]\n",
    ")\n",
    "# They parameterize sigmoid end points,\n",
    "from indl.misc.sigfuncs import sigmoid\n",
    "from functools import partial\n",
    "t_vec = (np.arange(N_TIMES, dtype=np.float32) / N_TIMES)[None, :]\n",
    "f_sig = partial(sigmoid, t_vec, B=10, x_offset=0.5)\n",
    "# which are then mixed with a known mixing matrix\n",
    "mix_mat = np.array([\n",
    "    [-0.3, -.28, -0.38, -0.45, -0.02, -0.12, -0.05, -0.48],\n",
    "    [0.27, 0.29, -0.34, 0.2, 0.41, 0.08, 0.11, 0.13],\n",
    "    [-0.14, 0.26, -0.28, -0.14, 0.1, -0.2, 0.4, 0.11],\n",
    "    [-0.05, -0.12, 0.28, 0.49, -0.12, 0.1, 0.17, 0.22]\n",
    "], dtype=np.float32).T\n",
    "#mix_mat = tf.convert_to_tensor(mix_mat)\n",
    "\n",
    "\n",
    "def gen_ds(n_iters=1e2, latent_size=LATENT_SIZE):\n",
    "    iter_ix = 0\n",
    "    while iter_ix < n_iters:\n",
    "        _input = tf.ones((latent_size,), dtype=tf.float32)\n",
    "        latent = true_dist.sample().numpy()\n",
    "        _y = np.reshape(latent, [latent_size, 1])\n",
    "        _y = f_sig(K=_y)\n",
    "        _y = mix_mat @ _y\n",
    "        _y = _y.T\n",
    "        yield _input, _y\n",
    "        iter_ix += 1\n",
    "\n",
    "        \n",
    "ds = tf.data.Dataset.from_generator(gen_ds, args=[1e2], output_types=(tf.float32, tf.float32),\n",
    "                                    output_shapes=((LATENT_SIZE,), (N_TIMES, N_SENSORS)))\n",
    "ds = ds.map(lambda x, y: (x, (tf.zeros(0, dtype=tf.float32), y))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 - 0s - loss: 91.9389 - q_latent_loss: 4.6523 - p_out_loss: 90.5613\n",
      "Epoch 2/100\n",
      "17/17 - 0s - loss: 11531.2471 - q_latent_loss: 4.3536 - p_out_loss: 11529.9590\n",
      "Epoch 3/100\n",
      "17/17 - 0s - loss: 743.5315 - q_latent_loss: 4.1376 - p_out_loss: 742.3065\n",
      "Epoch 4/100\n",
      "17/17 - 0s - loss: 137048.2969 - q_latent_loss: 3.9767 - p_out_loss: 137047.1250\n",
      "Epoch 5/100\n",
      "17/17 - 0s - loss: 113.2025 - q_latent_loss: 3.8272 - p_out_loss: 112.0695\n",
      "Epoch 6/100\n",
      "17/17 - 0s - loss: 74.9116 - q_latent_loss: 3.7352 - p_out_loss: 73.8058\n",
      "Epoch 7/100\n",
      "17/17 - 0s - loss: 79.9207 - q_latent_loss: 3.6573 - p_out_loss: 78.8380\n",
      "Epoch 8/100\n",
      "17/17 - 0s - loss: 372.0323 - q_latent_loss: 3.5850 - p_out_loss: 370.9711\n",
      "Epoch 9/100\n",
      "17/17 - 0s - loss: 60.3690 - q_latent_loss: 3.5167 - p_out_loss: 59.3279\n",
      "Epoch 10/100\n",
      "17/17 - 0s - loss: 7418.3252 - q_latent_loss: 3.4512 - p_out_loss: 7417.3027\n",
      "Epoch 11/100\n",
      "17/17 - 0s - loss: 3513.5659 - q_latent_loss: 3.3824 - p_out_loss: 3512.5649\n",
      "Epoch 12/100\n",
      "17/17 - 0s - loss: 53.7796 - q_latent_loss: 3.3220 - p_out_loss: 52.7962\n",
      "Epoch 13/100\n",
      "17/17 - 0s - loss: 20.4849 - q_latent_loss: 3.2672 - p_out_loss: 19.5177\n",
      "Epoch 14/100\n",
      "17/17 - 0s - loss: 1562.3738 - q_latent_loss: 3.2157 - p_out_loss: 1561.4218\n",
      "Epoch 15/100\n",
      "17/17 - 0s - loss: 963.6710 - q_latent_loss: 3.1652 - p_out_loss: 962.7339\n",
      "Epoch 16/100\n",
      "17/17 - 0s - loss: 2838.6292 - q_latent_loss: 3.1165 - p_out_loss: 2837.7068\n",
      "Epoch 17/100\n",
      "17/17 - 0s - loss: 35.5000 - q_latent_loss: 3.0708 - p_out_loss: 34.5910\n",
      "Epoch 18/100\n",
      "17/17 - 0s - loss: 706.3328 - q_latent_loss: 3.0287 - p_out_loss: 705.4363\n",
      "Epoch 19/100\n",
      "17/17 - 0s - loss: 52.2510 - q_latent_loss: 2.9887 - p_out_loss: 51.3663\n",
      "Epoch 20/100\n",
      "17/17 - 0s - loss: 7950.6792 - q_latent_loss: 2.9512 - p_out_loss: 7949.8062\n",
      "Epoch 21/100\n",
      "17/17 - 0s - loss: 190.5290 - q_latent_loss: 2.9112 - p_out_loss: 189.6672\n",
      "Epoch 22/100\n",
      "17/17 - 0s - loss: 27.1404 - q_latent_loss: 2.8756 - p_out_loss: 26.2891\n",
      "Epoch 23/100\n",
      "17/17 - 0s - loss: 222.1122 - q_latent_loss: 2.8434 - p_out_loss: 221.2705\n",
      "Epoch 24/100\n",
      "17/17 - 0s - loss: 26.6817 - q_latent_loss: 2.8132 - p_out_loss: 25.8489\n",
      "Epoch 25/100\n",
      "17/17 - 0s - loss: 301.0465 - q_latent_loss: 2.7844 - p_out_loss: 300.2223\n",
      "Epoch 26/100\n",
      "17/17 - 0s - loss: 17.3472 - q_latent_loss: 2.7568 - p_out_loss: 16.5312\n",
      "Epoch 27/100\n",
      "17/17 - 0s - loss: 47.1069 - q_latent_loss: 2.7306 - p_out_loss: 46.2986\n",
      "Epoch 28/100\n",
      "17/17 - 0s - loss: 34.0318 - q_latent_loss: 2.7057 - p_out_loss: 33.2309\n",
      "Epoch 29/100\n",
      "17/17 - 0s - loss: 25.4790 - q_latent_loss: 2.6819 - p_out_loss: 24.6851\n",
      "Epoch 30/100\n",
      "17/17 - 0s - loss: 35.2237 - q_latent_loss: 2.6592 - p_out_loss: 34.4365\n",
      "Epoch 31/100\n",
      "17/17 - 0s - loss: 17.7497 - q_latent_loss: 2.6375 - p_out_loss: 16.9690\n",
      "Epoch 32/100\n",
      "17/17 - 0s - loss: 19.0096 - q_latent_loss: 2.6168 - p_out_loss: 18.2350\n",
      "Epoch 33/100\n",
      "17/17 - 0s - loss: 17.3889 - q_latent_loss: 2.5969 - p_out_loss: 16.6202\n",
      "Epoch 34/100\n",
      "17/17 - 0s - loss: 3164.2163 - q_latent_loss: 2.5783 - p_out_loss: 3163.4539\n",
      "Epoch 35/100\n",
      "17/17 - 0s - loss: 318.1325 - q_latent_loss: 2.5614 - p_out_loss: 317.3743\n",
      "Epoch 36/100\n",
      "17/17 - 0s - loss: 496.7764 - q_latent_loss: 2.5442 - p_out_loss: 496.0233\n",
      "Epoch 37/100\n",
      "17/17 - 0s - loss: 43.8554 - q_latent_loss: 2.5274 - p_out_loss: 43.1073\n",
      "Epoch 38/100\n",
      "17/17 - 0s - loss: 127.4449 - q_latent_loss: 2.5114 - p_out_loss: 126.7015\n",
      "Epoch 39/100\n",
      "17/17 - 0s - loss: 34.3881 - q_latent_loss: 2.4962 - p_out_loss: 33.6492\n",
      "Epoch 40/100\n",
      "17/17 - 0s - loss: 92.4194 - q_latent_loss: 2.4816 - p_out_loss: 91.6848\n",
      "Epoch 41/100\n",
      "17/17 - 0s - loss: 2711.8994 - q_latent_loss: 2.4669 - p_out_loss: 2711.1697\n",
      "Epoch 42/100\n",
      "17/17 - 0s - loss: 32.7170 - q_latent_loss: 2.4484 - p_out_loss: 31.9922\n",
      "Epoch 43/100\n",
      "17/17 - 0s - loss: 368.3795 - q_latent_loss: 2.4338 - p_out_loss: 367.6590\n",
      "Epoch 44/100\n",
      "17/17 - 0s - loss: 185.1732 - q_latent_loss: 2.4207 - p_out_loss: 184.4566\n",
      "Epoch 45/100\n",
      "17/17 - 0s - loss: 1496.3325 - q_latent_loss: 2.4090 - p_out_loss: 1495.6195\n",
      "Epoch 46/100\n",
      "17/17 - 0s - loss: 40.2950 - q_latent_loss: 2.3979 - p_out_loss: 39.5852\n",
      "Epoch 47/100\n",
      "17/17 - 0s - loss: 3341.2693 - q_latent_loss: 2.3869 - p_out_loss: 3340.5627\n",
      "Epoch 48/100\n",
      "17/17 - 0s - loss: 317.7818 - q_latent_loss: 2.3705 - p_out_loss: 317.0801\n",
      "Epoch 49/100\n",
      "17/17 - 0s - loss: 129.0085 - q_latent_loss: 2.3577 - p_out_loss: 128.3107\n",
      "Epoch 50/100\n",
      "17/17 - 0s - loss: 677.0181 - q_latent_loss: 2.3475 - p_out_loss: 676.3232\n",
      "Epoch 51/100\n",
      "17/17 - 0s - loss: 44.6301 - q_latent_loss: 2.3379 - p_out_loss: 43.9381\n",
      "Epoch 52/100\n",
      "17/17 - 0s - loss: 13.6527 - q_latent_loss: 2.3290 - p_out_loss: 12.9633\n",
      "Epoch 53/100\n",
      "17/17 - 0s - loss: 2174.0337 - q_latent_loss: 2.3206 - p_out_loss: 2173.3469\n",
      "Epoch 54/100\n",
      "17/17 - 0s - loss: 85.2265 - q_latent_loss: 2.3118 - p_out_loss: 84.5422\n",
      "Epoch 55/100\n",
      "17/17 - 0s - loss: 13.3743 - q_latent_loss: 2.3037 - p_out_loss: 12.6924\n",
      "Epoch 56/100\n",
      "17/17 - 0s - loss: 46996.4805 - q_latent_loss: 2.2929 - p_out_loss: 46995.8008\n",
      "Epoch 57/100\n",
      "17/17 - 0s - loss: 23.0730 - q_latent_loss: 2.2527 - p_out_loss: 22.4061\n",
      "Epoch 58/100\n",
      "17/17 - 0s - loss: 11.3692 - q_latent_loss: 2.2361 - p_out_loss: 10.7074\n",
      "Epoch 59/100\n",
      "17/17 - 0s - loss: 25.0329 - q_latent_loss: 2.2285 - p_out_loss: 24.3733\n",
      "Epoch 60/100\n",
      "17/17 - 0s - loss: 17.7706 - q_latent_loss: 2.2227 - p_out_loss: 17.1126\n",
      "Epoch 61/100\n",
      "17/17 - 0s - loss: 10.4981 - q_latent_loss: 2.2176 - p_out_loss: 9.8417\n",
      "Epoch 62/100\n",
      "17/17 - 0s - loss: 32.7279 - q_latent_loss: 2.2127 - p_out_loss: 32.0729\n",
      "Epoch 63/100\n",
      "17/17 - 0s - loss: 30.3548 - q_latent_loss: 2.2081 - p_out_loss: 29.7012\n",
      "Epoch 64/100\n",
      "17/17 - 0s - loss: 30.8280 - q_latent_loss: 2.2037 - p_out_loss: 30.1757\n",
      "Epoch 65/100\n",
      "17/17 - 0s - loss: 41.9700 - q_latent_loss: 2.1996 - p_out_loss: 41.3189\n",
      "Epoch 66/100\n",
      "17/17 - 0s - loss: 54.7435 - q_latent_loss: 2.1956 - p_out_loss: 54.0936\n",
      "Epoch 67/100\n",
      "17/17 - 0s - loss: 43.0750 - q_latent_loss: 2.1918 - p_out_loss: 42.4262\n",
      "Epoch 68/100\n",
      "17/17 - 0s - loss: 243.1561 - q_latent_loss: 2.1882 - p_out_loss: 242.5083\n",
      "Epoch 69/100\n",
      "17/17 - 0s - loss: 22.0350 - q_latent_loss: 2.1842 - p_out_loss: 21.3885\n",
      "Epoch 70/100\n",
      "17/17 - 0s - loss: 99.8952 - q_latent_loss: 2.1805 - p_out_loss: 99.2498\n",
      "Epoch 71/100\n",
      "17/17 - 0s - loss: 31.7489 - q_latent_loss: 2.1772 - p_out_loss: 31.1044\n",
      "Epoch 72/100\n",
      "17/17 - 0s - loss: 10.3156 - q_latent_loss: 2.1741 - p_out_loss: 9.6721\n",
      "Epoch 73/100\n",
      "17/17 - 0s - loss: 10.8083 - q_latent_loss: 2.1710 - p_out_loss: 10.1656\n",
      "Epoch 74/100\n",
      "17/17 - 0s - loss: 149.1215 - q_latent_loss: 2.1682 - p_out_loss: 148.4797\n",
      "Epoch 75/100\n",
      "17/17 - 0s - loss: 11.4474 - q_latent_loss: 2.1655 - p_out_loss: 10.8064\n",
      "Epoch 76/100\n",
      "17/17 - 0s - loss: 11.3532 - q_latent_loss: 2.1629 - p_out_loss: 10.7130\n",
      "Epoch 77/100\n",
      "17/17 - 0s - loss: 9.8263 - q_latent_loss: 2.1603 - p_out_loss: 9.1868\n",
      "Epoch 78/100\n",
      "17/17 - 0s - loss: 12.8481 - q_latent_loss: 2.1578 - p_out_loss: 12.2094\n",
      "Epoch 79/100\n",
      "17/17 - 0s - loss: 10.0819 - q_latent_loss: 2.1554 - p_out_loss: 9.4439\n",
      "Epoch 80/100\n",
      "17/17 - 0s - loss: 9.9589 - q_latent_loss: 2.1531 - p_out_loss: 9.3216\n",
      "Epoch 81/100\n",
      "17/17 - 0s - loss: 148.1879 - q_latent_loss: 2.1508 - p_out_loss: 147.5513\n",
      "Epoch 82/100\n",
      "17/17 - 0s - loss: 13.6919 - q_latent_loss: 2.1483 - p_out_loss: 13.0560\n",
      "Epoch 83/100\n",
      "17/17 - 0s - loss: 11.4130 - q_latent_loss: 2.1462 - p_out_loss: 10.7777\n",
      "Epoch 84/100\n",
      "17/17 - 0s - loss: 62.1876 - q_latent_loss: 2.1442 - p_out_loss: 61.5529\n",
      "Epoch 85/100\n",
      "17/17 - 0s - loss: 37.1363 - q_latent_loss: 2.1424 - p_out_loss: 36.5021\n",
      "Epoch 86/100\n",
      "17/17 - 0s - loss: 9.0239 - q_latent_loss: 2.1406 - p_out_loss: 8.3903\n",
      "Epoch 87/100\n",
      "17/17 - 0s - loss: 9.6573 - q_latent_loss: 2.1389 - p_out_loss: 9.0242\n",
      "Epoch 88/100\n",
      "17/17 - 0s - loss: 16.5087 - q_latent_loss: 2.1371 - p_out_loss: 15.8761\n",
      "Epoch 89/100\n",
      "17/17 - 0s - loss: 9.0933 - q_latent_loss: 2.1355 - p_out_loss: 8.4612\n",
      "Epoch 90/100\n",
      "17/17 - 0s - loss: 11.1045 - q_latent_loss: 2.1339 - p_out_loss: 10.4729\n",
      "Epoch 91/100\n",
      "17/17 - 0s - loss: 14.6627 - q_latent_loss: 2.1323 - p_out_loss: 14.0315\n",
      "Epoch 92/100\n",
      "17/17 - 0s - loss: 12.7246 - q_latent_loss: 2.1308 - p_out_loss: 12.0939\n",
      "Epoch 93/100\n",
      "17/17 - 0s - loss: 18.2482 - q_latent_loss: 2.1294 - p_out_loss: 17.6179\n",
      "Epoch 94/100\n",
      "17/17 - 0s - loss: 12.0889 - q_latent_loss: 2.1280 - p_out_loss: 11.4590\n",
      "Epoch 95/100\n",
      "17/17 - 0s - loss: 35.7015 - q_latent_loss: 2.1267 - p_out_loss: 35.0720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "17/17 - 0s - loss: 156.0881 - q_latent_loss: 2.1253 - p_out_loss: 155.4590\n",
      "Epoch 97/100\n",
      "17/17 - 0s - loss: 8.8568 - q_latent_loss: 2.1240 - p_out_loss: 8.2280\n",
      "Epoch 98/100\n",
      "17/17 - 0s - loss: 10.9563 - q_latent_loss: 2.1228 - p_out_loss: 10.3279\n",
      "Epoch 99/100\n",
      "17/17 - 0s - loss: 10.1586 - q_latent_loss: 2.1217 - p_out_loss: 9.5306\n",
      "Epoch 100/100\n",
      "17/17 - 0s - loss: 14.0194 - q_latent_loss: 2.1206 - p_out_loss: 13.3917\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "# Try playing around with the 2nd loss_weights (below) and KL_WEIGHT (above).\n",
    "N_EPOCHS = 100\n",
    "\n",
    "K.clear_session()\n",
    "prior = make_mvn_prior(LATENT_SIZE, trainable_mean=True, trainable_var=True, offdiag=False)\n",
    "model_ = make_model(prior)\n",
    "\n",
    "model_.compile(optimizer='adam',\n",
    "               loss=[lambda _, model_latent: tfd.kl_divergence(model_latent, prior),\n",
    "                     lambda y_true, model_out: -model_out.log_prob(y_true)],\n",
    "               loss_weights=[0.0, 1.0])\n",
    "hist = model_.fit(ds, epochs=N_EPOCHS, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model est lat: [[ 0.51195845 -1.02728739  0.67845761 -0.11073773]]\n",
      "Model est out: [[ 0.10048061  1.36010552  0.30864524 -0.09840383  1.17217551 -0.67099368\n",
      "   0.95406085  0.03414997]]\n",
      "prior mean: [ 0.5117957  -0.8991166   0.66152537 -0.11197621]\n",
      "true lat: [-1.  1.  5. -5.]\n",
      "true out: [ 0.12000006  2.4699998  -2.76       -2.5         1.53       -1.3\n",
      "  1.31        0.05999994]\n"
     ]
    }
   ],
   "source": [
    "lat_wts = model_.get_layer(\"latent_loc\").weights\n",
    "lat_locs = np.ones((1, LATENT_SIZE)) @ lat_wts[0].numpy() + lat_wts[1].numpy()\n",
    "mix_wts = model_.get_layer(\"out_loc\").weights\n",
    "model_out = lat_locs @ mix_wts[0].numpy() + mix_wts[1].numpy()\n",
    "true_out = mix_mat @ true_dist.mean().numpy()\n",
    "\n",
    "print(f\"Model est lat: {lat_locs}\")\n",
    "print(f\"Model est out: {model_out}\")\n",
    "print(f\"prior mean: {prior.mean().numpy()}\")\n",
    "print(f\"true lat: {true_dist.mean().numpy()}\")\n",
    "print(f\"true out: {true_out.T}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64, 4)\n",
      "(<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 ,  0.0387747 ,  0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143,  0.06062925,  0.02439827, -0.01735771], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# test LearnableMultivariateNormalDiag\n",
    "prior_factory = LearnableMultivariateNormalDiag(LATENT_SIZE)\n",
    "learnable_prior = prior_factory()\n",
    "sample = learnable_prior.sample((100, 64))\n",
    "print(sample.shape)\n",
    "print(learnable_prior.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 ,  0.0387747 ,  0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143,  0.06062925,  0.02439827, -0.01735771], dtype=float32)>)\n",
      "['dense/kernel:0', 'dense/bias:0', 'latent_loc/kernel:0', 'latent_loc/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'out_loc/kernel:0', 'out_loc/bias:0', 'learnable_multivariate_normal_diag_2/mean:0', 'learnable_multivariate_normal_diag_2/transformed_scale:0']\n",
      "Epoch 1/100\n",
      "17/17 - 0s - loss: 266.6295 - q_latent_loss: 6.5640 - p_out_loss: 264.6859\n",
      "Epoch 2/100\n",
      "17/17 - 0s - loss: 2032.0966 - q_latent_loss: 6.2853 - p_out_loss: 2030.2358\n",
      "Epoch 3/100\n",
      "17/17 - 0s - loss: 83.5799 - q_latent_loss: 6.0718 - p_out_loss: 81.7824\n",
      "Epoch 4/100\n",
      "17/17 - 0s - loss: 82.7522 - q_latent_loss: 5.8942 - p_out_loss: 81.0072\n",
      "Epoch 5/100\n",
      "17/17 - 0s - loss: 59.2224 - q_latent_loss: 5.7269 - p_out_loss: 57.5269\n",
      "Epoch 6/100\n",
      "17/17 - 0s - loss: 38.8948 - q_latent_loss: 5.5706 - p_out_loss: 37.2456\n",
      "Epoch 7/100\n",
      "17/17 - 0s - loss: 47.8537 - q_latent_loss: 5.4227 - p_out_loss: 46.2483\n",
      "Epoch 8/100\n",
      "17/17 - 0s - loss: 60.9186 - q_latent_loss: 5.2828 - p_out_loss: 59.3546\n",
      "Epoch 9/100\n",
      "17/17 - 0s - loss: 80.7008 - q_latent_loss: 5.1479 - p_out_loss: 79.1768\n",
      "Epoch 10/100\n",
      "17/17 - 0s - loss: 29.5548 - q_latent_loss: 5.0204 - p_out_loss: 28.0686\n",
      "Epoch 11/100\n",
      "17/17 - 0s - loss: 100.5337 - q_latent_loss: 4.9013 - p_out_loss: 99.0827\n",
      "Epoch 12/100\n",
      "17/17 - 0s - loss: 208.5356 - q_latent_loss: 4.7856 - p_out_loss: 207.1189\n",
      "Epoch 13/100\n",
      "17/17 - 0s - loss: 47.4895 - q_latent_loss: 4.6692 - p_out_loss: 46.1072\n",
      "Epoch 14/100\n",
      "17/17 - 0s - loss: 51.8070 - q_latent_loss: 4.5624 - p_out_loss: 50.4563\n",
      "Epoch 15/100\n",
      "17/17 - 0s - loss: 49.2825 - q_latent_loss: 4.4640 - p_out_loss: 47.9610\n",
      "Epoch 16/100\n",
      "17/17 - 0s - loss: 63.7341 - q_latent_loss: 4.3716 - p_out_loss: 62.4399\n",
      "Epoch 17/100\n",
      "17/17 - 0s - loss: 35.2299 - q_latent_loss: 4.2837 - p_out_loss: 33.9617\n",
      "Epoch 18/100\n",
      "17/17 - 0s - loss: 45.8432 - q_latent_loss: 4.2006 - p_out_loss: 44.5997\n",
      "Epoch 19/100\n",
      "17/17 - 0s - loss: 25.7876 - q_latent_loss: 4.1215 - p_out_loss: 24.5675\n",
      "Epoch 20/100\n",
      "17/17 - 0s - loss: 268.8558 - q_latent_loss: 4.0396 - p_out_loss: 267.6599\n",
      "Epoch 21/100\n",
      "17/17 - 0s - loss: 45.2869 - q_latent_loss: 3.9513 - p_out_loss: 44.1171\n",
      "Epoch 22/100\n",
      "17/17 - 0s - loss: 30.1766 - q_latent_loss: 3.8787 - p_out_loss: 29.0284\n",
      "Epoch 23/100\n",
      "17/17 - 0s - loss: 32.2969 - q_latent_loss: 3.8108 - p_out_loss: 31.1688\n",
      "Epoch 24/100\n",
      "17/17 - 0s - loss: 64.0437 - q_latent_loss: 3.7457 - p_out_loss: 62.9348\n",
      "Epoch 25/100\n",
      "17/17 - 0s - loss: 39.9464 - q_latent_loss: 3.6825 - p_out_loss: 38.8562\n",
      "Epoch 26/100\n",
      "17/17 - 0s - loss: 33.5094 - q_latent_loss: 3.6220 - p_out_loss: 32.4372\n",
      "Epoch 27/100\n",
      "17/17 - 0s - loss: 31.4306 - q_latent_loss: 3.5643 - p_out_loss: 30.3755\n",
      "Epoch 28/100\n",
      "17/17 - 0s - loss: 27.8061 - q_latent_loss: 3.5087 - p_out_loss: 26.7674\n",
      "Epoch 29/100\n",
      "17/17 - 0s - loss: 65.8272 - q_latent_loss: 3.4540 - p_out_loss: 64.8047\n",
      "Epoch 30/100\n",
      "17/17 - 0s - loss: 25.9475 - q_latent_loss: 3.4009 - p_out_loss: 24.9408\n",
      "Epoch 31/100\n",
      "17/17 - 0s - loss: 30.2780 - q_latent_loss: 3.3515 - p_out_loss: 29.2859\n",
      "Epoch 32/100\n",
      "17/17 - 0s - loss: 21.8850 - q_latent_loss: 3.3044 - p_out_loss: 20.9068\n",
      "Epoch 33/100\n",
      "17/17 - 0s - loss: 36.6851 - q_latent_loss: 3.2587 - p_out_loss: 35.7204\n",
      "Epoch 34/100\n",
      "17/17 - 0s - loss: 25.5569 - q_latent_loss: 3.2120 - p_out_loss: 24.6061\n",
      "Epoch 35/100\n",
      "17/17 - 0s - loss: 24.6902 - q_latent_loss: 3.1682 - p_out_loss: 23.7523\n",
      "Epoch 36/100\n",
      "17/17 - 0s - loss: 116.0450 - q_latent_loss: 3.1269 - p_out_loss: 115.1194\n",
      "Epoch 37/100\n",
      "17/17 - 0s - loss: 20.0418 - q_latent_loss: 3.0908 - p_out_loss: 19.1268\n",
      "Epoch 38/100\n",
      "17/17 - 0s - loss: 56.1398 - q_latent_loss: 3.0497 - p_out_loss: 55.2370\n",
      "Epoch 39/100\n",
      "17/17 - 0s - loss: 27.5171 - q_latent_loss: 3.0067 - p_out_loss: 26.6270\n",
      "Epoch 40/100\n",
      "17/17 - 0s - loss: 20.7006 - q_latent_loss: 2.9684 - p_out_loss: 19.8219\n",
      "Epoch 41/100\n",
      "17/17 - 0s - loss: 26.9046 - q_latent_loss: 2.9328 - p_out_loss: 26.0364\n",
      "Epoch 42/100\n",
      "17/17 - 0s - loss: 18.7693 - q_latent_loss: 2.8996 - p_out_loss: 17.9110\n",
      "Epoch 43/100\n",
      "17/17 - 0s - loss: 22.3650 - q_latent_loss: 2.8670 - p_out_loss: 21.5163\n",
      "Epoch 44/100\n",
      "17/17 - 0s - loss: 32.9155 - q_latent_loss: 2.8352 - p_out_loss: 32.0763\n",
      "Epoch 45/100\n",
      "17/17 - 0s - loss: 19.9130 - q_latent_loss: 2.8037 - p_out_loss: 19.0830\n",
      "Epoch 46/100\n",
      "17/17 - 0s - loss: 19.9001 - q_latent_loss: 2.7740 - p_out_loss: 19.0789\n",
      "Epoch 47/100\n",
      "17/17 - 0s - loss: 25.4838 - q_latent_loss: 2.7436 - p_out_loss: 24.6716\n",
      "Epoch 48/100\n",
      "17/17 - 0s - loss: 23.9622 - q_latent_loss: 2.7135 - p_out_loss: 23.1589\n",
      "Epoch 49/100\n",
      "17/17 - 0s - loss: 20.7703 - q_latent_loss: 2.6849 - p_out_loss: 19.9756\n",
      "Epoch 50/100\n",
      "17/17 - 0s - loss: 19.6302 - q_latent_loss: 2.6576 - p_out_loss: 18.8435\n",
      "Epoch 51/100\n",
      "17/17 - 0s - loss: 18.7125 - q_latent_loss: 2.6321 - p_out_loss: 17.9334\n",
      "Epoch 52/100\n",
      "17/17 - 0s - loss: 21.4065 - q_latent_loss: 2.6073 - p_out_loss: 20.6347\n",
      "Epoch 53/100\n",
      "17/17 - 0s - loss: 37.3685 - q_latent_loss: 2.5831 - p_out_loss: 36.6039\n",
      "Epoch 54/100\n",
      "17/17 - 0s - loss: 15.8975 - q_latent_loss: 2.5606 - p_out_loss: 15.1395\n",
      "Epoch 55/100\n",
      "17/17 - 0s - loss: 15.6574 - q_latent_loss: 2.5387 - p_out_loss: 14.9059\n",
      "Epoch 56/100\n",
      "17/17 - 0s - loss: 28.7901 - q_latent_loss: 2.5174 - p_out_loss: 28.0449\n",
      "Epoch 57/100\n",
      "17/17 - 0s - loss: 99.3240 - q_latent_loss: 2.4972 - p_out_loss: 98.5848\n",
      "Epoch 58/100\n",
      "17/17 - 0s - loss: 19.6783 - q_latent_loss: 2.4761 - p_out_loss: 18.9453\n",
      "Epoch 59/100\n",
      "17/17 - 0s - loss: 18.9958 - q_latent_loss: 2.4563 - p_out_loss: 18.2688\n",
      "Epoch 60/100\n",
      "17/17 - 0s - loss: 21.3663 - q_latent_loss: 2.4364 - p_out_loss: 20.6451\n",
      "Epoch 61/100\n",
      "17/17 - 0s - loss: 26.8008 - q_latent_loss: 2.4179 - p_out_loss: 26.0850\n",
      "Epoch 62/100\n",
      "17/17 - 0s - loss: 13.9355 - q_latent_loss: 2.3984 - p_out_loss: 13.2256\n",
      "Epoch 63/100\n",
      "17/17 - 0s - loss: 14.0786 - q_latent_loss: 2.3803 - p_out_loss: 13.3740\n",
      "Epoch 64/100\n",
      "17/17 - 0s - loss: 20.6991 - q_latent_loss: 2.3634 - p_out_loss: 19.9995\n",
      "Epoch 65/100\n",
      "17/17 - 0s - loss: 33.9438 - q_latent_loss: 2.3476 - p_out_loss: 33.2488\n",
      "Epoch 66/100\n",
      "17/17 - 0s - loss: 19.5023 - q_latent_loss: 2.3325 - p_out_loss: 18.8118\n",
      "Epoch 67/100\n",
      "17/17 - 0s - loss: 16.1214 - q_latent_loss: 2.3179 - p_out_loss: 15.4353\n",
      "Epoch 68/100\n",
      "17/17 - 0s - loss: 33.3983 - q_latent_loss: 2.3044 - p_out_loss: 32.7162\n",
      "Epoch 69/100\n",
      "17/17 - 0s - loss: 14.1833 - q_latent_loss: 2.2933 - p_out_loss: 13.5045\n",
      "Epoch 70/100\n",
      "17/17 - 0s - loss: 33.0913 - q_latent_loss: 2.2802 - p_out_loss: 32.4163\n",
      "Epoch 71/100\n",
      "17/17 - 0s - loss: 15.5565 - q_latent_loss: 2.2661 - p_out_loss: 14.8857\n",
      "Epoch 72/100\n",
      "17/17 - 0s - loss: 23.7552 - q_latent_loss: 2.2522 - p_out_loss: 23.0885\n",
      "Epoch 73/100\n",
      "17/17 - 0s - loss: 15.8186 - q_latent_loss: 2.2402 - p_out_loss: 15.1554\n",
      "Epoch 74/100\n",
      "17/17 - 0s - loss: 15.8109 - q_latent_loss: 2.2277 - p_out_loss: 15.1514\n",
      "Epoch 75/100\n",
      "17/17 - 0s - loss: 23.2216 - q_latent_loss: 2.2153 - p_out_loss: 22.5659\n",
      "Epoch 76/100\n",
      "17/17 - 0s - loss: 17.1244 - q_latent_loss: 2.2021 - p_out_loss: 16.4725\n",
      "Epoch 77/100\n",
      "17/17 - 0s - loss: 20.2818 - q_latent_loss: 2.1875 - p_out_loss: 19.6343\n",
      "Epoch 78/100\n",
      "17/17 - 0s - loss: 20.1146 - q_latent_loss: 2.1751 - p_out_loss: 19.4708\n",
      "Epoch 79/100\n",
      "17/17 - 0s - loss: 12.0626 - q_latent_loss: 2.1647 - p_out_loss: 11.4218\n",
      "Epoch 80/100\n",
      "17/17 - 0s - loss: 17.5959 - q_latent_loss: 2.1547 - p_out_loss: 16.9580\n",
      "Epoch 81/100\n",
      "17/17 - 0s - loss: 46.9798 - q_latent_loss: 2.1428 - p_out_loss: 46.3454\n",
      "Epoch 82/100\n",
      "17/17 - 0s - loss: 20.0080 - q_latent_loss: 2.1244 - p_out_loss: 19.3792\n",
      "Epoch 83/100\n",
      "17/17 - 0s - loss: 11.8902 - q_latent_loss: 2.1120 - p_out_loss: 11.2651\n",
      "Epoch 84/100\n",
      "17/17 - 0s - loss: 17.5359 - q_latent_loss: 2.1015 - p_out_loss: 16.9139\n",
      "Epoch 85/100\n",
      "17/17 - 0s - loss: 14.8084 - q_latent_loss: 2.0917 - p_out_loss: 14.1892\n",
      "Epoch 86/100\n",
      "17/17 - 0s - loss: 9.6016 - q_latent_loss: 2.0823 - p_out_loss: 8.9852\n",
      "Epoch 87/100\n",
      "17/17 - 0s - loss: 13.4432 - q_latent_loss: 2.0736 - p_out_loss: 12.8294\n",
      "Epoch 88/100\n",
      "17/17 - 0s - loss: 16.5978 - q_latent_loss: 2.0652 - p_out_loss: 15.9865\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 0s - loss: 21.3484 - q_latent_loss: 2.0557 - p_out_loss: 20.7399\n",
      "Epoch 90/100\n",
      "17/17 - 0s - loss: 11.3400 - q_latent_loss: 2.0439 - p_out_loss: 10.7350\n",
      "Epoch 91/100\n",
      "17/17 - 0s - loss: 14.2551 - q_latent_loss: 2.0345 - p_out_loss: 13.6529\n",
      "Epoch 92/100\n",
      "17/17 - 0s - loss: 14.2384 - q_latent_loss: 2.0268 - p_out_loss: 13.6385\n",
      "Epoch 93/100\n",
      "17/17 - 0s - loss: 16.3489 - q_latent_loss: 2.0194 - p_out_loss: 15.7511\n",
      "Epoch 94/100\n",
      "17/17 - 0s - loss: 14.2265 - q_latent_loss: 2.0118 - p_out_loss: 13.6310\n",
      "Epoch 95/100\n",
      "17/17 - 0s - loss: 11.5992 - q_latent_loss: 2.0041 - p_out_loss: 11.0060\n",
      "Epoch 96/100\n",
      "17/17 - 0s - loss: 11.7333 - q_latent_loss: 1.9971 - p_out_loss: 11.1421\n",
      "Epoch 97/100\n",
      "17/17 - 0s - loss: 12.1329 - q_latent_loss: 1.9904 - p_out_loss: 11.5438\n",
      "Epoch 98/100\n",
      "17/17 - 0s - loss: 13.7211 - q_latent_loss: 1.9832 - p_out_loss: 13.1341\n",
      "Epoch 99/100\n",
      "17/17 - 0s - loss: 37.2112 - q_latent_loss: 1.9745 - p_out_loss: 36.6267\n",
      "Epoch 100/100\n",
      "17/17 - 0s - loss: 9.3972 - q_latent_loss: 1.9630 - p_out_loss: 8.8161\n",
      "Model est lat: [[ 2.51292503 -1.26424221 -1.11180196 -0.02588509]]\n",
      "Model est out: [[-0.27425705  2.39304429 -2.49596335  0.29950965  1.37276651 -0.43098222\n",
      "  -0.45473986  0.07839915]]\n",
      "prior mean: [ 1.3514248  -1.2266612  -0.8751619  -0.03475915]\n",
      "true lat: [-1.  1.  5. -5.]\n",
      "true out: [ 0.12000006  2.4699998  -2.76       -2.5         1.53       -1.3\n",
      "  1.31        0.05999994]\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_ = make_model(learnable_prior)\n",
    "\n",
    "model_.compile(optimizer='adam',\n",
    "               loss=[lambda _, model_latent: tfd.kl_divergence(model_latent, learnable_prior),\n",
    "                     lambda y_true, model_out: -model_out.log_prob(y_true)],\n",
    "               loss_weights=[0.0, 1.0])\n",
    "print(learnable_prior.trainable_variables)\n",
    "print([_.name for _ in model_.trainable_variables])\n",
    "\n",
    "hist = model_.fit(ds, epochs=N_EPOCHS, verbose=2)\n",
    "\n",
    "lat_wts = model_.get_layer(\"latent_loc\").weights\n",
    "lat_locs = np.ones((1, LATENT_SIZE)) @ lat_wts[0].numpy() + lat_wts[1].numpy()\n",
    "mix_wts = model_.get_layer(\"out_loc\").weights\n",
    "model_out = lat_locs @ mix_wts[0].numpy() + mix_wts[1].numpy()\n",
    "true_out = mix_mat @ true_dist.mean().numpy()\n",
    "\n",
    "print(f\"Model est lat: {lat_locs}\")\n",
    "print(f\"Model est out: {model_out}\")\n",
    "print(f\"prior mean: {learnable_prior.mean().numpy()}\")\n",
    "print(f\"true lat: {true_dist.mean().numpy()}\")\n",
    "print(f\"true out: {true_out.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dynamic Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 3 outputs, the first 2 are null\n",
    "#ds_dyn = ds.map(lambda x, y: (x, (y[0], y[0], y[1])))\n",
    "ds_dyn = ds.map(lambda x, y: (x, y[1]))\n",
    "KL_WEIGHT = 0.001\n",
    "LATENT_SIZE_DYNAMIC = 1  # Integer dimensionality of each dynamic, time-variant latent variable `z_t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tmp = LearnableMultivariateNormalDiagCell(3, 4)\n",
    "#tmp.build((None, 10, 5))\n",
    "#tmp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 10, 1)\n",
      "mean: [[ 0.          0.42388976  0.45631832  0.365768    0.20130846  0.37873474\n",
      "   0.31262326  0.26073667  0.15399611  0.14049806]\n",
      " [ 0.          0.08804662  0.0361465  -0.03267653 -0.08733355  0.19941618\n",
      "   0.30335566  0.3730844   0.2744042   0.17948757]]\n",
      "stddev: [[1.00001    0.929902   0.9554563  0.99371654 1.0142238  0.97018814\n",
      "  1.0006421  1.0033575  1.0105829  1.0065393 ]\n",
      " [1.00001    0.9952911  1.0008274  1.0000954  0.99874425 0.98230976\n",
      "  0.9771384  0.9683764  1.000174   1.0049998 ]]\n",
      "['learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/recurrent_kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/bias:0']\n"
     ]
    }
   ],
   "source": [
    "# test DynamicEncoder and LearnableMultivariateNormalDiagCell\n",
    "K.clear_session()\n",
    "dynamic_encoder = DynamicEncoder(N_HIDDEN, N_TIMES, LATENT_SIZE_DYNAMIC)\n",
    "sample, dynamic_prior = dynamic_encoder.sample_dynamic_prior(\n",
    "    N_TIMES, samples=N_SAMPLES, batches=1)\n",
    "print(sample.shape)\n",
    "print(\"mean:\", np.squeeze(dynamic_prior.mean()))\n",
    "print(\"stddev:\", np.squeeze(dynamic_prior.stddev()))\n",
    "print([_.name for _ in dynamic_encoder.trainable_variables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['learnable_multivariate_normal_diag/mean:0', 'learnable_multivariate_normal_diag/untransformed_stddev:0']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "f_model = FactorizedAutoEncoder(N_HIDDEN, N_TIMES, LATENT_SIZE, LATENT_SIZE_DYNAMIC, N_SENSORS)\n",
    "# Most of the trainable variables don't present themselves until the model pieces are called.\n",
    "print([_.name for _ in f_model.static_encoder.trainable_variables])\n",
    "print([_.name for _ in f_model.dynamic_encoder.trainable_variables])\n",
    "print([_.name for _ in f_model.decoder.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Epoch 0/200:\tloss=3777.438; Losses: [3777.3972, 0.0028260916, 0.03819199]\n",
      ".\n",
      "Epoch 1/200:\tloss=908.620; Losses: [908.5834, 0.002681077, 0.034407526]\n",
      ".\n",
      "Epoch 2/200:\tloss=682.733; Losses: [682.7001, 0.0025911012, 0.030505255]\n",
      ".\n",
      "Epoch 3/200:\tloss=317.985; Losses: [317.9555, 0.0024803109, 0.027142774]\n",
      ".\n",
      "Epoch 4/200:\tloss=737.753; Losses: [737.7305, 0.0024128703, 0.019867169]\n",
      ".\n",
      "Epoch 5/200:\tloss=293.983; Losses: [293.9585, 0.0023704215, 0.022514882]\n",
      ".\n",
      "Epoch 6/200:\tloss=412.075; Losses: [412.0592, 0.002335041, 0.013460781]\n",
      ".\n",
      "Epoch 7/200:\tloss=306.618; Losses: [306.60327, 0.002284743, 0.012861049]\n",
      ".\n",
      "Epoch 8/200:\tloss=215.916; Losses: [215.9029, 0.0022479186, 0.0111077]\n",
      ".\n",
      "Epoch 9/200:\tloss=223.136; Losses: [223.12366, 0.0022122823, 0.010420884]\n",
      ".\n",
      "Epoch 10/200:\tloss=300.156; Losses: [300.14383, 0.0021807426, 0.009667382]\n",
      ".\n",
      "Epoch 11/200:\tloss=179.766; Losses: [179.75607, 0.0021481065, 0.007676568]\n",
      ".\n",
      "Epoch 12/200:\tloss=215.981; Losses: [215.97124, 0.0021149626, 0.008112778]\n",
      ".\n",
      "Epoch 13/200:\tloss=208.054; Losses: [208.04565, 0.0020882282, 0.006530414]\n",
      ".\n",
      "Epoch 14/200:\tloss=207.146; Losses: [207.13806, 0.0020594192, 0.0063194335]\n",
      ".\n",
      "Epoch 15/200:\tloss=261.805; Losses: [261.7982, 0.0020306753, 0.0045144106]\n",
      ".\n",
      "Epoch 16/200:\tloss=178.393; Losses: [178.38637, 0.0020005947, 0.0045015276]\n",
      ".\n",
      "Epoch 17/200:\tloss=284.872; Losses: [284.86642, 0.0019730518, 0.003997615]\n",
      ".\n",
      "Epoch 18/200:\tloss=212.054; Losses: [212.04866, 0.0019469936, 0.0034680453]\n",
      ".\n",
      "Epoch 19/200:\tloss=145.862; Losses: [145.85641, 0.001922644, 0.004011639]\n",
      ".\n",
      "Epoch 20/200:\tloss=182.153; Losses: [182.14563, 0.0018981744, 0.0059485184]\n",
      ".\n",
      "Epoch 21/200:\tloss=154.575; Losses: [154.56941, 0.0018728755, 0.0038126395]\n",
      ".\n",
      "Epoch 22/200:\tloss=214.752; Losses: [214.74734, 0.0018469194, 0.003135754]\n",
      ".\n",
      "Epoch 23/200:\tloss=179.347; Losses: [179.34248, 0.001825613, 0.0029100403]\n",
      ".\n",
      "Epoch 24/200:\tloss=354.274; Losses: [354.26898, 0.0018005224, 0.0028155171]\n",
      ".\n",
      "Epoch 25/200:\tloss=181.006; Losses: [181.0021, 0.0017763268, 0.0022955306]\n",
      ".\n",
      "Epoch 26/200:\tloss=142.006; Losses: [142.00166, 0.0017520584, 0.0022001117]\n",
      ".\n",
      "Epoch 27/200:\tloss=158.932; Losses: [158.92723, 0.0017273662, 0.0026816982]\n",
      ".\n",
      "Epoch 28/200:\tloss=175.159; Losses: [175.15494, 0.0017028436, 0.0019087334]\n",
      ".\n",
      "Epoch 29/200:\tloss=167.915; Losses: [167.91084, 0.0016797789, 0.002798946]\n",
      ".\n",
      "Epoch 30/200:\tloss=152.785; Losses: [152.78006, 0.001658167, 0.0029456303]\n",
      ".\n",
      "Epoch 31/200:\tloss=158.407; Losses: [158.40344, 0.0016350556, 0.0018697139]\n",
      ".\n",
      "Epoch 32/200:\tloss=151.065; Losses: [151.06094, 0.0016126116, 0.00282249]\n",
      ".\n",
      "Epoch 33/200:\tloss=181.075; Losses: [181.07072, 0.0015892526, 0.0025259533]\n",
      ".\n",
      "Epoch 34/200:\tloss=157.210; Losses: [157.20605, 0.0015682159, 0.0026225548]\n",
      ".\n",
      "Epoch 35/200:\tloss=151.200; Losses: [151.19623, 0.0015464819, 0.0017979483]\n",
      ".\n",
      "Epoch 36/200:\tloss=157.111; Losses: [157.10796, 0.0015237778, 0.0016808716]\n",
      ".\n",
      "Epoch 37/200:\tloss=160.398; Losses: [160.39449, 0.0015015532, 0.0018031715]\n",
      ".\n",
      "Epoch 38/200:\tloss=133.418; Losses: [133.41472, 0.0014805306, 0.0017712188]\n",
      ".\n",
      "Epoch 39/200:\tloss=161.662; Losses: [161.65845, 0.0014592485, 0.0018676165]\n",
      ".\n",
      "Epoch 40/200:\tloss=181.789; Losses: [181.78532, 0.0014379938, 0.0023018767]\n",
      ".\n",
      "Epoch 41/200:\tloss=183.568; Losses: [183.56451, 0.00142015, 0.001671126]\n",
      ".\n",
      "Epoch 42/200:\tloss=211.679; Losses: [211.67618, 0.0014024411, 0.001817507]\n",
      ".\n",
      "Epoch 43/200:\tloss=235.384; Losses: [235.38095, 0.0013794828, 0.0018946148]\n",
      ".\n",
      "Epoch 44/200:\tloss=139.088; Losses: [139.08447, 0.0013571468, 0.0019089653]\n",
      ".\n",
      "Epoch 45/200:\tloss=160.254; Losses: [160.25092, 0.0013359843, 0.0013933791]\n",
      ".\n",
      "Epoch 46/200:\tloss=142.206; Losses: [142.20291, 0.0013167453, 0.0014015753]\n",
      ".\n",
      "Epoch 47/200:\tloss=138.814; Losses: [138.8115, 0.0012925405, 0.0014298331]\n",
      ".\n",
      "Epoch 48/200:\tloss=130.677; Losses: [130.67343, 0.0012679367, 0.0021604125]\n",
      ".\n",
      "Epoch 49/200:\tloss=145.296; Losses: [145.29306, 0.0012432866, 0.0013332999]\n",
      ".\n",
      "Epoch 50/200:\tloss=133.338; Losses: [133.33516, 0.0012180805, 0.0012685797]\n",
      ".\n",
      "Epoch 51/200:\tloss=138.212; Losses: [138.20908, 0.0011976506, 0.0017989981]\n",
      ".\n",
      "Epoch 52/200:\tloss=136.139; Losses: [136.13644, 0.0011768966, 0.001470595]\n",
      ".\n",
      "Epoch 53/200:\tloss=141.839; Losses: [141.83646, 0.0011539405, 0.0017275128]\n",
      ".\n",
      "Epoch 54/200:\tloss=159.297; Losses: [159.29402, 0.0011311076, 0.0017414299]\n",
      ".\n",
      "Epoch 55/200:\tloss=125.919; Losses: [125.91669, 0.0011091225, 0.0013509693]\n",
      ".\n",
      "Epoch 56/200:\tloss=135.710; Losses: [135.7079, 0.0010871431, 0.0010571101]\n",
      ".\n",
      "Epoch 57/200:\tloss=124.548; Losses: [124.545654, 0.0010660599, 0.0011921946]\n",
      ".\n",
      "Epoch 58/200:\tloss=128.607; Losses: [128.60472, 0.0010461143, 0.0010578154]\n",
      ".\n",
      "Epoch 59/200:\tloss=200.879; Losses: [200.87674, 0.0010245068, 0.0014237395]\n",
      ".\n",
      "Epoch 60/200:\tloss=181.939; Losses: [181.93698, 0.0010024481, 0.0010916217]\n",
      ".\n",
      "Epoch 61/200:\tloss=161.069; Losses: [161.06737, 0.0009815091, 0.0009930782]\n",
      ".\n",
      "Epoch 62/200:\tloss=129.661; Losses: [129.65881, 0.0009596758, 0.001090972]\n",
      ".\n",
      "Epoch 63/200:\tloss=342.733; Losses: [342.73068, 0.000939325, 0.001181667]\n",
      ".\n",
      "Epoch 64/200:\tloss=160.802; Losses: [160.8003, 0.00091621274, 0.0012079075]\n",
      ".\n",
      "Epoch 65/200:\tloss=123.200; Losses: [123.19836, 0.00089694076, 0.0009983401]\n",
      ".\n",
      "Epoch 66/200:\tloss=134.465; Losses: [134.46295, 0.00087896077, 0.0009002821]\n",
      ".\n",
      "Epoch 67/200:\tloss=205.839; Losses: [205.83714, 0.0008604548, 0.001061962]\n",
      ".\n",
      "Epoch 68/200:\tloss=144.191; Losses: [144.18906, 0.0008421927, 0.0011492949]\n",
      ".\n",
      "Epoch 69/200:\tloss=164.397; Losses: [164.39539, 0.0008238552, 0.0010998722]\n",
      ".\n",
      "Epoch 70/200:\tloss=131.024; Losses: [131.02272, 0.00080561615, 0.000867295]\n",
      ".\n",
      "Epoch 71/200:\tloss=130.408; Losses: [130.40652, 0.0007878286, 0.0009589862]\n",
      ".\n",
      "Epoch 72/200:\tloss=120.511; Losses: [120.509705, 0.0007217523, 0.0008722847]\n",
      ".\n",
      "Epoch 73/200:\tloss=122.388; Losses: [122.38655, 0.00069110864, 0.00080618635]\n",
      ".\n",
      "Epoch 74/200:\tloss=123.200; Losses: [123.198654, 0.0006731994, 0.0008244566]\n",
      ".\n",
      "Epoch 75/200:\tloss=117.884; Losses: [117.88217, 0.00065816526, 0.00086460914]\n",
      ".\n",
      "Epoch 76/200:\tloss=123.508; Losses: [123.50694, 0.0006448629, 0.0008477152]\n",
      ".\n",
      "Epoch 77/200:\tloss=121.749; Losses: [121.74744, 0.0006321136, 0.0008150753]\n",
      ".\n",
      "Epoch 78/200:\tloss=145.549; Losses: [145.5473, 0.00061951997, 0.00082959904]\n",
      ".\n",
      "Epoch 79/200:\tloss=135.341; Losses: [135.33992, 0.00060778105, 0.0007312283]\n",
      ".\n",
      "Epoch 80/200:\tloss=131.476; Losses: [131.47452, 0.0005965105, 0.0008391714]\n",
      ".\n",
      "Epoch 81/200:\tloss=123.978; Losses: [123.976944, 0.00058624754, 0.0008059401]\n",
      ".\n",
      "Epoch 82/200:\tloss=136.084; Losses: [136.08298, 0.0005766748, 0.0007252015]\n",
      ".\n",
      "Epoch 83/200:\tloss=137.815; Losses: [137.81375, 0.00056776253, 0.0009108439]\n",
      ".\n",
      "Epoch 84/200:\tloss=116.955; Losses: [116.95401, 0.0005592232, 0.0008040637]\n",
      ".\n",
      "Epoch 85/200:\tloss=131.525; Losses: [131.52376, 0.00055153086, 0.0007604256]\n",
      ".\n",
      "Epoch 86/200:\tloss=135.716; Losses: [135.71432, 0.00054452394, 0.0006871256]\n",
      ".\n",
      "Epoch 87/200:\tloss=191.940; Losses: [191.93927, 0.0005378095, 0.0006448448]\n",
      ".\n",
      "Epoch 88/200:\tloss=170.746; Losses: [170.74509, 0.00053181086, 0.0006591724]\n",
      ".\n",
      "Epoch 89/200:\tloss=121.373; Losses: [121.37161, 0.00052640436, 0.00079500565]\n",
      ".\n",
      "Epoch 90/200:\tloss=126.909; Losses: [126.90761, 0.0005213883, 0.0006021685]\n",
      ".\n",
      "Epoch 91/200:\tloss=122.121; Losses: [122.120255, 0.0005157513, 0.0006873938]\n",
      ".\n",
      "Epoch 92/200:\tloss=129.156; Losses: [129.15442, 0.0005111307, 0.00064897194]\n",
      ".\n",
      "Epoch 93/200:\tloss=113.183; Losses: [113.18219, 0.0005073488, 0.000602577]\n",
      ".\n",
      "Epoch 94/200:\tloss=146.389; Losses: [146.38794, 0.00050397916, 0.0005585851]\n",
      ".\n",
      "Epoch 95/200:\tloss=130.446; Losses: [130.44531, 0.0005009744, 0.00056175387]\n",
      ".\n",
      "Epoch 96/200:\tloss=118.884; Losses: [118.88327, 0.0004950377, 0.0005437781]\n",
      ".\n",
      "Epoch 97/200:\tloss=114.319; Losses: [114.3183, 0.0004938163, 0.0006134198]\n",
      ".\n",
      "Epoch 98/200:\tloss=134.747; Losses: [134.74594, 0.0004912615, 0.0005468172]\n",
      ".\n",
      "Epoch 99/200:\tloss=127.224; Losses: [127.22336, 0.0004886875, 0.00050384714]\n",
      ".\n",
      "Epoch 100/200:\tloss=123.007; Losses: [123.00618, 0.0004862357, 0.00056520273]\n",
      ".\n",
      "Epoch 101/200:\tloss=130.374; Losses: [130.3726, 0.0004841056, 0.0005738659]\n",
      ".\n",
      "Epoch 102/200:\tloss=120.120; Losses: [120.11898, 0.00048118114, 0.00054365897]\n",
      ".\n",
      "Epoch 103/200:\tloss=107.167; Losses: [107.16606, 0.00047940924, 0.0004886348]\n",
      ".\n",
      "Epoch 104/200:\tloss=112.270; Losses: [112.268585, 0.00047771176, 0.0004745159]\n",
      ".\n",
      "Epoch 105/200:\tloss=125.537; Losses: [125.53565, 0.0004758754, 0.00047247266]\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200:\tloss=109.324; Losses: [109.32308, 0.00047426036, 0.00046340926]\n",
      ".\n",
      "Epoch 107/200:\tloss=113.328; Losses: [113.327286, 0.0004729222, 0.00046490182]\n",
      ".\n",
      "Epoch 108/200:\tloss=117.106; Losses: [117.10452, 0.000471713, 0.00062898267]\n",
      ".\n",
      "Epoch 109/200:\tloss=122.371; Losses: [122.37039, 0.0004705812, 0.00051386596]\n",
      ".\n",
      "Epoch 110/200:\tloss=119.422; Losses: [119.42122, 0.0004696009, 0.0005573735]\n",
      ".\n",
      "Epoch 111/200:\tloss=131.784; Losses: [131.78348, 0.000468354, 0.00041559048]\n",
      ".\n",
      "Epoch 112/200:\tloss=124.476; Losses: [124.475006, 0.00046699354, 0.00041292777]\n",
      ".\n",
      "Epoch 113/200:\tloss=104.487; Losses: [104.486534, 0.00046530017, 0.00042556314]\n",
      ".\n",
      "Epoch 114/200:\tloss=119.418; Losses: [119.41684, 0.0004641684, 0.00039730535]\n",
      ".\n",
      "Epoch 115/200:\tloss=117.776; Losses: [117.77547, 0.00046339296, 0.00056288636]\n",
      ".\n",
      "Epoch 116/200:\tloss=112.189; Losses: [112.18817, 0.0004628609, 0.00045002214]\n",
      ".\n",
      "Epoch 117/200:\tloss=116.317; Losses: [116.31613, 0.0004616853, 0.00036934114]\n",
      ".\n",
      "Epoch 118/200:\tloss=159.105; Losses: [159.10422, 0.00046110825, 0.0003716088]\n",
      ".\n",
      "Epoch 119/200:\tloss=116.958; Losses: [116.95712, 0.00045984008, 0.0003543413]\n",
      ".\n",
      "Epoch 120/200:\tloss=108.100; Losses: [108.09944, 0.00045923653, 0.00045118056]\n",
      ".\n",
      "Epoch 121/200:\tloss=107.565; Losses: [107.56447, 0.00045848632, 0.00033903003]\n",
      ".\n",
      "Epoch 122/200:\tloss=117.631; Losses: [117.62992, 0.0004574218, 0.00033954927]\n",
      ".\n",
      "Epoch 123/200:\tloss=116.075; Losses: [116.07385, 0.0004564249, 0.00036835673]\n",
      ".\n",
      "Epoch 124/200:\tloss=106.798; Losses: [106.79701, 0.0004566427, 0.00032678706]\n",
      ".\n",
      "Epoch 125/200:\tloss=113.363; Losses: [113.36252, 0.0004562596, 0.00042438688]\n",
      ".\n",
      "Epoch 126/200:\tloss=118.104; Losses: [118.10279, 0.00045581322, 0.00032525152]\n",
      ".\n",
      "Epoch 127/200:\tloss=113.516; Losses: [113.51486, 0.00045547614, 0.0005056638]\n",
      ".\n",
      "Epoch 128/200:\tloss=117.624; Losses: [117.62353, 0.00045549794, 0.00034517745]\n",
      ".\n",
      "Epoch 129/200:\tloss=112.273; Losses: [112.272316, 0.00045538746, 0.00029629772]\n",
      ".\n",
      "Epoch 130/200:\tloss=113.328; Losses: [113.32768, 0.0004549961, 0.0003071945]\n",
      ".\n",
      "Epoch 131/200:\tloss=111.756; Losses: [111.75513, 0.00045427072, 0.00032724047]\n",
      ".\n",
      "Epoch 132/200:\tloss=107.796; Losses: [107.795494, 0.00045377907, 0.00027464304]\n",
      ".\n",
      "Epoch 133/200:\tloss=150.595; Losses: [150.59428, 0.00045307074, 0.0003010978]\n",
      ".\n",
      "Epoch 134/200:\tloss=120.134; Losses: [120.13356, 0.00045292114, 0.00029552256]\n",
      ".\n",
      "Epoch 135/200:\tloss=120.130; Losses: [120.12947, 0.00045320712, 0.0002585037]\n",
      ".\n",
      "Epoch 136/200:\tloss=117.070; Losses: [117.06926, 0.0004533619, 0.00026611943]\n",
      ".\n",
      "Epoch 137/200:\tloss=111.006; Losses: [111.00518, 0.00045333267, 0.0002824646]\n",
      ".\n",
      "Epoch 138/200:\tloss=115.901; Losses: [115.90064, 0.00045347284, 0.00030576388]\n",
      ".\n",
      "Epoch 139/200:\tloss=111.147; Losses: [111.146286, 0.000453111, 0.0002558468]\n",
      ".\n",
      "Epoch 140/200:\tloss=103.128; Losses: [103.12687, 0.0004522237, 0.00038727812]\n",
      ".\n",
      "Epoch 141/200:\tloss=115.025; Losses: [115.024475, 0.00045187408, 0.0002339398]\n",
      ".\n",
      "Epoch 142/200:\tloss=117.170; Losses: [117.16969, 0.0004520767, 0.00023050333]\n",
      ".\n",
      "Epoch 143/200:\tloss=105.315; Losses: [105.31448, 0.0004517807, 0.00026579303]\n",
      ".\n",
      "Epoch 144/200:\tloss=114.114; Losses: [114.113625, 0.00045176974, 0.0002442702]\n",
      ".\n",
      "Epoch 145/200:\tloss=108.179; Losses: [108.178154, 0.00045183185, 0.00022483827]\n",
      ".\n",
      "Epoch 146/200:\tloss=119.408; Losses: [119.407074, 0.0004509559, 0.00021656642]\n",
      ".\n",
      "Epoch 147/200:\tloss=116.504; Losses: [116.50336, 0.000450821, 0.00021879328]\n",
      ".\n",
      "Epoch 148/200:\tloss=108.465; Losses: [108.464005, 0.0004506137, 0.00031175395]\n",
      ".\n",
      "Epoch 149/200:\tloss=99.284; Losses: [99.28293, 0.00045024417, 0.00022024836]\n",
      ".\n",
      "Epoch 150/200:\tloss=105.143; Losses: [105.14198, 0.00044986696, 0.00024956765]\n",
      ".\n",
      "Epoch 151/200:\tloss=107.015; Losses: [107.01389, 0.0004503439, 0.00019209863]\n",
      ".\n",
      "Epoch 152/200:\tloss=109.961; Losses: [109.96058, 0.00045052002, 0.00036505712]\n",
      ".\n",
      "Epoch 153/200:\tloss=110.943; Losses: [110.94235, 0.00045058262, 0.00019362733]\n",
      ".\n",
      "Epoch 154/200:\tloss=105.146; Losses: [105.14586, 0.0004505078, 0.00018350873]\n",
      ".\n",
      "Epoch 155/200:\tloss=153.239; Losses: [153.23862, 0.00045094165, 0.00018688777]\n",
      ".\n",
      "Epoch 156/200:\tloss=97.193; Losses: [97.192276, 0.0004497521, 0.0002643012]\n",
      ".\n",
      "Epoch 157/200:\tloss=116.076; Losses: [116.075356, 0.00044927179, 0.00018770616]\n",
      ".\n",
      "Epoch 158/200:\tloss=99.644; Losses: [99.64349, 0.00044897772, 0.00019957994]\n",
      ".\n",
      "Epoch 159/200:\tloss=101.686; Losses: [101.68573, 0.00044913022, 0.0001649716]\n",
      ".\n",
      "Epoch 160/200:\tloss=114.998; Losses: [114.99737, 0.00044872603, 0.0001598363]\n",
      ".\n",
      "Epoch 161/200:\tloss=126.449; Losses: [126.44795, 0.00044798924, 0.00017636445]\n",
      ".\n",
      "Epoch 162/200:\tloss=99.323; Losses: [99.32204, 0.00044971833, 0.0001718228]\n",
      ".\n",
      "Epoch 163/200:\tloss=118.403; Losses: [118.402115, 0.0004499098, 0.00016231032]\n",
      ".\n",
      "Epoch 164/200:\tloss=101.217; Losses: [101.21654, 0.00044922353, 0.00015090306]\n",
      ".\n",
      "Epoch 165/200:\tloss=132.002; Losses: [132.0016, 0.0004491811, 0.00018679435]\n",
      ".\n",
      "Epoch 166/200:\tloss=103.262; Losses: [103.26103, 0.00044870118, 0.00014671378]\n",
      ".\n",
      "Epoch 167/200:\tloss=98.593; Losses: [98.592026, 0.0004482735, 0.00017167021]\n",
      ".\n",
      "Epoch 168/200:\tloss=102.641; Losses: [102.64062, 0.00044823167, 0.0001966377]\n",
      ".\n",
      "Epoch 169/200:\tloss=110.199; Losses: [110.19867, 0.00044768318, 0.00014072815]\n",
      ".\n",
      "Epoch 170/200:\tloss=98.456; Losses: [98.45533, 0.00044640992, 0.00013351238]\n",
      ".\n",
      "Epoch 171/200:\tloss=107.700; Losses: [107.698944, 0.00044608887, 0.000128763]\n",
      ".\n",
      "Epoch 172/200:\tloss=110.314; Losses: [110.31317, 0.0004454155, 0.00015472547]\n",
      ".\n",
      "Epoch 173/200:\tloss=101.824; Losses: [101.82384, 0.00044520054, 0.00012376827]\n",
      ".\n",
      "Epoch 174/200:\tloss=100.615; Losses: [100.61448, 0.00044518447, 0.00012106997]\n",
      ".\n",
      "Epoch 175/200:\tloss=99.010; Losses: [99.00989, 0.00044499052, 0.0001265088]\n",
      ".\n",
      "Epoch 176/200:\tloss=104.999; Losses: [104.99841, 0.0004448745, 0.00017745573]\n",
      ".\n",
      "Epoch 177/200:\tloss=98.012; Losses: [98.0118, 0.0004448767, 0.00016406355]\n",
      ".\n",
      "Epoch 178/200:\tloss=99.610; Losses: [99.60982, 0.0004446858, 0.00011498472]\n",
      ".\n",
      "Epoch 179/200:\tloss=108.899; Losses: [108.89821, 0.00044491427, 0.00010847509]\n",
      ".\n",
      "Epoch 180/200:\tloss=118.136; Losses: [118.13521, 0.00044519745, 0.00010489453]\n",
      ".\n",
      "Epoch 181/200:\tloss=98.810; Losses: [98.80894, 0.000445671, 0.00012609128]\n",
      ".\n",
      "Epoch 182/200:\tloss=96.406; Losses: [96.40544, 0.0004462903, 0.00010188887]\n",
      ".\n",
      "Epoch 183/200:\tloss=98.501; Losses: [98.50068, 0.00044781342, 9.952572e-05]\n",
      ".\n",
      "Epoch 184/200:\tloss=97.149; Losses: [97.14829, 0.00044934792, 0.00010874969]\n",
      ".\n",
      "Epoch 185/200:\tloss=100.637; Losses: [100.63678, 0.00044996737, 9.560937e-05]\n",
      ".\n",
      "Epoch 186/200:\tloss=97.202; Losses: [97.201294, 0.00045056257, 9.1939364e-05]\n",
      ".\n",
      "Epoch 187/200:\tloss=99.839; Losses: [99.83876, 0.0004511009, 8.99044e-05]\n",
      ".\n",
      "Epoch 188/200:\tloss=118.457; Losses: [118.45691, 0.00045174357, 9.007633e-05]\n",
      ".\n",
      "Epoch 189/200:\tloss=102.419; Losses: [102.41877, 0.00045339097, 9.094182e-05]\n",
      ".\n",
      "Epoch 190/200:\tloss=110.813; Losses: [110.812386, 0.00045527803, 8.5699685e-05]\n",
      ".\n",
      "Epoch 191/200:\tloss=99.384; Losses: [99.38332, 0.000456504, 8.375079e-05]\n",
      ".\n",
      "Epoch 192/200:\tloss=103.581; Losses: [103.58075, 0.0004564843, 9.893996e-05]\n",
      ".\n",
      "Epoch 193/200:\tloss=97.621; Losses: [97.62078, 0.00045904273, 8.179152e-05]\n",
      ".\n",
      "Epoch 194/200:\tloss=94.909; Losses: [94.90842, 0.00046064917, 7.898855e-05]\n",
      ".\n",
      "Epoch 195/200:\tloss=100.840; Losses: [100.83898, 0.0004623049, 8.4420986e-05]\n",
      ".\n",
      "Epoch 196/200:\tloss=98.157; Losses: [98.15616, 0.00046370056, 8.1935854e-05]\n",
      ".\n",
      "Epoch 197/200:\tloss=95.283; Losses: [95.28199, 0.00046543585, 7.4171934e-05]\n",
      ".\n",
      "Epoch 198/200:\tloss=98.005; Losses: [98.00419, 0.0004669357, 7.385877e-05]\n",
      ".\n",
      "Epoch 199/200:\tloss=103.609; Losses: [103.608406, 0.00046879202, 7.1431816e-05]\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 200\n",
    "if False:\n",
    "    f_model.compile(optimizer='adam',\n",
    "                    loss=lambda y_true, model_out: -model_out.log_prob(y_true))\n",
    "    hist = f_model.fit(ds_dyn, epochs=N_EPOCHS, verbose=2)\n",
    "else:\n",
    "    @tf.function\n",
    "    def grad(model, inputs, preds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_f = model.static_encoder(inputs)\n",
    "            q_z = model.dynamic_encoder(inputs)\n",
    "            p_full = model.decoder([tf.convert_to_tensor(q_f),\n",
    "                                    tf.convert_to_tensor(q_z)])\n",
    "\n",
    "            # Reconstruction log-likelihood: p(output|input)\n",
    "            recon_post_log_prob = p_full.log_prob(preds)\n",
    "            recon_post_log_prob = tf.reduce_sum(recon_post_log_prob,\n",
    "                                                axis=-1)  # Sum over time axis\n",
    "            recon_post_log_prob = tf.reduce_mean(recon_post_log_prob)\n",
    "\n",
    "            # KL Divergence - analytical\n",
    "            # Static\n",
    "            static_prior = model.static_encoder.static_prior_factory()\n",
    "            stat_kl = tfd.kl_divergence(q_f, static_prior)\n",
    "            stat_kl = KL_WEIGHT * stat_kl\n",
    "            stat_kl = tf.reduce_mean(stat_kl)\n",
    "            \n",
    "            # Dynamic\n",
    "            _, dynamic_prior = model.dynamic_encoder.sample_dynamic_prior(\n",
    "                N_TIMES, samples=1, batches=1\n",
    "            )\n",
    "            dyn_kl = tfd.kl_divergence(q_z, dynamic_prior)\n",
    "            dyn_kl = tf.reduce_sum(dyn_kl, axis=-1)\n",
    "            dyn_kl = tf.squeeze(dyn_kl)\n",
    "            dyn_kl = KL_WEIGHT * dyn_kl\n",
    "            dyn_kl = tf.reduce_mean(dyn_kl)\n",
    "            \n",
    "            loss = -recon_post_log_prob + stat_kl + dyn_kl\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        return loss, grads, (-recon_post_log_prob, stat_kl, dyn_kl)\n",
    "\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    for epoch_ix in range(N_EPOCHS):\n",
    "        for step_ix, batch in enumerate(ds_dyn):\n",
    "            inputs, preds = batch\n",
    "            loss, grads, loss_comps = grad(f_model, inputs, preds)\n",
    "            optim.apply_gradients(zip(grads, f_model.trainable_variables))\n",
    "            if (step_ix % 200) == 0:\n",
    "                print('.')\n",
    "        print(f\"Epoch {epoch_ix}/{N_EPOCHS}:\\tloss={loss:.3f}; \"\n",
    "              f\"Losses: {[_.numpy() for _ in loss_comps]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6796231, -2.5568666, -2.4644861, -2.4013069, -2.3772488,\n",
       "       -2.3707619, -2.377105 , -2.332031 , -2.3327737, -2.360692 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, dyn_prior = f_model.dynamic_encoder.sample_dynamic_prior(10)\n",
    "np.squeeze(dyn_prior.mean().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 10, 1)\n",
      "tf.Tensor(\n",
      "[[ 0.        ]\n",
      " [ 0.04328619]\n",
      " [ 0.08498121]\n",
      " [-0.17377347]\n",
      " [-0.09743058]\n",
      " [-0.30255282]\n",
      " [-0.22110605]\n",
      " [-0.36379734]\n",
      " [-0.30933833]\n",
      " [-0.13590682]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "dynamic_prior = RNNMultivariateNormalDiag(VariationalLSTMCell(N_HIDDEN,\n",
    "                                                              output_dim=LATENT_SIZE_DYNAMIC),\n",
    "                                          n_timesteps=N_TIMES, output_dim=LATENT_SIZE_DYNAMIC)\n",
    "sample = dynamic_prior.sample((N_SAMPLES, BATCH_SIZE))\n",
    "print(sample.shape)\n",
    "print(dynamic_prior.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}