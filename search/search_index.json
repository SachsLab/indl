{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction indl (Intracranial Neurophys and Deep Learning) is a Python package providing some tools to assist with deep-learning analysis of neurophysiology data, with an emphasis on intracranial neurophysiology. It is intended to be a companion to the SachsLab Tutorial on Intracranial Neurophysiology and Deep Learning . The SachsLab also uses this library in some of their research. Install pip install git+https://github.com/SachsLab/indl.git Dependencies I haven't added all the requirements to settings.ini yet so they won't be installed automatically. Chances are that if you are using this Python package then you already have the required dependencies installed in your environment. Some of the dependencies are: numpy scipy tensorflow tensorflow-probability h5py (only required for importing our custom monkey neurophys files) pandas (only required for importing our custom monkey neurophys files) Documentation The documentation is under construction but can be found at https://SachsLab.github.io/indl/ Maintenance Notes Some notes for ongoing maintaining of this repository. Repository Organization Library code in the /indl folder Unit tests in the /tests folder. Note this uses the pytest framework and conventions. I'm still building out the unit tests. Documentation in the /docs folder If you build the docs locally then you'll also get the /site directory, but this should be git ignored. Maintaining the Documentation You will need to install several Python packages to maintain the documentation. pip install mkdocs mkdocstrings mknotebooks mkdocs-material Pygments The /docs/API folder has stubs to tell the mkdocstrings plugin to build the API documentation from the docstrings in the library code itself. The /docs/{top-level-section} folders contain a mix of .md and .ipynb documentation. The latter are converted to .md by the mknotebooks plugin during building. Run mkdocs gh-deploy to build the documentation, commit to the gh-deploy branch, and push to GitHub. This will make the documentation available at https://SachsLab.github.io/indl/ Here is a great cheat sheet for mkdocs and mkdocstrings syntax. Testing the documentation locally mkdocs serve Running the unit tests There aren't that many tests yet; I'm still building them out.","title":"Introduction"},{"location":"#introduction","text":"indl (Intracranial Neurophys and Deep Learning) is a Python package providing some tools to assist with deep-learning analysis of neurophysiology data, with an emphasis on intracranial neurophysiology. It is intended to be a companion to the SachsLab Tutorial on Intracranial Neurophysiology and Deep Learning . The SachsLab also uses this library in some of their research.","title":"Introduction"},{"location":"#install","text":"pip install git+https://github.com/SachsLab/indl.git","title":"Install"},{"location":"#dependencies","text":"I haven't added all the requirements to settings.ini yet so they won't be installed automatically. Chances are that if you are using this Python package then you already have the required dependencies installed in your environment. Some of the dependencies are: numpy scipy tensorflow tensorflow-probability h5py (only required for importing our custom monkey neurophys files) pandas (only required for importing our custom monkey neurophys files)","title":"Dependencies"},{"location":"#documentation","text":"The documentation is under construction but can be found at https://SachsLab.github.io/indl/","title":"Documentation"},{"location":"#maintenance-notes","text":"Some notes for ongoing maintaining of this repository.","title":"Maintenance Notes"},{"location":"#repository-organization","text":"Library code in the /indl folder Unit tests in the /tests folder. Note this uses the pytest framework and conventions. I'm still building out the unit tests. Documentation in the /docs folder If you build the docs locally then you'll also get the /site directory, but this should be git ignored.","title":"Repository Organization"},{"location":"#maintaining-the-documentation","text":"You will need to install several Python packages to maintain the documentation. pip install mkdocs mkdocstrings mknotebooks mkdocs-material Pygments The /docs/API folder has stubs to tell the mkdocstrings plugin to build the API documentation from the docstrings in the library code itself. The /docs/{top-level-section} folders contain a mix of .md and .ipynb documentation. The latter are converted to .md by the mknotebooks plugin during building. Run mkdocs gh-deploy to build the documentation, commit to the gh-deploy branch, and push to GitHub. This will make the documentation available at https://SachsLab.github.io/indl/ Here is a great cheat sheet for mkdocs and mkdocstrings syntax.","title":"Maintaining the Documentation"},{"location":"#testing-the-documentation-locally","text":"mkdocs serve","title":"Testing the documentation locally"},{"location":"#running-the-unit-tests","text":"There aren't that many tests yet; I'm still building them out.","title":"Running the unit tests"},{"location":"API/data/","text":"data augmentations add_depth_dim ( X , y ) Add extra dimension at tail for x only. This is trivial to do in-line. This is slightly more convenient than writing a labmda. Parameters: Name Type Description Default X tf.tensor required y tf.tensor required Returns: Type Description tf.tensor, tf.tensor X, y tuple, with X having a new trailing dimension. Source code in indl/data/augmentations.py def add_depth_dim ( X , y ): \"\"\" Add extra dimension at tail for x only. This is trivial to do in-line. This is slightly more convenient than writing a labmda. Args: X (tf.tensor): y (tf.tensor): Returns: tf.tensor, tf.tensor: X, y tuple, with X having a new trailing dimension. \"\"\" x_dat = tf . expand_dims ( X , - 1 ) # Prepare as an image, with only 1 colour-depth channel. return x_dat , y cast_type ( X , y , x_type = tf . float32 , y_type = tf . uint8 ) Cast input pair to new dtypes. Parameters: Name Type Description Default X tf.tensor Input tensor required y tf.tensor Input labels required x_type tf.dtypes tf data type tf.float32 y_type tf.dtypes tf data type tf.uint8 Returns: Type Description tf.tensor, tf.tensor X, y tuple, each cast to its new type. Source code in indl/data/augmentations.py def cast_type ( X , y , x_type = tf . float32 , y_type = tf . uint8 ): \"\"\" Cast input pair to new dtypes. Args: X (tf.tensor): Input tensor y (tf.tensor): Input labels x_type (tf.dtypes): tf data type y_type (tf.dtypes): tf data type Returns: tf.tensor, tf.tensor: X, y tuple, each cast to its new type. \"\"\" x_dat = tf . cast ( X , x_type ) y_dat = tf . cast ( y , y_type ) return x_dat , y_dat random_slice ( X , y , training = True , max_offset = 0 , axis = 1 ) Slice a tensor X along axis, beginning at a random offset up to max_offset, taking (X.shape[axis] - max_offset) samples. If training==False, this will take the last N-max_offset samples. Parameters: Name Type Description Default X tf.tensor input tensor required y tf.tensor input labels required training bool if the model is run in training state True max_offset int number of samples 0 axis int axis along which to slice 1 Returns: Type Description tf.tensor, tf.tensor X, y tuple randomly sliced. Source code in indl/data/augmentations.py def random_slice ( X , y , training = True , max_offset = 0 , axis = 1 ): \"\"\" Slice a tensor X along axis, beginning at a random offset up to max_offset, taking (X.shape[axis] - max_offset) samples. If training==False, this will take the last N-max_offset samples. Args: X (tf.tensor): input tensor y (tf.tensor): input labels training (bool): if the model is run in training state max_offset (int): number of samples axis (int): axis along which to slice Returns: tf.tensor, tf.tensor: X, y tuple randomly sliced. \"\"\" if training : offset = tf . random . uniform ( shape = [], minval = 0 , maxval = max_offset , dtype = tf . int32 ) else : offset = max_offset n_subsamps = X . shape [ axis ] - max_offset if axis == 0 : if len ( y . shape ) > axis and y . shape [ axis ] == X . shape [ axis ]: y = tf . slice ( y , [ offset , 0 ], [ n_subsamps , - 1 ]) X = tf . slice ( X , [ offset , 0 ], [ n_subsamps , - 1 ]) else : # axis == 1 if len ( y . shape ) > axis and y . shape [ axis ] == X . shape [ axis ]: y = tf . slice ( y , [ 0 , offset ], [ - 1 , n_subsamps ]) X = tf . slice ( X , [ 0 , offset ], [ - 1 , n_subsamps ]) return X , y helper get_tf_dataset ( X , Y , training = True , batch_size = 8 , max_offset = 0 , slice_ax = 1 ) Convert a pair of tf tensors into a tf.data.Dataset with some augmentations. The added augmentations are: add_depth_dim (with default params) cast_type (with default params) random_slice Parameters: Name Type Description Default X tf.tensor X data - must be compatible with above augmentations. required Y tf.tensor Y data - must be compatible with above augmentations. required training bool or tuple passed to random_slice , or if a tuple (e.g. from sklearn.model_selection.train_test_split) then this function returns training and test sets. True batch_size int Unused I think. 8 max_offset int Passed to random_slice 0 slice_ax int Passed to random_slice 1 Returns: Type Description tf.data.Dataset(, tf.Dataset) A tensorflow dataset with extra augmentations. If training is a tuple then two datasets are returning: training set and test set. Source code in indl/data/helper.py def get_tf_dataset ( X , Y , training = True , batch_size = 8 , max_offset = 0 , slice_ax = 1 ): \"\"\" Convert a pair of tf tensors into a tf.data.Dataset with some augmentations. The added augmentations are: - `add_depth_dim` (with default params) - `cast_type` (with default params) - `random_slice` Args: X (tf.tensor): X data - must be compatible with above augmentations. Y (tf.tensor): Y data - must be compatible with above augmentations. training (bool or tuple): passed to `random_slice`, or if a tuple (e.g. from sklearn.model_selection.train_test_split) then this function returns training and test sets. batch_size (int): Unused I think. max_offset (int): Passed to `random_slice` slice_ax (int): Passed to `random_slice` Returns: tf.data.Dataset(, tf.Dataset): A tensorflow dataset with extra augmentations. If training is a tuple then two datasets are returning: training set and test set. \"\"\" # TODO: trn_test as arg if isinstance ( training , tuple ): ds_train = get_tf_dataset ( X [ training [ 0 ]], Y [ training [ 0 ]], training = True , batch_size = batch_size ) ds_test = get_tf_dataset ( X [ training [ 1 ]], Y [ training [ 1 ]], training = False , batch_size = batch_size ) return ds_train , ds_test _ds = tf . data . Dataset . from_tensor_slices (( X , Y )) _ds = _ds . map ( add_depth_dim ) _ds = _ds . map ( cast_type ) slice_fun = partial ( random_slice , training = training , max_offset = max_offset , axis = slice_ax ) _ds = _ds . map ( slice_fun ) if training : _ds = _ds . shuffle () _ds = _ds . batch ( X . shape [ 0 ] + 1 , drop_remainder = not training ) return _ds","title":"data"},{"location":"API/data/#data","text":"","title":"data"},{"location":"API/data/#indl.data","text":"","title":"indl.data"},{"location":"API/data/#indl.data.augmentations","text":"","title":"augmentations"},{"location":"API/data/#indl.data.augmentations.add_depth_dim","text":"Add extra dimension at tail for x only. This is trivial to do in-line. This is slightly more convenient than writing a labmda. Parameters: Name Type Description Default X tf.tensor required y tf.tensor required Returns: Type Description tf.tensor, tf.tensor X, y tuple, with X having a new trailing dimension. Source code in indl/data/augmentations.py def add_depth_dim ( X , y ): \"\"\" Add extra dimension at tail for x only. This is trivial to do in-line. This is slightly more convenient than writing a labmda. Args: X (tf.tensor): y (tf.tensor): Returns: tf.tensor, tf.tensor: X, y tuple, with X having a new trailing dimension. \"\"\" x_dat = tf . expand_dims ( X , - 1 ) # Prepare as an image, with only 1 colour-depth channel. return x_dat , y","title":"add_depth_dim()"},{"location":"API/data/#indl.data.augmentations.cast_type","text":"Cast input pair to new dtypes. Parameters: Name Type Description Default X tf.tensor Input tensor required y tf.tensor Input labels required x_type tf.dtypes tf data type tf.float32 y_type tf.dtypes tf data type tf.uint8 Returns: Type Description tf.tensor, tf.tensor X, y tuple, each cast to its new type. Source code in indl/data/augmentations.py def cast_type ( X , y , x_type = tf . float32 , y_type = tf . uint8 ): \"\"\" Cast input pair to new dtypes. Args: X (tf.tensor): Input tensor y (tf.tensor): Input labels x_type (tf.dtypes): tf data type y_type (tf.dtypes): tf data type Returns: tf.tensor, tf.tensor: X, y tuple, each cast to its new type. \"\"\" x_dat = tf . cast ( X , x_type ) y_dat = tf . cast ( y , y_type ) return x_dat , y_dat","title":"cast_type()"},{"location":"API/data/#indl.data.augmentations.random_slice","text":"Slice a tensor X along axis, beginning at a random offset up to max_offset, taking (X.shape[axis] - max_offset) samples. If training==False, this will take the last N-max_offset samples. Parameters: Name Type Description Default X tf.tensor input tensor required y tf.tensor input labels required training bool if the model is run in training state True max_offset int number of samples 0 axis int axis along which to slice 1 Returns: Type Description tf.tensor, tf.tensor X, y tuple randomly sliced. Source code in indl/data/augmentations.py def random_slice ( X , y , training = True , max_offset = 0 , axis = 1 ): \"\"\" Slice a tensor X along axis, beginning at a random offset up to max_offset, taking (X.shape[axis] - max_offset) samples. If training==False, this will take the last N-max_offset samples. Args: X (tf.tensor): input tensor y (tf.tensor): input labels training (bool): if the model is run in training state max_offset (int): number of samples axis (int): axis along which to slice Returns: tf.tensor, tf.tensor: X, y tuple randomly sliced. \"\"\" if training : offset = tf . random . uniform ( shape = [], minval = 0 , maxval = max_offset , dtype = tf . int32 ) else : offset = max_offset n_subsamps = X . shape [ axis ] - max_offset if axis == 0 : if len ( y . shape ) > axis and y . shape [ axis ] == X . shape [ axis ]: y = tf . slice ( y , [ offset , 0 ], [ n_subsamps , - 1 ]) X = tf . slice ( X , [ offset , 0 ], [ n_subsamps , - 1 ]) else : # axis == 1 if len ( y . shape ) > axis and y . shape [ axis ] == X . shape [ axis ]: y = tf . slice ( y , [ 0 , offset ], [ - 1 , n_subsamps ]) X = tf . slice ( X , [ 0 , offset ], [ - 1 , n_subsamps ]) return X , y","title":"random_slice()"},{"location":"API/data/#indl.data.helper","text":"","title":"helper"},{"location":"API/data/#indl.data.helper.get_tf_dataset","text":"Convert a pair of tf tensors into a tf.data.Dataset with some augmentations. The added augmentations are: add_depth_dim (with default params) cast_type (with default params) random_slice Parameters: Name Type Description Default X tf.tensor X data - must be compatible with above augmentations. required Y tf.tensor Y data - must be compatible with above augmentations. required training bool or tuple passed to random_slice , or if a tuple (e.g. from sklearn.model_selection.train_test_split) then this function returns training and test sets. True batch_size int Unused I think. 8 max_offset int Passed to random_slice 0 slice_ax int Passed to random_slice 1 Returns: Type Description tf.data.Dataset(, tf.Dataset) A tensorflow dataset with extra augmentations. If training is a tuple then two datasets are returning: training set and test set. Source code in indl/data/helper.py def get_tf_dataset ( X , Y , training = True , batch_size = 8 , max_offset = 0 , slice_ax = 1 ): \"\"\" Convert a pair of tf tensors into a tf.data.Dataset with some augmentations. The added augmentations are: - `add_depth_dim` (with default params) - `cast_type` (with default params) - `random_slice` Args: X (tf.tensor): X data - must be compatible with above augmentations. Y (tf.tensor): Y data - must be compatible with above augmentations. training (bool or tuple): passed to `random_slice`, or if a tuple (e.g. from sklearn.model_selection.train_test_split) then this function returns training and test sets. batch_size (int): Unused I think. max_offset (int): Passed to `random_slice` slice_ax (int): Passed to `random_slice` Returns: tf.data.Dataset(, tf.Dataset): A tensorflow dataset with extra augmentations. If training is a tuple then two datasets are returning: training set and test set. \"\"\" # TODO: trn_test as arg if isinstance ( training , tuple ): ds_train = get_tf_dataset ( X [ training [ 0 ]], Y [ training [ 0 ]], training = True , batch_size = batch_size ) ds_test = get_tf_dataset ( X [ training [ 1 ]], Y [ training [ 1 ]], training = False , batch_size = batch_size ) return ds_train , ds_test _ds = tf . data . Dataset . from_tensor_slices (( X , Y )) _ds = _ds . map ( add_depth_dim ) _ds = _ds . map ( cast_type ) slice_fun = partial ( random_slice , training = training , max_offset = max_offset , axis = slice_ax ) _ds = _ds . map ( slice_fun ) if training : _ds = _ds . shuffle () _ds = _ds . batch ( X . shape [ 0 ] + 1 , drop_remainder = not training ) return _ds","title":"get_tf_dataset()"},{"location":"API/fileio/","text":"fileio","title":"fileio"},{"location":"API/fileio/#fileio","text":"","title":"fileio"},{"location":"API/fileio/#indl.fileio","text":"","title":"indl.fileio"},{"location":"API/lfads/","text":"lfads complex_cell ComplexCell call ( self , inputs , states , training = None ) The function that contains the logic for one RNN step calculation. Parameters: Name Type Description Default inputs the input tensor, which is a slide from the overall RNN input by the time dimension (usually the second dimension). required states the state tensor from previous step, which has the same shape as (batch, state_size) . In the case of timestep 0, it will be the initial state user specified, or zero filled tensor otherwise. required Returns: Type Description A tuple of two tensors output tensor for the current timestep, with size output_size . state tensor for next step, which has the shape of state_size . Source code in indl/model/lfads/complex_cell.py def call ( self , inputs , states , training = None ): # if external inputs are used split the inputs if self . ext_input_dim > 0 : con_i = inputs [:, : - self . ext_input_dim ] ext_inputs = inputs [:, - self . ext_input_dim :] else : con_i = inputs gen_state , con_state = states [: 2 ] if self . co_dim > 0 : # if controller is used # input to the controller is (con_i and previous step's factors) prev_fac = self . dropout ( gen_state , training = training ) prev_fac = self . fac_lin ( prev_fac ) con_inputs = tf . concat ([ con_i , prev_fac ], axis = 1 ) con_inputs = self . dropout ( con_inputs , training = training ) # controller GRU recursion, get new state con_outputs , con_s_new = self . con_cell ( con_inputs , con_state , training = training ) # calculate the inputs to the generator # transformation to mean and logvar of the posterior co_mean = self . mean_lin ( con_s_new ) co_logvar = self . logvar_lin ( con_s_new ) cos_posterior = DiagonalGaussianFromExisting ( co_mean , co_logvar ) if training : # TODO: (training or \"posterior_sample_and_average\"), whatever the latter is. co_out = cos_posterior . sample else : co_out = cos_posterior . mean else : # pass zeros (0-dim) as inputs to generator co_out = tf . zeros ([ tf . shape ( input = gen_state )[ 0 ], 0 ]) con_s_new = co_mean = co_logvar = tf . zeros ([ tf . shape ( input = gen_state )[ 0 ], 0 ]) # generator's inputs if self . ext_input_dim > 0 and self . inject_ext_input_to_gen : # passing external inputs along with controller output as generator's input gen_inputs = tf . concat ([ co_out , ext_inputs ], axis = 1 ) elif self . ext_input_dim > 0 and not self . inject_ext_input_to_gen : assert 0 , \"Not Implemented!\" else : # using only controller output as generator's input gen_inputs = co_out # generator GRU recursion, get the new state gen_outputs , gen_s_new = self . gen_cell ( gen_inputs , gen_state , training = training ) # calculate the factors gen_s_new_dropped = self . dropout ( gen_s_new , training = training ) fac_s_new = self . fac_lin ( gen_s_new_dropped ) # Output the states and other values to make them available after RNN new_state = [ gen_s_new , con_s_new , co_mean , co_logvar , co_out , fac_s_new ] return new_state , new_state get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Type Description Python dictionary. Source code in indl/model/lfads/complex_cell.py def get_config ( self ): config = { 'units_gen' : self . units_gen , 'units_con' : self . units_con , 'factors_dim' : self . factors_dim , 'co_dim' : self . co_dim , 'ext_input_dim' : self . ext_input_dim , 'inject_ext_input_to_gen' : self . inject_ext_input_to_gen } base_config = super () . get_config () gru_config = self . gen_cell . get_config () return dict ( list ( base_config . items ()) + list ( gru_config . items ()) + list ( config . items ())) utils CoordinatedDropout call ( self , inputs , training = None ) This is where the layer's logic lives. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required **kwargs Additional keyword arguments. required Returns: Type Description A tensor or list/tuple of tensors. Source code in indl/model/lfads/utils.py def call ( self , inputs , training = None ): if training is None : training = K . learning_phase () def dropped_inputs (): rate = self . rate noise_shape = self . noise_shape seed = self . seed name = None with ops . name_scope ( None , \"coordinated_dropout\" , [ inputs ]) as name : is_rate_number = isinstance ( rate , numbers . Real ) if is_rate_number and ( rate < 0 or rate >= 1 ): raise ValueError ( \"rate must be a scalar tensor or a float in the \" \"range [0, 1), got %g \" % rate ) x = ops . convert_to_tensor ( inputs , name = \"x\" ) x_dtype = x . dtype if not x_dtype . is_floating : raise ValueError ( \"x has to be a floating point tensor since it's going \" \"to be scaled. Got a %s tensor instead.\" % x_dtype ) is_executing_eagerly = context . executing_eagerly () if not tensor_util . is_tensor ( rate ): if is_rate_number : keep_prob = 1 - rate scale = 1 / keep_prob scale = ops . convert_to_tensor ( scale , dtype = x_dtype ) ret = gen_math_ops . mul ( x , scale ) else : raise ValueError ( \"rate is neither scalar nor scalar tensor %r \" % rate ) else : rate . get_shape () . assert_has_rank ( 0 ) rate_dtype = rate . dtype if rate_dtype != x_dtype : if not rate_dtype . is_compatible_with ( x_dtype ): raise ValueError ( \"Tensor dtype %s is incomptaible with Tensor dtype %s : %r \" % ( x_dtype . name , rate_dtype . name , rate )) rate = gen_math_ops . cast ( rate , x_dtype , name = \"rate\" ) one_tensor = constant_op . constant ( 1 , dtype = x_dtype ) ret = gen_math_ops . real_div ( x , gen_math_ops . sub ( one_tensor , rate )) noise_shape = nn_ops . _get_noise_shape ( x , noise_shape ) # Sample a uniform distribution on [0.0, 1.0) and select values larger # than rate. # # NOTE: Random uniform can only generate 2^23 floats on [1.0, 2.0) # and subtract 1.0. random_tensor = random_ops . random_uniform ( noise_shape , seed = seed , dtype = x_dtype ) # NOTE: if (1.0 + rate) - 1 is equal to rate, then that float is selected, # hence a >= comparison is used. keep_mask = random_tensor >= rate ret = gen_math_ops . mul ( ret , gen_math_ops . cast ( keep_mask , x_dtype )) if not is_executing_eagerly : ret . set_shape ( x . get_shape ()) return ( ret , keep_mask ) output = tf_utils . smart_cond ( training , dropped_inputs , lambda : ( array_ops . identity ( inputs ), array_ops . ones_like ( inputs ) > 0 )) return output compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. required Returns: Type Description An input shape tuple. Source code in indl/model/lfads/utils.py def compute_output_shape ( self , input_shape ): return input_shape , input_shape DiagonalGaussianFromExisting Diagonal Gaussian with different constant mean and variances in each dimension. logp ( self , z = None ) Compute the log-likelihood under the distribution. Parameters: Name Type Description Default z optional value to compute likelihood for, if None, use sample. None Returns: Type Description The likelihood of z under the model. Source code in indl/model/lfads/utils.py def logp ( self , z = None ): \"\"\"Compute the log-likelihood under the distribution. Args: z (optional): value to compute likelihood for, if None, use sample. Returns: The likelihood of z under the model. \"\"\" if z is None : z = self . sample # This is needed to make sure that the gradients are simple. # The value of the function shouldn't change. if z == self . sample_bxn : return gaussian_pos_log_likelihood ( self . mean_bxn , self . logvar_bxn , self . noise_bxn ) return diag_gaussian_log_likelihood ( z , self . mean_bxn , self . logvar_bxn ) Gaussian Base class for Gaussian distribution classes. GRUClipCell build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in indl/model/lfads/utils.py def build ( self , input_shape ): super ( GRUClipCell , self ) . build ( input_shape ) # * tfkl initializes all bias as zeros by default : LFADS inits gate's to ones and candidate's to zeros # * tfkl has separate input_bias and recurrent_bias : LFADS has recurrent_bias only if self . _init_gate_bias_ones : init_weights = self . get_weights () if not self . reset_after : init_weights [ 2 ][: 2 * self . units ] = 1. else : # separate biases for input and recurrent. We only modify recurrent. init_weights [ 2 ][ 1 ][: 2 * self . units ] = 1. self . set_weights ( init_weights ) call ( self , inputs , states , training = None ) This is where the layer's logic lives. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required **kwargs Additional keyword arguments. required Returns: Type Description A tensor or list/tuple of tensors. Source code in indl/model/lfads/utils.py def call ( self , inputs , states , training = None ): h , _ = super () . call ( inputs , states , training = training ) h = tf . clip_by_value ( h , - self . _clip_value , self . _clip_value ) new_state = [ h ] if nest . is_sequence ( states ) else h return h , new_state LearnableAutoRegressive1Prior AR(1) model where autocorrelation and process variance are learned parameters. Assumed zero mean. __init__ ( self , batch_size , z_size , autocorrelation_taus , noise_variances , do_train_prior_ar_atau , do_train_prior_ar_nvar , name ) special Create a learnable autoregressive (1) process. Parameters: Name Type Description Default batch_size The size of the batch, i.e. 0th dim in 2D tensor of samples. required z_size The dimension of the distribution, i.e. 1st dim in 2D tensor. required autocorrelation_taus The auto correlation time constant of the AR(1) required noise_variances The variance of the additive noise, not the process variance. required do_train_prior_ar_atau Train or leave as constant, the autocorrelation? required do_train_prior_ar_nvar Train or leave as constant, the noise variance? required num_steps Number of steps to run the process. required name The name to prefix to learned TF variables. required Source code in indl/model/lfads/utils.py def __init__ ( self , batch_size , z_size , autocorrelation_taus , noise_variances , do_train_prior_ar_atau , do_train_prior_ar_nvar , name ): \"\"\"Create a learnable autoregressive (1) process. Args: batch_size: The size of the batch, i.e. 0th dim in 2D tensor of samples. z_size: The dimension of the distribution, i.e. 1st dim in 2D tensor. autocorrelation_taus: The auto correlation time constant of the AR(1) process. A value of 0 is uncorrelated gaussian noise. noise_variances: The variance of the additive noise, *not* the process variance. do_train_prior_ar_atau: Train or leave as constant, the autocorrelation? do_train_prior_ar_nvar: Train or leave as constant, the noise variance? num_steps: Number of steps to run the process. name: The name to prefix to learned TF variables. \"\"\" # Note the use of the plural in all of these quantities. This is intended # to mark that even though a sample z_t from the posterior is thought of a # single sample of a multidimensional gaussian, the prior is actually # thought of as U AR(1) processes, where U is the dimension of the inferred # input. size_bx1 = tf . stack ([ batch_size , 1 ]) size__xu = [ None , z_size ] # process variance, the variance at time t over all instantiations of AR(1) # with these parameters. log_evar_inits_1xu = tf . expand_dims ( tf . math . log ( noise_variances ), 0 ) self . logevars_1xu = logevars_1xu = \\ tf . Variable ( log_evar_inits_1xu , name = name + \"/logevars\" , dtype = tf . float32 , trainable = do_train_prior_ar_nvar ) self . logevars_bxu = logevars_bxu = tf . tile ( logevars_1xu , size_bx1 ) logevars_bxu . set_shape ( size__xu ) # tile loses shape # \\tau, which is the autocorrelation time constant of the AR(1) process log_atau_inits_1xu = tf . expand_dims ( tf . math . log ( autocorrelation_taus ), 0 ) self . logataus_1xu = logataus_1xu = \\ tf . Variable ( log_atau_inits_1xu , name = name + \"/logatau\" , dtype = tf . float32 , trainable = do_train_prior_ar_atau ) # phi in x_t = \\mu + phi x_tm1 + \\eps # phi = exp(-1/tau) # phi = exp(-1/exp(logtau)) # phi = exp(-exp(-logtau)) phis_1xu = tf . exp ( - tf . exp ( - logataus_1xu )) self . phis_bxu = phis_bxu = tf . tile ( phis_1xu , size_bx1 ) phis_bxu . set_shape ( size__xu ) # process noise # pvar = evar / (1- phi^2) # logpvar = log ( exp(logevar) / (1 - phi^2) ) # logpvar = logevar - log(1-phi^2) # logpvar = logevar - (log(1-phi) + log(1+phi)) self . logpvars_1xu = \\ logevars_1xu - tf . math . log ( 1.0 - phis_1xu ) - tf . math . log ( 1.0 + phis_1xu ) self . logpvars_bxu = logpvars_bxu = tf . tile ( self . logpvars_1xu , size_bx1 ) logpvars_bxu . set_shape ( size__xu ) # process mean (zero but included in for completeness) self . pmeans_bxu = pmeans_bxu = tf . zeros_like ( phis_bxu ) logp_t ( self , z_t_bxu , z_tm1_bxu = None ) Compute the log-likelihood under the distribution for a given time t, not the whole sequence. Parameters: Name Type Description Default z_t_bxu sample to compute likelihood for at time t. required z_tm1_bxu optional sample condition probability of z_t upon. None Returns: Type Description The likelihood of p_t under the model at time t. i.e. p(z_t|z_tm1_bxu) = N(z_tm1_bxu * phis, eps^2) Source code in indl/model/lfads/utils.py def logp_t ( self , z_t_bxu , z_tm1_bxu = None ): \"\"\"Compute the log-likelihood under the distribution for a given time t, not the whole sequence. Args: z_t_bxu: sample to compute likelihood for at time t. z_tm1_bxu (optional): sample condition probability of z_t upon. Returns: The likelihood of p_t under the model at time t. i.e. p(z_t|z_tm1_bxu) = N(z_tm1_bxu * phis, eps^2) \"\"\" if z_tm1_bxu is None : logp_tgtm1_bxu = diag_gaussian_log_likelihood ( z_t_bxu , self . pmeans_bxu , self . logpvars_bxu ) else : means_t_bxu = self . pmeans_bxu + self . phis_bxu * z_tm1_bxu logp_tgtm1_bxu = diag_gaussian_log_likelihood ( z_t_bxu , means_t_bxu , self . logevars_bxu ) return logp_tgtm1_bxu LearnableDiagonalGaussian Diagonal Gaussian with different means and variances in each dimension. Means and variances are optionally trainable. For LFADS ics prior, trainable_mean=True, trainable_var=False (both default). For LFADS cos prior (if not using AR1), trainable_mean=False, trainable_var=True diag_gaussian_log_likelihood ( z , mu = 0.0 , logvar = 0.0 ) Log-likelihood under a Gaussian distribution with diagonal covariance. Returns the log-likelihood for each dimension. One should sum the results for the log-likelihood under the full multidimensional model. Parameters: Name Type Description Default z The value to compute the log-likelihood. required mu The mean of the Gaussian 0.0 logvar The log variance of the Gaussian. 0.0 Returns: Type Description The log-likelihood under the Gaussian model. Source code in indl/model/lfads/utils.py def diag_gaussian_log_likelihood ( z , mu = 0.0 , logvar = 0.0 ): \"\"\"Log-likelihood under a Gaussian distribution with diagonal covariance. Returns the log-likelihood for each dimension. One should sum the results for the log-likelihood under the full multidimensional model. Args: z: The value to compute the log-likelihood. mu: The mean of the Gaussian logvar: The log variance of the Gaussian. Returns: The log-likelihood under the Gaussian model. \"\"\" return - 0.5 * ( logvar + np . log ( 2 * np . pi ) + \\ tf . square (( z - mu ) / tf . exp ( 0.5 * logvar ))) gaussian_pos_log_likelihood ( unused_mean , logvar , noise ) Gaussian log-likelihood function for a posterior in VAE Note: This function is specialized for a posterior distribution, that has the form of z = mean + sigma * noise. Parameters: Name Type Description Default unused_mean ignore required logvar The log variance of the distribution required noise The noise used in the sampling of the posterior. required Returns: Type Description The log-likelihood under the Gaussian model. Source code in indl/model/lfads/utils.py def gaussian_pos_log_likelihood ( unused_mean , logvar , noise ): \"\"\"Gaussian log-likelihood function for a posterior in VAE Note: This function is specialized for a posterior distribution, that has the form of z = mean + sigma * noise. Args: unused_mean: ignore logvar: The log variance of the distribution noise: The noise used in the sampling of the posterior. Returns: The log-likelihood under the Gaussian model. \"\"\" # ln N(z; mean, sigma) = - ln(sigma) - 0.5 ln 2pi - noise^2 / 2 return - 0.5 * ( logvar + np . log ( 2 * np . pi ) + tf . square ( noise ))","title":"lfads"},{"location":"API/lfads/#lfads","text":"","title":"lfads"},{"location":"API/lfads/#indl.model.lfads","text":"","title":"indl.model.lfads"},{"location":"API/lfads/#indl.model.lfads.complex_cell","text":"","title":"complex_cell"},{"location":"API/lfads/#indl.model.lfads.complex_cell.ComplexCell","text":"","title":"ComplexCell"},{"location":"API/lfads/#indl.model.lfads.complex_cell.ComplexCell.call","text":"The function that contains the logic for one RNN step calculation. Parameters: Name Type Description Default inputs the input tensor, which is a slide from the overall RNN input by the time dimension (usually the second dimension). required states the state tensor from previous step, which has the same shape as (batch, state_size) . In the case of timestep 0, it will be the initial state user specified, or zero filled tensor otherwise. required Returns: Type Description A tuple of two tensors output tensor for the current timestep, with size output_size . state tensor for next step, which has the shape of state_size . Source code in indl/model/lfads/complex_cell.py def call ( self , inputs , states , training = None ): # if external inputs are used split the inputs if self . ext_input_dim > 0 : con_i = inputs [:, : - self . ext_input_dim ] ext_inputs = inputs [:, - self . ext_input_dim :] else : con_i = inputs gen_state , con_state = states [: 2 ] if self . co_dim > 0 : # if controller is used # input to the controller is (con_i and previous step's factors) prev_fac = self . dropout ( gen_state , training = training ) prev_fac = self . fac_lin ( prev_fac ) con_inputs = tf . concat ([ con_i , prev_fac ], axis = 1 ) con_inputs = self . dropout ( con_inputs , training = training ) # controller GRU recursion, get new state con_outputs , con_s_new = self . con_cell ( con_inputs , con_state , training = training ) # calculate the inputs to the generator # transformation to mean and logvar of the posterior co_mean = self . mean_lin ( con_s_new ) co_logvar = self . logvar_lin ( con_s_new ) cos_posterior = DiagonalGaussianFromExisting ( co_mean , co_logvar ) if training : # TODO: (training or \"posterior_sample_and_average\"), whatever the latter is. co_out = cos_posterior . sample else : co_out = cos_posterior . mean else : # pass zeros (0-dim) as inputs to generator co_out = tf . zeros ([ tf . shape ( input = gen_state )[ 0 ], 0 ]) con_s_new = co_mean = co_logvar = tf . zeros ([ tf . shape ( input = gen_state )[ 0 ], 0 ]) # generator's inputs if self . ext_input_dim > 0 and self . inject_ext_input_to_gen : # passing external inputs along with controller output as generator's input gen_inputs = tf . concat ([ co_out , ext_inputs ], axis = 1 ) elif self . ext_input_dim > 0 and not self . inject_ext_input_to_gen : assert 0 , \"Not Implemented!\" else : # using only controller output as generator's input gen_inputs = co_out # generator GRU recursion, get the new state gen_outputs , gen_s_new = self . gen_cell ( gen_inputs , gen_state , training = training ) # calculate the factors gen_s_new_dropped = self . dropout ( gen_s_new , training = training ) fac_s_new = self . fac_lin ( gen_s_new_dropped ) # Output the states and other values to make them available after RNN new_state = [ gen_s_new , con_s_new , co_mean , co_logvar , co_out , fac_s_new ] return new_state , new_state","title":"call()"},{"location":"API/lfads/#indl.model.lfads.complex_cell.ComplexCell.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Type Description Python dictionary. Source code in indl/model/lfads/complex_cell.py def get_config ( self ): config = { 'units_gen' : self . units_gen , 'units_con' : self . units_con , 'factors_dim' : self . factors_dim , 'co_dim' : self . co_dim , 'ext_input_dim' : self . ext_input_dim , 'inject_ext_input_to_gen' : self . inject_ext_input_to_gen } base_config = super () . get_config () gru_config = self . gen_cell . get_config () return dict ( list ( base_config . items ()) + list ( gru_config . items ()) + list ( config . items ()))","title":"get_config()"},{"location":"API/lfads/#indl.model.lfads.utils","text":"","title":"utils"},{"location":"API/lfads/#indl.model.lfads.utils.CoordinatedDropout","text":"","title":"CoordinatedDropout"},{"location":"API/lfads/#indl.model.lfads.utils.CoordinatedDropout.call","text":"This is where the layer's logic lives. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required **kwargs Additional keyword arguments. required Returns: Type Description A tensor or list/tuple of tensors. Source code in indl/model/lfads/utils.py def call ( self , inputs , training = None ): if training is None : training = K . learning_phase () def dropped_inputs (): rate = self . rate noise_shape = self . noise_shape seed = self . seed name = None with ops . name_scope ( None , \"coordinated_dropout\" , [ inputs ]) as name : is_rate_number = isinstance ( rate , numbers . Real ) if is_rate_number and ( rate < 0 or rate >= 1 ): raise ValueError ( \"rate must be a scalar tensor or a float in the \" \"range [0, 1), got %g \" % rate ) x = ops . convert_to_tensor ( inputs , name = \"x\" ) x_dtype = x . dtype if not x_dtype . is_floating : raise ValueError ( \"x has to be a floating point tensor since it's going \" \"to be scaled. Got a %s tensor instead.\" % x_dtype ) is_executing_eagerly = context . executing_eagerly () if not tensor_util . is_tensor ( rate ): if is_rate_number : keep_prob = 1 - rate scale = 1 / keep_prob scale = ops . convert_to_tensor ( scale , dtype = x_dtype ) ret = gen_math_ops . mul ( x , scale ) else : raise ValueError ( \"rate is neither scalar nor scalar tensor %r \" % rate ) else : rate . get_shape () . assert_has_rank ( 0 ) rate_dtype = rate . dtype if rate_dtype != x_dtype : if not rate_dtype . is_compatible_with ( x_dtype ): raise ValueError ( \"Tensor dtype %s is incomptaible with Tensor dtype %s : %r \" % ( x_dtype . name , rate_dtype . name , rate )) rate = gen_math_ops . cast ( rate , x_dtype , name = \"rate\" ) one_tensor = constant_op . constant ( 1 , dtype = x_dtype ) ret = gen_math_ops . real_div ( x , gen_math_ops . sub ( one_tensor , rate )) noise_shape = nn_ops . _get_noise_shape ( x , noise_shape ) # Sample a uniform distribution on [0.0, 1.0) and select values larger # than rate. # # NOTE: Random uniform can only generate 2^23 floats on [1.0, 2.0) # and subtract 1.0. random_tensor = random_ops . random_uniform ( noise_shape , seed = seed , dtype = x_dtype ) # NOTE: if (1.0 + rate) - 1 is equal to rate, then that float is selected, # hence a >= comparison is used. keep_mask = random_tensor >= rate ret = gen_math_ops . mul ( ret , gen_math_ops . cast ( keep_mask , x_dtype )) if not is_executing_eagerly : ret . set_shape ( x . get_shape ()) return ( ret , keep_mask ) output = tf_utils . smart_cond ( training , dropped_inputs , lambda : ( array_ops . identity ( inputs ), array_ops . ones_like ( inputs ) > 0 )) return output","title":"call()"},{"location":"API/lfads/#indl.model.lfads.utils.CoordinatedDropout.compute_output_shape","text":"Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. required Returns: Type Description An input shape tuple. Source code in indl/model/lfads/utils.py def compute_output_shape ( self , input_shape ): return input_shape , input_shape","title":"compute_output_shape()"},{"location":"API/lfads/#indl.model.lfads.utils.DiagonalGaussianFromExisting","text":"Diagonal Gaussian with different constant mean and variances in each dimension.","title":"DiagonalGaussianFromExisting"},{"location":"API/lfads/#indl.model.lfads.utils.DiagonalGaussianFromExisting.logp","text":"Compute the log-likelihood under the distribution. Parameters: Name Type Description Default z optional value to compute likelihood for, if None, use sample. None Returns: Type Description The likelihood of z under the model. Source code in indl/model/lfads/utils.py def logp ( self , z = None ): \"\"\"Compute the log-likelihood under the distribution. Args: z (optional): value to compute likelihood for, if None, use sample. Returns: The likelihood of z under the model. \"\"\" if z is None : z = self . sample # This is needed to make sure that the gradients are simple. # The value of the function shouldn't change. if z == self . sample_bxn : return gaussian_pos_log_likelihood ( self . mean_bxn , self . logvar_bxn , self . noise_bxn ) return diag_gaussian_log_likelihood ( z , self . mean_bxn , self . logvar_bxn )","title":"logp()"},{"location":"API/lfads/#indl.model.lfads.utils.Gaussian","text":"Base class for Gaussian distribution classes.","title":"Gaussian"},{"location":"API/lfads/#indl.model.lfads.utils.GRUClipCell","text":"","title":"GRUClipCell"},{"location":"API/lfads/#indl.model.lfads.utils.GRUClipCell.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in indl/model/lfads/utils.py def build ( self , input_shape ): super ( GRUClipCell , self ) . build ( input_shape ) # * tfkl initializes all bias as zeros by default : LFADS inits gate's to ones and candidate's to zeros # * tfkl has separate input_bias and recurrent_bias : LFADS has recurrent_bias only if self . _init_gate_bias_ones : init_weights = self . get_weights () if not self . reset_after : init_weights [ 2 ][: 2 * self . units ] = 1. else : # separate biases for input and recurrent. We only modify recurrent. init_weights [ 2 ][ 1 ][: 2 * self . units ] = 1. self . set_weights ( init_weights )","title":"build()"},{"location":"API/lfads/#indl.model.lfads.utils.GRUClipCell.call","text":"This is where the layer's logic lives. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required **kwargs Additional keyword arguments. required Returns: Type Description A tensor or list/tuple of tensors. Source code in indl/model/lfads/utils.py def call ( self , inputs , states , training = None ): h , _ = super () . call ( inputs , states , training = training ) h = tf . clip_by_value ( h , - self . _clip_value , self . _clip_value ) new_state = [ h ] if nest . is_sequence ( states ) else h return h , new_state","title":"call()"},{"location":"API/lfads/#indl.model.lfads.utils.LearnableAutoRegressive1Prior","text":"AR(1) model where autocorrelation and process variance are learned parameters. Assumed zero mean.","title":"LearnableAutoRegressive1Prior"},{"location":"API/lfads/#indl.model.lfads.utils.LearnableAutoRegressive1Prior.__init__","text":"Create a learnable autoregressive (1) process. Parameters: Name Type Description Default batch_size The size of the batch, i.e. 0th dim in 2D tensor of samples. required z_size The dimension of the distribution, i.e. 1st dim in 2D tensor. required autocorrelation_taus The auto correlation time constant of the AR(1) required noise_variances The variance of the additive noise, not the process variance. required do_train_prior_ar_atau Train or leave as constant, the autocorrelation? required do_train_prior_ar_nvar Train or leave as constant, the noise variance? required num_steps Number of steps to run the process. required name The name to prefix to learned TF variables. required Source code in indl/model/lfads/utils.py def __init__ ( self , batch_size , z_size , autocorrelation_taus , noise_variances , do_train_prior_ar_atau , do_train_prior_ar_nvar , name ): \"\"\"Create a learnable autoregressive (1) process. Args: batch_size: The size of the batch, i.e. 0th dim in 2D tensor of samples. z_size: The dimension of the distribution, i.e. 1st dim in 2D tensor. autocorrelation_taus: The auto correlation time constant of the AR(1) process. A value of 0 is uncorrelated gaussian noise. noise_variances: The variance of the additive noise, *not* the process variance. do_train_prior_ar_atau: Train or leave as constant, the autocorrelation? do_train_prior_ar_nvar: Train or leave as constant, the noise variance? num_steps: Number of steps to run the process. name: The name to prefix to learned TF variables. \"\"\" # Note the use of the plural in all of these quantities. This is intended # to mark that even though a sample z_t from the posterior is thought of a # single sample of a multidimensional gaussian, the prior is actually # thought of as U AR(1) processes, where U is the dimension of the inferred # input. size_bx1 = tf . stack ([ batch_size , 1 ]) size__xu = [ None , z_size ] # process variance, the variance at time t over all instantiations of AR(1) # with these parameters. log_evar_inits_1xu = tf . expand_dims ( tf . math . log ( noise_variances ), 0 ) self . logevars_1xu = logevars_1xu = \\ tf . Variable ( log_evar_inits_1xu , name = name + \"/logevars\" , dtype = tf . float32 , trainable = do_train_prior_ar_nvar ) self . logevars_bxu = logevars_bxu = tf . tile ( logevars_1xu , size_bx1 ) logevars_bxu . set_shape ( size__xu ) # tile loses shape # \\tau, which is the autocorrelation time constant of the AR(1) process log_atau_inits_1xu = tf . expand_dims ( tf . math . log ( autocorrelation_taus ), 0 ) self . logataus_1xu = logataus_1xu = \\ tf . Variable ( log_atau_inits_1xu , name = name + \"/logatau\" , dtype = tf . float32 , trainable = do_train_prior_ar_atau ) # phi in x_t = \\mu + phi x_tm1 + \\eps # phi = exp(-1/tau) # phi = exp(-1/exp(logtau)) # phi = exp(-exp(-logtau)) phis_1xu = tf . exp ( - tf . exp ( - logataus_1xu )) self . phis_bxu = phis_bxu = tf . tile ( phis_1xu , size_bx1 ) phis_bxu . set_shape ( size__xu ) # process noise # pvar = evar / (1- phi^2) # logpvar = log ( exp(logevar) / (1 - phi^2) ) # logpvar = logevar - log(1-phi^2) # logpvar = logevar - (log(1-phi) + log(1+phi)) self . logpvars_1xu = \\ logevars_1xu - tf . math . log ( 1.0 - phis_1xu ) - tf . math . log ( 1.0 + phis_1xu ) self . logpvars_bxu = logpvars_bxu = tf . tile ( self . logpvars_1xu , size_bx1 ) logpvars_bxu . set_shape ( size__xu ) # process mean (zero but included in for completeness) self . pmeans_bxu = pmeans_bxu = tf . zeros_like ( phis_bxu )","title":"__init__()"},{"location":"API/lfads/#indl.model.lfads.utils.LearnableAutoRegressive1Prior.logp_t","text":"Compute the log-likelihood under the distribution for a given time t, not the whole sequence. Parameters: Name Type Description Default z_t_bxu sample to compute likelihood for at time t. required z_tm1_bxu optional sample condition probability of z_t upon. None Returns: Type Description The likelihood of p_t under the model at time t. i.e. p(z_t|z_tm1_bxu) = N(z_tm1_bxu * phis, eps^2) Source code in indl/model/lfads/utils.py def logp_t ( self , z_t_bxu , z_tm1_bxu = None ): \"\"\"Compute the log-likelihood under the distribution for a given time t, not the whole sequence. Args: z_t_bxu: sample to compute likelihood for at time t. z_tm1_bxu (optional): sample condition probability of z_t upon. Returns: The likelihood of p_t under the model at time t. i.e. p(z_t|z_tm1_bxu) = N(z_tm1_bxu * phis, eps^2) \"\"\" if z_tm1_bxu is None : logp_tgtm1_bxu = diag_gaussian_log_likelihood ( z_t_bxu , self . pmeans_bxu , self . logpvars_bxu ) else : means_t_bxu = self . pmeans_bxu + self . phis_bxu * z_tm1_bxu logp_tgtm1_bxu = diag_gaussian_log_likelihood ( z_t_bxu , means_t_bxu , self . logevars_bxu ) return logp_tgtm1_bxu","title":"logp_t()"},{"location":"API/lfads/#indl.model.lfads.utils.LearnableDiagonalGaussian","text":"Diagonal Gaussian with different means and variances in each dimension. Means and variances are optionally trainable. For LFADS ics prior, trainable_mean=True, trainable_var=False (both default). For LFADS cos prior (if not using AR1), trainable_mean=False, trainable_var=True","title":"LearnableDiagonalGaussian"},{"location":"API/lfads/#indl.model.lfads.utils.diag_gaussian_log_likelihood","text":"Log-likelihood under a Gaussian distribution with diagonal covariance. Returns the log-likelihood for each dimension. One should sum the results for the log-likelihood under the full multidimensional model. Parameters: Name Type Description Default z The value to compute the log-likelihood. required mu The mean of the Gaussian 0.0 logvar The log variance of the Gaussian. 0.0 Returns: Type Description The log-likelihood under the Gaussian model. Source code in indl/model/lfads/utils.py def diag_gaussian_log_likelihood ( z , mu = 0.0 , logvar = 0.0 ): \"\"\"Log-likelihood under a Gaussian distribution with diagonal covariance. Returns the log-likelihood for each dimension. One should sum the results for the log-likelihood under the full multidimensional model. Args: z: The value to compute the log-likelihood. mu: The mean of the Gaussian logvar: The log variance of the Gaussian. Returns: The log-likelihood under the Gaussian model. \"\"\" return - 0.5 * ( logvar + np . log ( 2 * np . pi ) + \\ tf . square (( z - mu ) / tf . exp ( 0.5 * logvar )))","title":"diag_gaussian_log_likelihood()"},{"location":"API/lfads/#indl.model.lfads.utils.gaussian_pos_log_likelihood","text":"Gaussian log-likelihood function for a posterior in VAE Note: This function is specialized for a posterior distribution, that has the form of z = mean + sigma * noise. Parameters: Name Type Description Default unused_mean ignore required logvar The log variance of the distribution required noise The noise used in the sampling of the posterior. required Returns: Type Description The log-likelihood under the Gaussian model. Source code in indl/model/lfads/utils.py def gaussian_pos_log_likelihood ( unused_mean , logvar , noise ): \"\"\"Gaussian log-likelihood function for a posterior in VAE Note: This function is specialized for a posterior distribution, that has the form of z = mean + sigma * noise. Args: unused_mean: ignore logvar: The log variance of the distribution noise: The noise used in the sampling of the posterior. Returns: The log-likelihood under the Gaussian model. \"\"\" # ln N(z; mean, sigma) = - ln(sigma) - 0.5 ln 2pi - noise^2 / 2 return - 0.5 * ( logvar + np . log ( 2 * np . pi ) + tf . square ( noise ))","title":"gaussian_pos_log_likelihood()"},{"location":"API/metrics/","text":"metrics dprime ( y_true , y_pred , pmarg = 0.01 , outputs = [ 'dprime' , 'bias' , 'accuracy' ]) Calculate D-Prime for binary data. 70% for both classes is d=1.0488. Highest possible is 6.93, but effectively 4.65 for 99% http://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf This function is not designed to behave as a valid 'Tensorflow metric'. Parameters: Name Type Description Default y_true array-like True labels. required y_pred array-like Predicted labels. required pmarg float 0.01 outputs List[str] list of outputs among 'dprime', 'bias', 'accuracy' ['dprime', 'bias', 'accuracy'] Returns: Type Description tuple Calculated d-prime value. Source code in indl/metrics.py def dprime ( y_true , y_pred , pmarg : float = 0.01 , outputs : List [ str ] = [ 'dprime' , 'bias' , 'accuracy' ]) -> tuple : \"\"\" Calculate D-Prime for binary data. 70% for both classes is d=1.0488. Highest possible is 6.93, but effectively 4.65 for 99% http://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf This function is not designed to behave as a valid 'Tensorflow metric'. Args: y_true (array-like): True labels. y_pred (array-like): Predicted labels. pmarg: outputs: list of outputs among 'dprime', 'bias', 'accuracy' Returns: Calculated d-prime value. \"\"\" import numpy as np from scipy.stats import norm # TODO: Adapt this function for tensorflow # y_pred = ops.convert_to_tensor(y_pred) # y_true = math_ops.cast(y_true, y_pred.dtype) # return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1) # TODO: Check that true_y only has 2 classes, and test_y is entirely within true_y classes. b_true = y_pred == y_true b_pos = np . unique ( y_true , return_inverse = True )[ 1 ] . astype ( bool ) true_pos = np . sum ( np . logical_and ( b_true , b_pos )) true_neg = np . sum ( np . logical_and ( b_true , ~ b_pos )) false_pos = np . sum ( np . logical_and ( ~ b_true , b_pos )) false_neg = np . sum ( np . logical_and ( ~ b_true , ~ b_pos )) tpr = true_pos / ( true_pos + false_neg ) tpr = max ( pmarg , min ( tpr , 1 - pmarg )) fpr = false_pos / ( false_pos + true_neg ) fpr = max ( pmarg , min ( fpr , 1 - pmarg )) ztpr = norm . ppf ( tpr , loc = 0 , scale = 1 ) zfpr = norm . ppf ( fpr , loc = 0 , scale = 1 ) # Other measures of performance: # sens = tp ./ (tp+fp) # spec = tn ./ (tn+fn) # balAcc = (sens+spec)/2 # informedness = sens+spec-1 output = tuple () for out in outputs : if out == 'dprime' : dprime = ztpr - zfpr output += ( dprime ,) elif out == 'bias' : bias = - ( ztpr + zfpr ) / 2 output += ( bias ,) elif out == 'accuracy' : accuracy = 100 * ( true_pos + true_neg ) / ( true_pos + false_pos + false_neg + true_neg ) output += ( accuracy ,) return output quickplot_history ( history ) A little helper function to do a quick plot of model fit results. Parameters: Name Type Description Default history tf.keras History required Source code in indl/metrics.py def quickplot_history ( history ) -> None : \"\"\" A little helper function to do a quick plot of model fit results. Args: history (tf.keras History): \"\"\" import matplotlib.pyplot as plt if hasattr ( history , 'history' ): history = history . history hist_metrics = [ _ for _ in history . keys () if not _ . startswith ( 'val_' )] for m_ix , m in enumerate ( hist_metrics ): plt . subplot ( len ( hist_metrics ), 1 , m_ix + 1 ) plt . plot ( history [ m ], label = 'Train' ) plt . plot ( history [ 'val_' + m ], label = 'Valid.' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( m ) plt . legend () plt . tight_layout () plt . show ()","title":"metrics"},{"location":"API/metrics/#metrics","text":"","title":"metrics"},{"location":"API/metrics/#indl.metrics","text":"","title":"indl.metrics"},{"location":"API/metrics/#indl.metrics.dprime","text":"Calculate D-Prime for binary data. 70% for both classes is d=1.0488. Highest possible is 6.93, but effectively 4.65 for 99% http://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf This function is not designed to behave as a valid 'Tensorflow metric'. Parameters: Name Type Description Default y_true array-like True labels. required y_pred array-like Predicted labels. required pmarg float 0.01 outputs List[str] list of outputs among 'dprime', 'bias', 'accuracy' ['dprime', 'bias', 'accuracy'] Returns: Type Description tuple Calculated d-prime value. Source code in indl/metrics.py def dprime ( y_true , y_pred , pmarg : float = 0.01 , outputs : List [ str ] = [ 'dprime' , 'bias' , 'accuracy' ]) -> tuple : \"\"\" Calculate D-Prime for binary data. 70% for both classes is d=1.0488. Highest possible is 6.93, but effectively 4.65 for 99% http://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf This function is not designed to behave as a valid 'Tensorflow metric'. Args: y_true (array-like): True labels. y_pred (array-like): Predicted labels. pmarg: outputs: list of outputs among 'dprime', 'bias', 'accuracy' Returns: Calculated d-prime value. \"\"\" import numpy as np from scipy.stats import norm # TODO: Adapt this function for tensorflow # y_pred = ops.convert_to_tensor(y_pred) # y_true = math_ops.cast(y_true, y_pred.dtype) # return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1) # TODO: Check that true_y only has 2 classes, and test_y is entirely within true_y classes. b_true = y_pred == y_true b_pos = np . unique ( y_true , return_inverse = True )[ 1 ] . astype ( bool ) true_pos = np . sum ( np . logical_and ( b_true , b_pos )) true_neg = np . sum ( np . logical_and ( b_true , ~ b_pos )) false_pos = np . sum ( np . logical_and ( ~ b_true , b_pos )) false_neg = np . sum ( np . logical_and ( ~ b_true , ~ b_pos )) tpr = true_pos / ( true_pos + false_neg ) tpr = max ( pmarg , min ( tpr , 1 - pmarg )) fpr = false_pos / ( false_pos + true_neg ) fpr = max ( pmarg , min ( fpr , 1 - pmarg )) ztpr = norm . ppf ( tpr , loc = 0 , scale = 1 ) zfpr = norm . ppf ( fpr , loc = 0 , scale = 1 ) # Other measures of performance: # sens = tp ./ (tp+fp) # spec = tn ./ (tn+fn) # balAcc = (sens+spec)/2 # informedness = sens+spec-1 output = tuple () for out in outputs : if out == 'dprime' : dprime = ztpr - zfpr output += ( dprime ,) elif out == 'bias' : bias = - ( ztpr + zfpr ) / 2 output += ( bias ,) elif out == 'accuracy' : accuracy = 100 * ( true_pos + true_neg ) / ( true_pos + false_pos + false_neg + true_neg ) output += ( accuracy ,) return output","title":"dprime()"},{"location":"API/metrics/#indl.metrics.quickplot_history","text":"A little helper function to do a quick plot of model fit results. Parameters: Name Type Description Default history tf.keras History required Source code in indl/metrics.py def quickplot_history ( history ) -> None : \"\"\" A little helper function to do a quick plot of model fit results. Args: history (tf.keras History): \"\"\" import matplotlib.pyplot as plt if hasattr ( history , 'history' ): history = history . history hist_metrics = [ _ for _ in history . keys () if not _ . startswith ( 'val_' )] for m_ix , m in enumerate ( hist_metrics ): plt . subplot ( len ( hist_metrics ), 1 , m_ix + 1 ) plt . plot ( history [ m ], label = 'Train' ) plt . plot ( history [ 'val_' + m ], label = 'Valid.' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( m ) plt . legend () plt . tight_layout () plt . show ()","title":"quickplot_history()"},{"location":"API/misc/","text":"misc kernels Alpha ( x , sigma , reverse = False ) Parameters: Name Type Description Default x ndarray required sigma float required reverse bool Reverse the direction of the kernel. False Returns: Type Description ndarray (x >= 0) * 2. * (x / sigma ** 2) * np.exp(-x * np.sqrt(2.) / sigma) Source code in indl/misc/kernels.py def Alpha ( x : np . ndarray , sigma : float , reverse : bool = False ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: Reverse the direction of the kernel. Returns: ```math (x >= 0) * 2. * (x / sigma ** 2) * np.exp(-x * np.sqrt(2.) / sigma) ``` \"\"\" if reverse : return ( x <= 0 ) * - 2. * ( x / sigma ** 2 ) * np . exp ( x * np . sqrt ( 2. ) / sigma ) else : return ( x >= 0 ) * 2. * ( x / sigma ** 2 ) * np . exp ( - x * np . sqrt ( 2. ) / sigma ) Boxcar ( x , sigma , reverse = None ) Parameters: Name Type Description Default x ndarray required sigma float required reverse Optional[bool] unused None Returns: Type Description ndarray (0.5 / (np.sqrt(3.0) * sigma)) * (np.abs(x) < (np.sqrt(3.0) * sigma)) Source code in indl/misc/kernels.py def Boxcar ( x : np . ndarray , sigma : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): sigma: reverse: unused Returns: ```math (0.5 / (np.sqrt(3.0) * sigma)) * (np.abs(x) < (np.sqrt(3.0) * sigma)) ``` \"\"\" return ( 0.5 / ( np . sqrt ( 3.0 ) * sigma )) * ( np . abs ( x ) < ( np . sqrt ( 3.0 ) * sigma )) Cauchy ( x , w , reverse = None ) Parameters: Name Type Description Default x ndarray required w float required reverse Optional[bool] unused None Returns: Type Description ndarray 1 / (np.pi * w * (1 + (x / w)**2)) Source code in indl/misc/kernels.py def Cauchy ( x : np . ndarray , w : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): w: reverse: unused Returns: ```math 1 / (np.pi * w * (1 + (x / w)**2)) ``` \"\"\" return 1 / ( np . pi * w * ( 1 + ( x / w ) ** 2 )) Exponential ( x , sigma , reverse = False ) Parameters: Name Type Description Default x ndarray required sigma float required reverse bool False Returns: Type Description ndarray (x >= 0) * (1. / sigma) * np.exp(-x / sigma) Source code in indl/misc/kernels.py def Exponential ( x : np . ndarray , sigma : float , reverse : bool = False ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: Returns: ```math (x >= 0) * (1. / sigma) * np.exp(-x / sigma) ``` \"\"\" if reverse : return ( x <= 0 ) * ( 1. / sigma ) * np . exp ( x / sigma ) else : return ( x >= 0 ) * ( 1. / sigma ) * np . exp ( - x / sigma ) fftkernel ( x , w ) Parameters: Name Type Description Default x ndarray required w float required Returns: Type Description ndarray Source code in indl/misc/kernels.py def fftkernel ( x : np . ndarray , w : float ) -> np . ndarray : \"\"\" Args: x: w: Returns: \"\"\" # forward padded transform L = x . size Lmax = L + 3 * w n = int ( 2 ** np . ceil ( np . log2 ( Lmax ))) X = np . fft . fft ( x , n ) # generate kernel domain f = np . linspace ( 0 , n - 1 , n ) / n f = np . concatenate (( - f [ 0 : np . int ( n / 2 + 1 )], f [ 1 : np . int ( n / 2 - 1 + 1 )][:: - 1 ])) # evaluate kernel K = np . exp ( - 0.5 * ( w * 2 * np . pi * f ) ** 2 ) # convolve and transform back from frequency domain y = np . real ( np . fft . ifft ( X * K , n )) y = y [ 0 : L ] return y fftkernelWin ( x , w , WinFunc ) Parameters: Name Type Description Default x ndarray data required w float kernel parameter required WinFunc str 'Boxcar', 'Laplace', 'Cauchy', 'Gauss' (default) required Returns: Type Description ndarray Source code in indl/misc/kernels.py def fftkernelWin ( x : np . ndarray , w : float , WinFunc : str ) -> np . ndarray : \"\"\" Args: x: data w: kernel parameter WinFunc: 'Boxcar', 'Laplace', 'Cauchy', 'Gauss' (default) Returns: \"\"\" # forward padded transform L = x . size Lmax = L + 3 * w n = 2 ** np . ceil ( np . log2 ( Lmax )) X = np . fft . fft ( x , n . astype ( np . int )) # generate kernel domain f = np . linspace ( 0 , n - 1 , n ) / n f = np . concatenate (( - f [ 0 : np . int ( n / 2 + 1 )], f [ 1 : np . int ( n / 2 - 1 + 1 )][:: - 1 ])) t = 2 * np . pi * f # determine window function - evaluate kernel if WinFunc == 'Boxcar' : a = 12 ** 0.5 * w K = 2 * np . sin ( a * t / 2 ) / ( a * t ) K [ 0 ] = 1 elif WinFunc == 'Laplace' : K = 1 / ( 1 + ( w * 2 * np . pi * f ) ** 2 / 2 ) elif WinFunc == 'Cauchy' : K = np . exp ( - w * np . abs ( 2 * np . pi * f )) else : # WinFunc == 'Gauss' K = np . exp ( - 0.5 * ( w * 2 * np . pi * f ) ** 2 ) # convolve and transform back from frequency domain y = np . real ( np . fft . ifft ( X * K , n )) y = y [ 0 : L ] return y Gauss ( x , sigma , reverse = None ) Parameters: Name Type Description Default x ndarray required sigma float required reverse Optional[bool] unused None Returns: Type Description ndarray (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * (x / sigma) ** 2) Source code in indl/misc/kernels.py def Gauss ( x : np . ndarray , sigma : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: unused Returns: ```math (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * (x / sigma) ** 2) ``` \"\"\" return ( 1.0 / ( np . sqrt ( 2.0 * np . pi ) * sigma )) * np . exp ( - 0.5 * ( x / sigma ) ** 2 ) ilogexp ( x ) Parameters: Name Type Description Default x ndarray required Returns: Type Description ndarray Source code in indl/misc/kernels.py def ilogexp ( x : np . ndarray ) -> np . ndarray : \"\"\" Args: x: Returns: \"\"\" # TODO: Check dtype and convert x scalars to numpy array y = np . zeros ( x . shape ) y [ x < 1e2 ] = np . log ( np . exp ( x [ x < 1e2 ]) - 1 ) y [ x >= 1e2 ] = x [ x >= 1e2 ] return y Laplace ( x , w , reverse = None ) Parameters: Name Type Description Default x ndarray required w float required reverse Optional[bool] unused None Returns: Type Description ndarray 1 / 2**0.5 / w * np.exp(-(2**0.5) / w / np.abs(x)) Source code in indl/misc/kernels.py def Laplace ( x : np . ndarray , w : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): w: reverse: unused Returns: ```math 1 / 2**0.5 / w * np.exp(-(2**0.5) / w / np.abs(x)) ``` \"\"\" return 1 / 2 ** 0.5 / w * np . exp ( - ( 2 ** 0.5 ) / w / np . abs ( x )) logexp ( x ) Parameters: Name Type Description Default x ndarray required Returns: Type Description ndarray Source code in indl/misc/kernels.py def logexp ( x : np . ndarray ) -> np . ndarray : \"\"\" Args: x: Returns: \"\"\" # TODO: Check dtype and convert x scalars to numpy array y = np . zeros ( x . shape ) y [ x < 1e2 ] = np . log ( 1 + np . exp ( x [ x < 1e2 ])) y [ x >= 1e2 ] = x [ x >= 1e2 ] return y sshist ( x , N = range ( 2 , 501 ), SN = 30 ) Returns the optimal number of bins in a histogram used for density estimation. Optimization principle is to minimize expected L2 loss function between the histogram and an unknown underlying density function. An assumption made is merely that samples are drawn from the density independently each other. The optimal binwidth D* is obtained as a minimizer of the formula, (2K-V) / D^2, where K and V are mean and variance of sample counts across bins with width D. Optimal number of bins is given as (max(x) - min(x)) / D. Parameters x : array_like One-dimensional data to fit histogram to. N : array_like, optional Array containing number of histogram bins to evaluate for fit. Default value = 500. SN : double, optional Scalar natural number defining number of bins for shift-averaging. Returns optN : int Optimal number of bins to represent the data in X N : double Maximum number of bins to be evaluated. Default value = 500. C : array_like Cost function C[i] of evaluating histogram fit with N[i] bins See Also sskernel, ssvkernel References .. [1] H. Shimazaki and S. Shinomoto, \"A method for selecting the bin size of a time histogram,\" in Neural Computation 19(6), 1503-1527, 2007 http://dx.doi.org/10.1162/neco.2007.19.6.1503 Source code in indl/misc/kernels.py def sshist ( x : np . ndarray , N = range ( 2 , 501 ), SN : int = 30 ) -> Tuple [ int , float , np . ndarray ]: \"\"\" Returns the optimal number of bins in a histogram used for density estimation. Optimization principle is to minimize expected L2 loss function between the histogram and an unknown underlying density function. An assumption made is merely that samples are drawn from the density independently each other. The optimal binwidth D* is obtained as a minimizer of the formula, (2K-V) / D^2, where K and V are mean and variance of sample counts across bins with width D. Optimal number of bins is given as (max(x) - min(x)) / D. Parameters ---------- x : array_like One-dimensional data to fit histogram to. N : array_like, optional Array containing number of histogram bins to evaluate for fit. Default value = 500. SN : double, optional Scalar natural number defining number of bins for shift-averaging. Returns ------- optN : int Optimal number of bins to represent the data in X N : double Maximum number of bins to be evaluated. Default value = 500. C : array_like Cost function C[i] of evaluating histogram fit with N[i] bins See Also -------- sskernel, ssvkernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"A method for selecting the bin size of a time histogram,\" in Neural Computation 19(6), 1503-1527, 2007 http://dx.doi.org/10.1162/neco.2007.19.6.1503 \"\"\" # determine range of input 'x' x_min = np . min ( x ) x_max = np . max ( x ) # get smallest difference 'dx' between all pairwise samples buf = np . abs ( np . diff ( np . sort ( x ))) dx = min ( buf [ buf > 0 ]) # setup bins to evaluate N_MIN = 2 N_MAX = min ( np . floor (( x_max - x_min ) / ( 2 * dx )), max ( N )) N = range ( N_MIN , N_MAX + 1 ) D = ( x_max - x_min ) / N # compute cost function over each possible number of bins Cs = np . zeros (( len ( N ), SN )) for i , n in enumerate ( N ): # loop over number of bins shift = np . linspace ( 0 , D [ i ], SN ) for p , sh in enumerate ( shift ): # loop over shift window positions # define bin edges edges = np . linspace ( x_min + sh - D [ i ] / 2 , x_max + sh - D [ i ] / 2 , N [ i ] + 1 ) # count number of events in these bins ki = np . histogram ( x , edges ) # get mean and variance of events k = ki [ 0 ] . mean () v = np . sum (( ki [ 0 ] - k ) ** 2 ) / N [ i ] Cs [ i , p ] = ( 2 * k - v ) / D [ i ] ** 2 # average over shift window C = Cs . mean ( axis = 1 ) # get bin count that minimizes cost C idx = np . argmin ( C ) optN = N [ idx ] optD = D [ idx ] edges = np . linspace ( x_min , x_max , optN ) return optN , optD , edges , C , N sskernel ( x , tin = None , W = None , nbs = 1000.0 ) Generates a kernel density estimate with globally-optimized bandwidth. The optimal bandwidth is obtained as a minimizer of the formula, sum_{i,j} \\int k(x - x_i) k(x - x_j) dx - 2 sum_{i~=j} k(x_i - x_j), where k(x) is the kernel function. Parameters x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. W : array_like, optional The kernel bandwidths to use in optimization. Should not be chosen smaller than the sampling resolution of 'x'. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y' Returns y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : double The optimal global kernel bandwidth. W : array_like The kernel bandwidths evaluated during optimization. C : array_like The cost functions associated with the bandwidths 'W'. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also sshist, ssvkernel References .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 Source code in indl/misc/kernels.py def sskernel ( x : np . ndarray , tin : Optional [ np . ndarray ] = None , W : Optional [ np . ndarray ] = None , nbs : Optional [ int ] = 1e3 ) -> Tuple [ np . ndarray , np . ndarray , float , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Generates a kernel density estimate with globally-optimized bandwidth. The optimal bandwidth is obtained as a minimizer of the formula, sum_{i,j} \\int k(x - x_i) k(x - x_j) dx - 2 sum_{i~=j} k(x_i - x_j), where k(x) is the kernel function. Parameters ---------- x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. W : array_like, optional The kernel bandwidths to use in optimization. Should not be chosen smaller than the sampling resolution of 'x'. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y' Returns ------- y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : double The optimal global kernel bandwidth. W : array_like The kernel bandwidths evaluated during optimization. C : array_like The cost functions associated with the bandwidths 'W'. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also -------- sshist, ssvkernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 \"\"\" def CostFunction ( y_hist , N , w , dt ): # build normal smoothing kernel yh = fftkernel ( y_hist , w / dt ) # formula for density C = np . sum ( yh ** 2 ) * dt - 2 * np . sum ( yh * y_hist ) * dt + 2 \\ / ( 2 * np . pi ) ** 0.5 / w / N C = C * N ** 2 return C , yh T = np . max ( x ) - np . min ( x ) # Total span of input spike times dx = np . sort ( np . diff ( np . sort ( x ))) # Sorted ISIs dt_samp = dx [ np . nonzero ( dx )][ 0 ] # Smallest nonzero ISI # set argument 't' if not provided if tin is None : tin = np . linspace ( np . min ( x ), np . max ( x ), int ( min ( np . ceil ( T / dt_samp ), 1e3 ))) t = tin else : if dt_samp > min ( np . diff ( tin )): t = np . linspace ( min ( tin ), max ( tin ), int ( min ( np . ceil ( T / dt_samp ), 1e3 ))) else : t = tin x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] # Spike times that fall within tin # calculate delta t dt = min ( np . diff ( t )) # create the finest histogram thist_edges = np . r_ [ t - dt / 2 , t [ - 1 ] + dt / 2 ] y_hist , _ = np . histogram ( x_ab , thist_edges ) N = sum ( y_hist ) . astype ( np . float ) y_hist = y_hist / ( N * dt ) # density # global search if input 'W' is defined if W is not None : C = np . zeros (( 1 , len ( W ))) C_min = np . Inf for k , w in enumerate ( W ): C [ k ], yh = CostFunction ( y_hist , N , w , dt ) if ( C [ k ] < C_min ): C_min = C [ k ] optw = w y = yh else : # optimized search using golden section k = 0 C = np . zeros (( 20 , 1 )) W = np . zeros (( 20 , 1 )) Wmin = 2 * dt Wmax = np . max ( x ) - np . min ( x ) tol = 10e-5 phi = ( 5 ** 0.5 + 1 ) / 2 a = ilogexp ( Wmin ) b = ilogexp ( Wmax ) c1 = ( phi - 1 ) * a + ( 2 - phi ) * b c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 , dummy = CostFunction ( y_hist , N , logexp ( c1 ), dt ) f2 , dummy = CostFunction ( y_hist , N , logexp ( c2 ), dt ) while ( np . abs ( b - a ) > tol * ( np . abs ( c1 ) + np . abs ( c2 ))) & ( k < 20 ): if f1 < f2 : b = c2 c2 = c1 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b f2 = f1 f1 , yh1 = CostFunction ( y_hist , N , logexp ( c1 ), dt ) W [ k ] = logexp ( c1 ) C [ k ] = f1 optw = logexp ( c1 ) y = yh1 / np . sum ( yh1 * dt ) else : a = c1 c1 = c2 c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = f2 f2 , yh2 = CostFunction ( y_hist , N , logexp ( c2 ), dt ) W [ k ] = logexp ( c2 ) C [ k ] = f2 optw = logexp ( c2 ) y = yh2 / np . sum ( yh2 * dt ) # increment iteration counter k = k + 1 # discard unused entries in gs, C C = C [ 0 : k ] W = W [ 0 : k ] # estimate confidence intervals by bootstrapping yb = None confb95 = None if nbs > 0 : nbs = np . asarray ( nbs ) yb = np . zeros (( nbs , len ( tin ))) for i in range ( nbs ): idx = np . random . randint ( 0 , len ( x_ab ) - 1 , len ( x_ab )) xb = x_ab [ idx ] thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_histb = np . histogram ( xb , thist - dt / 2 )[ 0 ] / dt / N yb_buf = fftkernel ( y_histb , optw / dt ) yb_buf = yb_buf / np . sum ( yb_buf * dt ) yb [ i , ] = np . interp ( tin , t , yb_buf ) ybsort = np . sort ( yb , axis = 0 ) y95b = ybsort [ np . int ( np . floor ( 0.05 * nbs )), :] y95u = ybsort [ np . int ( np . floor ( 0.95 * nbs )), :] confb95 = np . concatenate (( y95b [ np . newaxis ], y95u [ np . newaxis ]), axis = 0 ) # return outputs y = np . interp ( tin , t , y ) t = tin return y , t , optw , W , C , confb95 , yb ssvkernel ( x , tin = None , M = 80 , nbs = 100.0 , WinFunc = 'Boxcar' ) Generates a locally adaptive kernel-density estimate for one-dimensional data. The user provides a one-dimensional vector of samples drawn from some underlying unknown distribution, and optionally the values where they want to estimate the probability density of that distribution. The algorithm solves an optimization problem to identify variable bandwidths across the domain where the data is provided. The optimization is based on a principle of minimizing expected L2 loss function between the kernel estimate and an unknown underlying density function. An assumption is merely that samples are drawn from the density independently of each other. The locally adaptive bandwidth is obtained by iteratively computing optimal fixed-size bandwidths wihtihn local intervals. The optimal bandwidths are selected such that they are selected in the intervals that are gamma times larger than the optimal bandwidths themselves. The paramter gamma is optimized by minimizing the L2 risk estimate. Parameters x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. Default value = None. M : int, optional The number of window sizes to evaluate. Default value = 80. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y'. WinFunc : string, optional The type of window function to use in estimating local bandwidth. Choose from one of 'Boxcar', 'Laplace', 'Cauchy' and 'Gauss'. Default value = 'Gauss'. Returns y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : array_like The optimal local kernel bandwidths at 't'. gs : array_like The stiffness constants of the variables bandwidths evaluated. C : array_like Cost functions associated with stiffness constraints. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also sshist, sskernel References .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 Source code in indl/misc/kernels.py def ssvkernel ( x : np . ndarray , tin : Optional [ np . ndarray ] = None , M : int = 80 , nbs : int = 1e2 , WinFunc : str = 'Boxcar' ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Generates a locally adaptive kernel-density estimate for one-dimensional data. The user provides a one-dimensional vector of samples drawn from some underlying unknown distribution, and optionally the values where they want to estimate the probability density of that distribution. The algorithm solves an optimization problem to identify variable bandwidths across the domain where the data is provided. The optimization is based on a principle of minimizing expected L2 loss function between the kernel estimate and an unknown underlying density function. An assumption is merely that samples are drawn from the density independently of each other. The locally adaptive bandwidth is obtained by iteratively computing optimal fixed-size bandwidths wihtihn local intervals. The optimal bandwidths are selected such that they are selected in the intervals that are gamma times larger than the optimal bandwidths themselves. The paramter gamma is optimized by minimizing the L2 risk estimate. Parameters ---------- x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. Default value = None. M : int, optional The number of window sizes to evaluate. Default value = 80. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y'. WinFunc : string, optional The type of window function to use in estimating local bandwidth. Choose from one of 'Boxcar', 'Laplace', 'Cauchy' and 'Gauss'. Default value = 'Gauss'. Returns ------- y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : array_like The optimal local kernel bandwidths at 't'. gs : array_like The stiffness constants of the variables bandwidths evaluated. C : array_like Cost functions associated with stiffness constraints. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also -------- sshist, sskernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 \"\"\" def CostFunction ( y_hist , N , t , dt , optws , WIN , WinFunc , g ): L = y_hist . size optwv = np . zeros (( L ,)) for k in range ( L ): gs = optws [:, k ] / WIN if g > np . max ( gs ): optwv [ k ] = np . min ( WIN ) else : if g < min ( gs ): optwv [ k ] = np . max ( WIN ) else : idx = np . max ( np . nonzero ( gs >= g )) optwv [ k ] = g * WIN [ idx ] # Nadaraya-Watson kernel regression optwp = np . zeros (( L ,)) for k in range ( L ): if WinFunc == 'Boxcar' : Z = Boxcar ( t [ k ] - t , optwv / g ) elif WinFunc == 'Laplace' : Z = Laplace ( t [ k ] - t , optwv / g ) elif WinFunc == 'Cauchy' : Z = Cauchy ( t [ k ] - t , optwv / g ) else : # WinFunc == 'Gauss' Z = Gauss ( t [ k ] - t , optwv / g ) optwp [ k ] = np . sum ( optwv * Z ) / np . sum ( Z ) # speed-optimized baloon estimator idx = y_hist . nonzero () y_hist_nz = y_hist [ idx ] t_nz = t [ idx ] yv = np . zeros (( L ,)) for k in range ( L ): yv [ k ] = np . sum ( y_hist_nz * dt * Gauss ( t [ k ] - t_nz , optwp [ k ])) yv = yv * N / np . sum ( yv * dt ) # cost function of estimated kernel cg = yv ** 2 - 2 * yv * y_hist + 2 / ( 2 * np . pi ) ** 0.5 / optwp * y_hist Cg = np . sum ( cg * dt ) return Cg , yv , optwp # set argument 't' if not provided if tin is None : T = np . max ( x ) - np . min ( x ) dx = np . sort ( np . diff ( np . sort ( x ))) dt_samp = dx [ np . nonzero ( dx )][ 0 ] tin = np . linspace ( np . min ( x ), np . max ( x ), min ( np . ceil ( T / dt_samp ), 1e3 )) t = tin x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] else : T = np . max ( x ) - np . min ( x ) x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] dx = np . sort ( np . diff ( np . sort ( x ))) dt_samp = dx [ np . nonzero ( dx )][ 0 ] if dt_samp > min ( np . diff ( tin )): t = np . linspace ( min ( tin ), max ( tin ), min ( np . ceil ( T / dt_samp ), 1e3 )) else : t = tin # calculate delta t dt = min ( np . diff ( t )) # create the finest histogram thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_hist = np . histogram ( x_ab , thist - dt / 2 )[ 0 ] / dt L = y_hist . size N = sum ( y_hist * dt ) . astype ( np . float ) # initialize window sizes W = logexp ( np . linspace ( ilogexp ( 5 * dt ), ilogexp ( T ), M )) # compute local cost functions c = np . zeros (( M , L )) for j in range ( M ): w = W [ j ] yh = fftkernel ( y_hist , w / dt ) c [ j , :] = yh ** 2 - 2 * yh * y_hist + 2 / ( 2 * np . pi ) ** 0.5 / w * y_hist # initialize optimal ws optws = np . zeros (( M , L )) for i in range ( M ): Win = W [ i ] C_local = np . zeros (( M , L )) for j in range ( M ): C_local [ j , :] = fftkernelWin ( c [ j , :], Win / dt , WinFunc ) n = np . argmin ( C_local , axis = 0 ) optws [ i , :] = W [ n ] # golden section search for stiffness parameter of variable bandwidths k = 0 gs = np . zeros (( 30 , 1 )) C = np . zeros (( 30 , 1 )) tol = 1e-5 a = 1e-12 b = 1 phi = ( 5 ** 0.5 + 1 ) / 2 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c1 )[ 0 ] f2 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c2 )[ 0 ] while ( np . abs ( b - a ) > tol * ( abs ( c1 ) + abs ( c2 ))) & ( k < 30 ): if f1 < f2 : b = c2 c2 = c1 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b f2 = f1 f1 , yv1 , optwp1 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c1 ) yopt = yv1 / np . sum ( yv1 * dt ) optw = optwp1 else : a = c1 c1 = c2 c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = f2 f2 , yv2 , optwp2 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c2 ) yopt = yv2 / np . sum ( yv2 * dt ) optw = optwp2 # capture estimates and increment iteration counter gs [ k ] = c1 C [ k ] = f1 k = k + 1 # discard unused entries in gs, C gs = gs [ 0 : k ] C = C [ 0 : k ] # estimate confidence intervals by bootstrapping nbs = np . asarray ( nbs ) yb = np . zeros (( nbs , tin . size )) for i in range ( nbs ): Nb = np . random . poisson ( lam = N ) idx = np . random . randint ( 0 , N , Nb ) xb = x_ab [ idx ] thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_histb = np . histogram ( xb , thist - dt / 2 )[ 0 ] idx = y_histb . nonzero () y_histb_nz = y_histb [ idx ] t_nz = t [ idx ] yb_buf = np . zeros (( L , )) for k in range ( L ): yb_buf [ k ] = np . sum ( y_histb_nz * Gauss ( t [ k ] - t_nz , optw [ k ])) / Nb yb_buf = yb_buf / np . sum ( yb_buf * dt ) yb [ i , :] = np . interp ( tin , t , yb_buf ) ybsort = np . sort ( yb , axis = 0 ) y95b = ybsort [ np . int ( np . floor ( 0.05 * nbs )), :] y95u = ybsort [ np . int ( np . floor ( 0.95 * nbs )), :] confb95 = np . concatenate (( y95b [ np . newaxis ], y95u [ np . newaxis ]), axis = 0 ) # return outputs y = np . interp ( tin , t , yopt ) optw = np . interp ( tin , t , optw ) t = tin return y , t , optw , gs , C , confb95 , yb sigfuncs minimum_jerk ( x , a0 = 0 , af = 1 , degree = 1 , duration = None ) A 1-D trajectory that minimizes jerk (3rd time-derivative of position). A minimum jerk trajectory is considered \"smooth\" and to be a feature of natural limb movements. https://storage.googleapis.com/wzukusers/user-31382847/documents/5a7253343814f4Iv6Hnt/minimumjerk.pdf A minimum-jerk trajectory is defined by: trajectory = a0 + (af - a0) * (10(dx^3) - 15(dx^4) + 6(dx^5)) where a0 is the resting position, and af is the finishing position. duration is assumed to be the extent of x but it may be overidden. Parameters: Name Type Description Default x ndarray required a0 0 af 1 degree 1 duration float Total duration of x in seconds. If None (default), calculated duration from x. None Returns: Type Description ndarray a0 + (af - a0) * (10 (dx.^3) - 15 (dx.^4) + 6*(dx.^5)) Source code in indl/misc/sigfuncs.py def minimum_jerk ( x : np . ndarray , a0 = 0 , af = 1 , degree = 1 , duration = None ) -> np . ndarray : r \"\"\" A 1-D trajectory that minimizes jerk (3rd time-derivative of position). A minimum jerk trajectory is considered \"smooth\" and to be a feature of natural limb movements. https://storage.googleapis.com/wzukusers/user-31382847/documents/5a7253343814f4Iv6Hnt/minimumjerk.pdf A minimum-jerk trajectory is defined by: ```math trajectory = a0 + (af - a0) * (10(dx^3) - 15(dx^4) + 6(dx^5)) ``` where a0 is the resting position, and af is the finishing position. duration is assumed to be the extent of x but it may be overidden. Args: x: a0: af: degree: duration (float): Total duration of x in seconds. If None (default), calculated duration from x. Returns: a0 + (af - a0) * (10*(dx.^3) - 15*(dx.^4) + 6*(dx.^5)) \"\"\" x = np . array ( x )[:, None ] assert ( x . size == x . shape [ 0 ]), f \"x must be 1D trajectory, found { x . shape } \" a0 = np . atleast_2d ( a0 ) af = np . atleast_2d ( af ) if duration is None : sorted_x = np . sort ( x , axis = 0 ) dx = np . diff ( x , axis = 0 ) duration = sorted_x [ - 1 ] - sorted_x [ 0 ] + np . min ( dx ) dx = x / duration if degree not in [ 1 , 2 ]: # Default - no derivative k0 = a0 x1 = 10 * dx ** 3 x2 = - 15 * dx ** 4 x3 = 6 * dx ** 5 elif degree == 1 : # Velocity k0 = np . zeros_like ( a0 ) x1 = 30 * dx ** 2 x2 = - 60 * dx ** 3 x3 = 30 * dx ** 4 elif degree == 2 : # Acceleration k0 = np . zeros_like ( a0 ) x1 = 60 * dx x2 = - 180 * dx ** 2 x3 = 120 * dx ** 3 return k0 + ( af - a0 ) * ( x1 + x2 + x3 ) sigmoid ( x , A = 0 , K = 1 , C = 1 , Q = 1 , B = 1 , v = 1 , x_offset = 0 ) Returns f(x) = A + (K - A) / ((C + Q * np.exp(-B (x - x_offset))) *(1/v)) This is a generalized logistic function. The default arguments reduce this to a simple sigmoid: f(x) = 1 / (1 + np.exp(-x)) Parameters: Name Type Description Default x np.ndarray required A float 0 K float 1 C float 1 Q float 1 B float 1 v float 1 x_offset 0 Returns: Type Description np.ndarray A + (K - A) / ((C + Q * np.exp(-B (x - x_offset))) *(1/v)) Source code in indl/misc/sigfuncs.py def sigmoid ( x , A = 0 , K = 1 , C = 1 , Q = 1 , B = 1 , v = 1 , x_offset = 0 ): \"\"\" Returns f(x) = A + (K - A) / ((C + Q * np.exp(-B*(x - x_offset)))**(1/v)) This is a generalized logistic function. The default arguments reduce this to a simple sigmoid: f(x) = 1 / (1 + np.exp(-x)) Args: x (np.ndarray): A (float): K (float): C (float): Q (float): B (float): v (float): x_offset: Returns: np.ndarray: A + (K - A) / ((C + Q * np.exp(-B*(x - x_offset)))**(1/v)) \"\"\" y = A + ( K - A ) / (( C + Q * np . exp ( - B * ( x - x_offset ))) ** ( 1 / v )) return y","title":"misc"},{"location":"API/misc/#misc","text":"","title":"misc"},{"location":"API/misc/#indl.misc","text":"","title":"indl.misc"},{"location":"API/misc/#indl.misc.kernels","text":"","title":"kernels"},{"location":"API/misc/#indl.misc.kernels.Alpha","text":"Parameters: Name Type Description Default x ndarray required sigma float required reverse bool Reverse the direction of the kernel. False Returns: Type Description ndarray (x >= 0) * 2. * (x / sigma ** 2) * np.exp(-x * np.sqrt(2.) / sigma) Source code in indl/misc/kernels.py def Alpha ( x : np . ndarray , sigma : float , reverse : bool = False ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: Reverse the direction of the kernel. Returns: ```math (x >= 0) * 2. * (x / sigma ** 2) * np.exp(-x * np.sqrt(2.) / sigma) ``` \"\"\" if reverse : return ( x <= 0 ) * - 2. * ( x / sigma ** 2 ) * np . exp ( x * np . sqrt ( 2. ) / sigma ) else : return ( x >= 0 ) * 2. * ( x / sigma ** 2 ) * np . exp ( - x * np . sqrt ( 2. ) / sigma )","title":"Alpha()"},{"location":"API/misc/#indl.misc.kernels.Boxcar","text":"Parameters: Name Type Description Default x ndarray required sigma float required reverse Optional[bool] unused None Returns: Type Description ndarray (0.5 / (np.sqrt(3.0) * sigma)) * (np.abs(x) < (np.sqrt(3.0) * sigma)) Source code in indl/misc/kernels.py def Boxcar ( x : np . ndarray , sigma : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): sigma: reverse: unused Returns: ```math (0.5 / (np.sqrt(3.0) * sigma)) * (np.abs(x) < (np.sqrt(3.0) * sigma)) ``` \"\"\" return ( 0.5 / ( np . sqrt ( 3.0 ) * sigma )) * ( np . abs ( x ) < ( np . sqrt ( 3.0 ) * sigma ))","title":"Boxcar()"},{"location":"API/misc/#indl.misc.kernels.Cauchy","text":"Parameters: Name Type Description Default x ndarray required w float required reverse Optional[bool] unused None Returns: Type Description ndarray 1 / (np.pi * w * (1 + (x / w)**2)) Source code in indl/misc/kernels.py def Cauchy ( x : np . ndarray , w : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): w: reverse: unused Returns: ```math 1 / (np.pi * w * (1 + (x / w)**2)) ``` \"\"\" return 1 / ( np . pi * w * ( 1 + ( x / w ) ** 2 ))","title":"Cauchy()"},{"location":"API/misc/#indl.misc.kernels.Exponential","text":"Parameters: Name Type Description Default x ndarray required sigma float required reverse bool False Returns: Type Description ndarray (x >= 0) * (1. / sigma) * np.exp(-x / sigma) Source code in indl/misc/kernels.py def Exponential ( x : np . ndarray , sigma : float , reverse : bool = False ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: Returns: ```math (x >= 0) * (1. / sigma) * np.exp(-x / sigma) ``` \"\"\" if reverse : return ( x <= 0 ) * ( 1. / sigma ) * np . exp ( x / sigma ) else : return ( x >= 0 ) * ( 1. / sigma ) * np . exp ( - x / sigma )","title":"Exponential()"},{"location":"API/misc/#indl.misc.kernels.fftkernel","text":"Parameters: Name Type Description Default x ndarray required w float required Returns: Type Description ndarray Source code in indl/misc/kernels.py def fftkernel ( x : np . ndarray , w : float ) -> np . ndarray : \"\"\" Args: x: w: Returns: \"\"\" # forward padded transform L = x . size Lmax = L + 3 * w n = int ( 2 ** np . ceil ( np . log2 ( Lmax ))) X = np . fft . fft ( x , n ) # generate kernel domain f = np . linspace ( 0 , n - 1 , n ) / n f = np . concatenate (( - f [ 0 : np . int ( n / 2 + 1 )], f [ 1 : np . int ( n / 2 - 1 + 1 )][:: - 1 ])) # evaluate kernel K = np . exp ( - 0.5 * ( w * 2 * np . pi * f ) ** 2 ) # convolve and transform back from frequency domain y = np . real ( np . fft . ifft ( X * K , n )) y = y [ 0 : L ] return y","title":"fftkernel()"},{"location":"API/misc/#indl.misc.kernels.fftkernelWin","text":"Parameters: Name Type Description Default x ndarray data required w float kernel parameter required WinFunc str 'Boxcar', 'Laplace', 'Cauchy', 'Gauss' (default) required Returns: Type Description ndarray Source code in indl/misc/kernels.py def fftkernelWin ( x : np . ndarray , w : float , WinFunc : str ) -> np . ndarray : \"\"\" Args: x: data w: kernel parameter WinFunc: 'Boxcar', 'Laplace', 'Cauchy', 'Gauss' (default) Returns: \"\"\" # forward padded transform L = x . size Lmax = L + 3 * w n = 2 ** np . ceil ( np . log2 ( Lmax )) X = np . fft . fft ( x , n . astype ( np . int )) # generate kernel domain f = np . linspace ( 0 , n - 1 , n ) / n f = np . concatenate (( - f [ 0 : np . int ( n / 2 + 1 )], f [ 1 : np . int ( n / 2 - 1 + 1 )][:: - 1 ])) t = 2 * np . pi * f # determine window function - evaluate kernel if WinFunc == 'Boxcar' : a = 12 ** 0.5 * w K = 2 * np . sin ( a * t / 2 ) / ( a * t ) K [ 0 ] = 1 elif WinFunc == 'Laplace' : K = 1 / ( 1 + ( w * 2 * np . pi * f ) ** 2 / 2 ) elif WinFunc == 'Cauchy' : K = np . exp ( - w * np . abs ( 2 * np . pi * f )) else : # WinFunc == 'Gauss' K = np . exp ( - 0.5 * ( w * 2 * np . pi * f ) ** 2 ) # convolve and transform back from frequency domain y = np . real ( np . fft . ifft ( X * K , n )) y = y [ 0 : L ] return y","title":"fftkernelWin()"},{"location":"API/misc/#indl.misc.kernels.Gauss","text":"Parameters: Name Type Description Default x ndarray required sigma float required reverse Optional[bool] unused None Returns: Type Description ndarray (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * (x / sigma) ** 2) Source code in indl/misc/kernels.py def Gauss ( x : np . ndarray , sigma : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x: sigma: reverse: unused Returns: ```math (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * (x / sigma) ** 2) ``` \"\"\" return ( 1.0 / ( np . sqrt ( 2.0 * np . pi ) * sigma )) * np . exp ( - 0.5 * ( x / sigma ) ** 2 )","title":"Gauss()"},{"location":"API/misc/#indl.misc.kernels.ilogexp","text":"Parameters: Name Type Description Default x ndarray required Returns: Type Description ndarray Source code in indl/misc/kernels.py def ilogexp ( x : np . ndarray ) -> np . ndarray : \"\"\" Args: x: Returns: \"\"\" # TODO: Check dtype and convert x scalars to numpy array y = np . zeros ( x . shape ) y [ x < 1e2 ] = np . log ( np . exp ( x [ x < 1e2 ]) - 1 ) y [ x >= 1e2 ] = x [ x >= 1e2 ] return y","title":"ilogexp()"},{"location":"API/misc/#indl.misc.kernels.Laplace","text":"Parameters: Name Type Description Default x ndarray required w float required reverse Optional[bool] unused None Returns: Type Description ndarray 1 / 2**0.5 / w * np.exp(-(2**0.5) / w / np.abs(x)) Source code in indl/misc/kernels.py def Laplace ( x : np . ndarray , w : float , reverse : Optional [ bool ] = None ) -> np . ndarray : \"\"\" Args: x (np.ndarray): w: reverse: unused Returns: ```math 1 / 2**0.5 / w * np.exp(-(2**0.5) / w / np.abs(x)) ``` \"\"\" return 1 / 2 ** 0.5 / w * np . exp ( - ( 2 ** 0.5 ) / w / np . abs ( x ))","title":"Laplace()"},{"location":"API/misc/#indl.misc.kernels.logexp","text":"Parameters: Name Type Description Default x ndarray required Returns: Type Description ndarray Source code in indl/misc/kernels.py def logexp ( x : np . ndarray ) -> np . ndarray : \"\"\" Args: x: Returns: \"\"\" # TODO: Check dtype and convert x scalars to numpy array y = np . zeros ( x . shape ) y [ x < 1e2 ] = np . log ( 1 + np . exp ( x [ x < 1e2 ])) y [ x >= 1e2 ] = x [ x >= 1e2 ] return y","title":"logexp()"},{"location":"API/misc/#indl.misc.kernels.sshist","text":"Returns the optimal number of bins in a histogram used for density estimation. Optimization principle is to minimize expected L2 loss function between the histogram and an unknown underlying density function. An assumption made is merely that samples are drawn from the density independently each other. The optimal binwidth D* is obtained as a minimizer of the formula, (2K-V) / D^2, where K and V are mean and variance of sample counts across bins with width D. Optimal number of bins is given as (max(x) - min(x)) / D.","title":"sshist()"},{"location":"API/misc/#indl.misc.kernels.sshist--parameters","text":"x : array_like One-dimensional data to fit histogram to. N : array_like, optional Array containing number of histogram bins to evaluate for fit. Default value = 500. SN : double, optional Scalar natural number defining number of bins for shift-averaging.","title":"Parameters"},{"location":"API/misc/#indl.misc.kernels.sshist--returns","text":"optN : int Optimal number of bins to represent the data in X N : double Maximum number of bins to be evaluated. Default value = 500. C : array_like Cost function C[i] of evaluating histogram fit with N[i] bins","title":"Returns"},{"location":"API/misc/#indl.misc.kernels.sshist--see-also","text":"sskernel, ssvkernel","title":"See Also"},{"location":"API/misc/#indl.misc.kernels.sshist--references","text":".. [1] H. Shimazaki and S. Shinomoto, \"A method for selecting the bin size of a time histogram,\" in Neural Computation 19(6), 1503-1527, 2007 http://dx.doi.org/10.1162/neco.2007.19.6.1503 Source code in indl/misc/kernels.py def sshist ( x : np . ndarray , N = range ( 2 , 501 ), SN : int = 30 ) -> Tuple [ int , float , np . ndarray ]: \"\"\" Returns the optimal number of bins in a histogram used for density estimation. Optimization principle is to minimize expected L2 loss function between the histogram and an unknown underlying density function. An assumption made is merely that samples are drawn from the density independently each other. The optimal binwidth D* is obtained as a minimizer of the formula, (2K-V) / D^2, where K and V are mean and variance of sample counts across bins with width D. Optimal number of bins is given as (max(x) - min(x)) / D. Parameters ---------- x : array_like One-dimensional data to fit histogram to. N : array_like, optional Array containing number of histogram bins to evaluate for fit. Default value = 500. SN : double, optional Scalar natural number defining number of bins for shift-averaging. Returns ------- optN : int Optimal number of bins to represent the data in X N : double Maximum number of bins to be evaluated. Default value = 500. C : array_like Cost function C[i] of evaluating histogram fit with N[i] bins See Also -------- sskernel, ssvkernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"A method for selecting the bin size of a time histogram,\" in Neural Computation 19(6), 1503-1527, 2007 http://dx.doi.org/10.1162/neco.2007.19.6.1503 \"\"\" # determine range of input 'x' x_min = np . min ( x ) x_max = np . max ( x ) # get smallest difference 'dx' between all pairwise samples buf = np . abs ( np . diff ( np . sort ( x ))) dx = min ( buf [ buf > 0 ]) # setup bins to evaluate N_MIN = 2 N_MAX = min ( np . floor (( x_max - x_min ) / ( 2 * dx )), max ( N )) N = range ( N_MIN , N_MAX + 1 ) D = ( x_max - x_min ) / N # compute cost function over each possible number of bins Cs = np . zeros (( len ( N ), SN )) for i , n in enumerate ( N ): # loop over number of bins shift = np . linspace ( 0 , D [ i ], SN ) for p , sh in enumerate ( shift ): # loop over shift window positions # define bin edges edges = np . linspace ( x_min + sh - D [ i ] / 2 , x_max + sh - D [ i ] / 2 , N [ i ] + 1 ) # count number of events in these bins ki = np . histogram ( x , edges ) # get mean and variance of events k = ki [ 0 ] . mean () v = np . sum (( ki [ 0 ] - k ) ** 2 ) / N [ i ] Cs [ i , p ] = ( 2 * k - v ) / D [ i ] ** 2 # average over shift window C = Cs . mean ( axis = 1 ) # get bin count that minimizes cost C idx = np . argmin ( C ) optN = N [ idx ] optD = D [ idx ] edges = np . linspace ( x_min , x_max , optN ) return optN , optD , edges , C , N","title":"References"},{"location":"API/misc/#indl.misc.kernels.sskernel","text":"Generates a kernel density estimate with globally-optimized bandwidth. The optimal bandwidth is obtained as a minimizer of the formula, sum_{i,j} \\int k(x - x_i) k(x - x_j) dx - 2 sum_{i~=j} k(x_i - x_j), where k(x) is the kernel function.","title":"sskernel()"},{"location":"API/misc/#indl.misc.kernels.sskernel--parameters","text":"x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. W : array_like, optional The kernel bandwidths to use in optimization. Should not be chosen smaller than the sampling resolution of 'x'. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y'","title":"Parameters"},{"location":"API/misc/#indl.misc.kernels.sskernel--returns","text":"y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : double The optimal global kernel bandwidth. W : array_like The kernel bandwidths evaluated during optimization. C : array_like The cost functions associated with the bandwidths 'W'. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample.","title":"Returns"},{"location":"API/misc/#indl.misc.kernels.sskernel--see-also","text":"sshist, ssvkernel","title":"See Also"},{"location":"API/misc/#indl.misc.kernels.sskernel--references","text":".. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 Source code in indl/misc/kernels.py def sskernel ( x : np . ndarray , tin : Optional [ np . ndarray ] = None , W : Optional [ np . ndarray ] = None , nbs : Optional [ int ] = 1e3 ) -> Tuple [ np . ndarray , np . ndarray , float , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Generates a kernel density estimate with globally-optimized bandwidth. The optimal bandwidth is obtained as a minimizer of the formula, sum_{i,j} \\int k(x - x_i) k(x - x_j) dx - 2 sum_{i~=j} k(x_i - x_j), where k(x) is the kernel function. Parameters ---------- x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. W : array_like, optional The kernel bandwidths to use in optimization. Should not be chosen smaller than the sampling resolution of 'x'. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y' Returns ------- y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : double The optimal global kernel bandwidth. W : array_like The kernel bandwidths evaluated during optimization. C : array_like The cost functions associated with the bandwidths 'W'. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also -------- sshist, ssvkernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 \"\"\" def CostFunction ( y_hist , N , w , dt ): # build normal smoothing kernel yh = fftkernel ( y_hist , w / dt ) # formula for density C = np . sum ( yh ** 2 ) * dt - 2 * np . sum ( yh * y_hist ) * dt + 2 \\ / ( 2 * np . pi ) ** 0.5 / w / N C = C * N ** 2 return C , yh T = np . max ( x ) - np . min ( x ) # Total span of input spike times dx = np . sort ( np . diff ( np . sort ( x ))) # Sorted ISIs dt_samp = dx [ np . nonzero ( dx )][ 0 ] # Smallest nonzero ISI # set argument 't' if not provided if tin is None : tin = np . linspace ( np . min ( x ), np . max ( x ), int ( min ( np . ceil ( T / dt_samp ), 1e3 ))) t = tin else : if dt_samp > min ( np . diff ( tin )): t = np . linspace ( min ( tin ), max ( tin ), int ( min ( np . ceil ( T / dt_samp ), 1e3 ))) else : t = tin x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] # Spike times that fall within tin # calculate delta t dt = min ( np . diff ( t )) # create the finest histogram thist_edges = np . r_ [ t - dt / 2 , t [ - 1 ] + dt / 2 ] y_hist , _ = np . histogram ( x_ab , thist_edges ) N = sum ( y_hist ) . astype ( np . float ) y_hist = y_hist / ( N * dt ) # density # global search if input 'W' is defined if W is not None : C = np . zeros (( 1 , len ( W ))) C_min = np . Inf for k , w in enumerate ( W ): C [ k ], yh = CostFunction ( y_hist , N , w , dt ) if ( C [ k ] < C_min ): C_min = C [ k ] optw = w y = yh else : # optimized search using golden section k = 0 C = np . zeros (( 20 , 1 )) W = np . zeros (( 20 , 1 )) Wmin = 2 * dt Wmax = np . max ( x ) - np . min ( x ) tol = 10e-5 phi = ( 5 ** 0.5 + 1 ) / 2 a = ilogexp ( Wmin ) b = ilogexp ( Wmax ) c1 = ( phi - 1 ) * a + ( 2 - phi ) * b c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 , dummy = CostFunction ( y_hist , N , logexp ( c1 ), dt ) f2 , dummy = CostFunction ( y_hist , N , logexp ( c2 ), dt ) while ( np . abs ( b - a ) > tol * ( np . abs ( c1 ) + np . abs ( c2 ))) & ( k < 20 ): if f1 < f2 : b = c2 c2 = c1 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b f2 = f1 f1 , yh1 = CostFunction ( y_hist , N , logexp ( c1 ), dt ) W [ k ] = logexp ( c1 ) C [ k ] = f1 optw = logexp ( c1 ) y = yh1 / np . sum ( yh1 * dt ) else : a = c1 c1 = c2 c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = f2 f2 , yh2 = CostFunction ( y_hist , N , logexp ( c2 ), dt ) W [ k ] = logexp ( c2 ) C [ k ] = f2 optw = logexp ( c2 ) y = yh2 / np . sum ( yh2 * dt ) # increment iteration counter k = k + 1 # discard unused entries in gs, C C = C [ 0 : k ] W = W [ 0 : k ] # estimate confidence intervals by bootstrapping yb = None confb95 = None if nbs > 0 : nbs = np . asarray ( nbs ) yb = np . zeros (( nbs , len ( tin ))) for i in range ( nbs ): idx = np . random . randint ( 0 , len ( x_ab ) - 1 , len ( x_ab )) xb = x_ab [ idx ] thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_histb = np . histogram ( xb , thist - dt / 2 )[ 0 ] / dt / N yb_buf = fftkernel ( y_histb , optw / dt ) yb_buf = yb_buf / np . sum ( yb_buf * dt ) yb [ i , ] = np . interp ( tin , t , yb_buf ) ybsort = np . sort ( yb , axis = 0 ) y95b = ybsort [ np . int ( np . floor ( 0.05 * nbs )), :] y95u = ybsort [ np . int ( np . floor ( 0.95 * nbs )), :] confb95 = np . concatenate (( y95b [ np . newaxis ], y95u [ np . newaxis ]), axis = 0 ) # return outputs y = np . interp ( tin , t , y ) t = tin return y , t , optw , W , C , confb95 , yb","title":"References"},{"location":"API/misc/#indl.misc.kernels.ssvkernel","text":"Generates a locally adaptive kernel-density estimate for one-dimensional data. The user provides a one-dimensional vector of samples drawn from some underlying unknown distribution, and optionally the values where they want to estimate the probability density of that distribution. The algorithm solves an optimization problem to identify variable bandwidths across the domain where the data is provided. The optimization is based on a principle of minimizing expected L2 loss function between the kernel estimate and an unknown underlying density function. An assumption is merely that samples are drawn from the density independently of each other. The locally adaptive bandwidth is obtained by iteratively computing optimal fixed-size bandwidths wihtihn local intervals. The optimal bandwidths are selected such that they are selected in the intervals that are gamma times larger than the optimal bandwidths themselves. The paramter gamma is optimized by minimizing the L2 risk estimate.","title":"ssvkernel()"},{"location":"API/misc/#indl.misc.kernels.ssvkernel--parameters","text":"x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. Default value = None. M : int, optional The number of window sizes to evaluate. Default value = 80. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y'. WinFunc : string, optional The type of window function to use in estimating local bandwidth. Choose from one of 'Boxcar', 'Laplace', 'Cauchy' and 'Gauss'. Default value = 'Gauss'.","title":"Parameters"},{"location":"API/misc/#indl.misc.kernels.ssvkernel--returns","text":"y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : array_like The optimal local kernel bandwidths at 't'. gs : array_like The stiffness constants of the variables bandwidths evaluated. C : array_like Cost functions associated with stiffness constraints. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample.","title":"Returns"},{"location":"API/misc/#indl.misc.kernels.ssvkernel--see-also","text":"sshist, sskernel","title":"See Also"},{"location":"API/misc/#indl.misc.kernels.ssvkernel--references","text":".. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 Source code in indl/misc/kernels.py def ssvkernel ( x : np . ndarray , tin : Optional [ np . ndarray ] = None , M : int = 80 , nbs : int = 1e2 , WinFunc : str = 'Boxcar' ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Generates a locally adaptive kernel-density estimate for one-dimensional data. The user provides a one-dimensional vector of samples drawn from some underlying unknown distribution, and optionally the values where they want to estimate the probability density of that distribution. The algorithm solves an optimization problem to identify variable bandwidths across the domain where the data is provided. The optimization is based on a principle of minimizing expected L2 loss function between the kernel estimate and an unknown underlying density function. An assumption is merely that samples are drawn from the density independently of each other. The locally adaptive bandwidth is obtained by iteratively computing optimal fixed-size bandwidths wihtihn local intervals. The optimal bandwidths are selected such that they are selected in the intervals that are gamma times larger than the optimal bandwidths themselves. The paramter gamma is optimized by minimizing the L2 risk estimate. Parameters ---------- x : array_like The one-dimensional samples drawn from the underlying density tin : array_like, optional The values where the density estimate is to be evaluated in generating the output 'y'. Default value = None. M : int, optional The number of window sizes to evaluate. Default value = 80. nbs : int, optional The number of bootstrap samples to use in estimating the [0.05, 0.95] confidence interval of the output 'y'. WinFunc : string, optional The type of window function to use in estimating local bandwidth. Choose from one of 'Boxcar', 'Laplace', 'Cauchy' and 'Gauss'. Default value = 'Gauss'. Returns ------- y : array_like The estimated density, evaluated at points t / tin. t : array_like The points where the density estimate 'y' is evaluated. optw : array_like The optimal local kernel bandwidths at 't'. gs : array_like The stiffness constants of the variables bandwidths evaluated. C : array_like Cost functions associated with stiffness constraints. confb95 : array_like The 5% and 95% confidence interval of the kernel density estimate 'y'. Has dimensions 2 x len(y). confb95[0,:] corresponds to the 5% interval, and confb95[1,:] corresponds to the 95% interval. yb : array_like The bootstrap samples used in estimating confb95. Each row corresponds to one bootstrap sample. See Also -------- sshist, sskernel References ---------- .. [1] H. Shimazaki and S. Shinomoto, \"Kernel Bandwidth Optimization in Spike Rate Estimation,\" in Journal of Computational Neuroscience 29(1-2): 171\u2013182, 2010 http://dx.doi.org/10.1007/s10827-009-0180-4 \"\"\" def CostFunction ( y_hist , N , t , dt , optws , WIN , WinFunc , g ): L = y_hist . size optwv = np . zeros (( L ,)) for k in range ( L ): gs = optws [:, k ] / WIN if g > np . max ( gs ): optwv [ k ] = np . min ( WIN ) else : if g < min ( gs ): optwv [ k ] = np . max ( WIN ) else : idx = np . max ( np . nonzero ( gs >= g )) optwv [ k ] = g * WIN [ idx ] # Nadaraya-Watson kernel regression optwp = np . zeros (( L ,)) for k in range ( L ): if WinFunc == 'Boxcar' : Z = Boxcar ( t [ k ] - t , optwv / g ) elif WinFunc == 'Laplace' : Z = Laplace ( t [ k ] - t , optwv / g ) elif WinFunc == 'Cauchy' : Z = Cauchy ( t [ k ] - t , optwv / g ) else : # WinFunc == 'Gauss' Z = Gauss ( t [ k ] - t , optwv / g ) optwp [ k ] = np . sum ( optwv * Z ) / np . sum ( Z ) # speed-optimized baloon estimator idx = y_hist . nonzero () y_hist_nz = y_hist [ idx ] t_nz = t [ idx ] yv = np . zeros (( L ,)) for k in range ( L ): yv [ k ] = np . sum ( y_hist_nz * dt * Gauss ( t [ k ] - t_nz , optwp [ k ])) yv = yv * N / np . sum ( yv * dt ) # cost function of estimated kernel cg = yv ** 2 - 2 * yv * y_hist + 2 / ( 2 * np . pi ) ** 0.5 / optwp * y_hist Cg = np . sum ( cg * dt ) return Cg , yv , optwp # set argument 't' if not provided if tin is None : T = np . max ( x ) - np . min ( x ) dx = np . sort ( np . diff ( np . sort ( x ))) dt_samp = dx [ np . nonzero ( dx )][ 0 ] tin = np . linspace ( np . min ( x ), np . max ( x ), min ( np . ceil ( T / dt_samp ), 1e3 )) t = tin x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] else : T = np . max ( x ) - np . min ( x ) x_ab = x [( x >= min ( tin )) & ( x <= max ( tin ))] dx = np . sort ( np . diff ( np . sort ( x ))) dt_samp = dx [ np . nonzero ( dx )][ 0 ] if dt_samp > min ( np . diff ( tin )): t = np . linspace ( min ( tin ), max ( tin ), min ( np . ceil ( T / dt_samp ), 1e3 )) else : t = tin # calculate delta t dt = min ( np . diff ( t )) # create the finest histogram thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_hist = np . histogram ( x_ab , thist - dt / 2 )[ 0 ] / dt L = y_hist . size N = sum ( y_hist * dt ) . astype ( np . float ) # initialize window sizes W = logexp ( np . linspace ( ilogexp ( 5 * dt ), ilogexp ( T ), M )) # compute local cost functions c = np . zeros (( M , L )) for j in range ( M ): w = W [ j ] yh = fftkernel ( y_hist , w / dt ) c [ j , :] = yh ** 2 - 2 * yh * y_hist + 2 / ( 2 * np . pi ) ** 0.5 / w * y_hist # initialize optimal ws optws = np . zeros (( M , L )) for i in range ( M ): Win = W [ i ] C_local = np . zeros (( M , L )) for j in range ( M ): C_local [ j , :] = fftkernelWin ( c [ j , :], Win / dt , WinFunc ) n = np . argmin ( C_local , axis = 0 ) optws [ i , :] = W [ n ] # golden section search for stiffness parameter of variable bandwidths k = 0 gs = np . zeros (( 30 , 1 )) C = np . zeros (( 30 , 1 )) tol = 1e-5 a = 1e-12 b = 1 phi = ( 5 ** 0.5 + 1 ) / 2 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c1 )[ 0 ] f2 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c2 )[ 0 ] while ( np . abs ( b - a ) > tol * ( abs ( c1 ) + abs ( c2 ))) & ( k < 30 ): if f1 < f2 : b = c2 c2 = c1 c1 = ( phi - 1 ) * a + ( 2 - phi ) * b f2 = f1 f1 , yv1 , optwp1 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c1 ) yopt = yv1 / np . sum ( yv1 * dt ) optw = optwp1 else : a = c1 c1 = c2 c2 = ( 2 - phi ) * a + ( phi - 1 ) * b f1 = f2 f2 , yv2 , optwp2 = CostFunction ( y_hist , N , t , dt , optws , W , WinFunc , c2 ) yopt = yv2 / np . sum ( yv2 * dt ) optw = optwp2 # capture estimates and increment iteration counter gs [ k ] = c1 C [ k ] = f1 k = k + 1 # discard unused entries in gs, C gs = gs [ 0 : k ] C = C [ 0 : k ] # estimate confidence intervals by bootstrapping nbs = np . asarray ( nbs ) yb = np . zeros (( nbs , tin . size )) for i in range ( nbs ): Nb = np . random . poisson ( lam = N ) idx = np . random . randint ( 0 , N , Nb ) xb = x_ab [ idx ] thist = np . concatenate (( t , ( t [ - 1 ] + dt )[ np . newaxis ])) y_histb = np . histogram ( xb , thist - dt / 2 )[ 0 ] idx = y_histb . nonzero () y_histb_nz = y_histb [ idx ] t_nz = t [ idx ] yb_buf = np . zeros (( L , )) for k in range ( L ): yb_buf [ k ] = np . sum ( y_histb_nz * Gauss ( t [ k ] - t_nz , optw [ k ])) / Nb yb_buf = yb_buf / np . sum ( yb_buf * dt ) yb [ i , :] = np . interp ( tin , t , yb_buf ) ybsort = np . sort ( yb , axis = 0 ) y95b = ybsort [ np . int ( np . floor ( 0.05 * nbs )), :] y95u = ybsort [ np . int ( np . floor ( 0.95 * nbs )), :] confb95 = np . concatenate (( y95b [ np . newaxis ], y95u [ np . newaxis ]), axis = 0 ) # return outputs y = np . interp ( tin , t , yopt ) optw = np . interp ( tin , t , optw ) t = tin return y , t , optw , gs , C , confb95 , yb","title":"References"},{"location":"API/misc/#indl.misc.sigfuncs","text":"","title":"sigfuncs"},{"location":"API/misc/#indl.misc.sigfuncs.minimum_jerk","text":"A 1-D trajectory that minimizes jerk (3rd time-derivative of position). A minimum jerk trajectory is considered \"smooth\" and to be a feature of natural limb movements. https://storage.googleapis.com/wzukusers/user-31382847/documents/5a7253343814f4Iv6Hnt/minimumjerk.pdf A minimum-jerk trajectory is defined by: trajectory = a0 + (af - a0) * (10(dx^3) - 15(dx^4) + 6(dx^5)) where a0 is the resting position, and af is the finishing position. duration is assumed to be the extent of x but it may be overidden. Parameters: Name Type Description Default x ndarray required a0 0 af 1 degree 1 duration float Total duration of x in seconds. If None (default), calculated duration from x. None Returns: Type Description ndarray a0 + (af - a0) * (10 (dx.^3) - 15 (dx.^4) + 6*(dx.^5)) Source code in indl/misc/sigfuncs.py def minimum_jerk ( x : np . ndarray , a0 = 0 , af = 1 , degree = 1 , duration = None ) -> np . ndarray : r \"\"\" A 1-D trajectory that minimizes jerk (3rd time-derivative of position). A minimum jerk trajectory is considered \"smooth\" and to be a feature of natural limb movements. https://storage.googleapis.com/wzukusers/user-31382847/documents/5a7253343814f4Iv6Hnt/minimumjerk.pdf A minimum-jerk trajectory is defined by: ```math trajectory = a0 + (af - a0) * (10(dx^3) - 15(dx^4) + 6(dx^5)) ``` where a0 is the resting position, and af is the finishing position. duration is assumed to be the extent of x but it may be overidden. Args: x: a0: af: degree: duration (float): Total duration of x in seconds. If None (default), calculated duration from x. Returns: a0 + (af - a0) * (10*(dx.^3) - 15*(dx.^4) + 6*(dx.^5)) \"\"\" x = np . array ( x )[:, None ] assert ( x . size == x . shape [ 0 ]), f \"x must be 1D trajectory, found { x . shape } \" a0 = np . atleast_2d ( a0 ) af = np . atleast_2d ( af ) if duration is None : sorted_x = np . sort ( x , axis = 0 ) dx = np . diff ( x , axis = 0 ) duration = sorted_x [ - 1 ] - sorted_x [ 0 ] + np . min ( dx ) dx = x / duration if degree not in [ 1 , 2 ]: # Default - no derivative k0 = a0 x1 = 10 * dx ** 3 x2 = - 15 * dx ** 4 x3 = 6 * dx ** 5 elif degree == 1 : # Velocity k0 = np . zeros_like ( a0 ) x1 = 30 * dx ** 2 x2 = - 60 * dx ** 3 x3 = 30 * dx ** 4 elif degree == 2 : # Acceleration k0 = np . zeros_like ( a0 ) x1 = 60 * dx x2 = - 180 * dx ** 2 x3 = 120 * dx ** 3 return k0 + ( af - a0 ) * ( x1 + x2 + x3 )","title":"minimum_jerk()"},{"location":"API/misc/#indl.misc.sigfuncs.sigmoid","text":"Returns f(x) = A + (K - A) / ((C + Q * np.exp(-B (x - x_offset))) *(1/v)) This is a generalized logistic function. The default arguments reduce this to a simple sigmoid: f(x) = 1 / (1 + np.exp(-x)) Parameters: Name Type Description Default x np.ndarray required A float 0 K float 1 C float 1 Q float 1 B float 1 v float 1 x_offset 0 Returns: Type Description np.ndarray A + (K - A) / ((C + Q * np.exp(-B (x - x_offset))) *(1/v)) Source code in indl/misc/sigfuncs.py def sigmoid ( x , A = 0 , K = 1 , C = 1 , Q = 1 , B = 1 , v = 1 , x_offset = 0 ): \"\"\" Returns f(x) = A + (K - A) / ((C + Q * np.exp(-B*(x - x_offset)))**(1/v)) This is a generalized logistic function. The default arguments reduce this to a simple sigmoid: f(x) = 1 / (1 + np.exp(-x)) Args: x (np.ndarray): A (float): K (float): C (float): Q (float): B (float): v (float): x_offset: Returns: np.ndarray: A + (K - A) / ((C + Q * np.exp(-B*(x - x_offset)))**(1/v)) \"\"\" y = A + ( K - A ) / (( C + Q * np . exp ( - B * ( x - x_offset ))) ** ( 1 / v )) return y","title":"sigmoid()"},{"location":"API/regularizers/","text":"regularizers KernelLengthRegularizer Regularize a kernel by its length. Added loss is a int mask of 1s where abs(weight) is above threshold, and 0s otherwise, multiplied by a window. The window is typically shaped to penalize the presence of non-zero weights further away from the middle of the kernel. Use this regularizer if you want to try to find a minimal-length kernel. (after training, kernel can be shortened for faster inference). __init__ ( self , kernel_size , window_scale = 0.01 , window_func = 'poly' , poly_exp = 2 , threshold = 1e-07 ) special Parameters: Name Type Description Default kernel_size Iterable[int] length(s) of kernel(s) required window_scale float scale factor to apply to window 0.01 window_func str 'hann', 'hamming', 'poly' (default) 'poly' poly_exp int exponent used when window_func=='poly' 2 threshold float weight threshold, below which weights will not be penalized. 1e-07 Source code in indl/regularizers.py def __init__ ( self , kernel_size : Iterable [ int ], window_scale : float = 1e-2 , window_func : str = 'poly' , poly_exp : int = 2 , threshold : float = tf . keras . backend . epsilon ()): \"\"\" Args: kernel_size: length(s) of kernel(s) window_scale: scale factor to apply to window window_func: 'hann', 'hamming', 'poly' (default) poly_exp: exponent used when window_func=='poly' threshold: weight threshold, below which weights will not be penalized. \"\"\" self . kernel_size = kernel_size self . window_scale = window_scale self . window_func = window_func self . poly_exp = poly_exp self . threshold = threshold self . rebuild_window () get_config ( self ) Returns the config of the regularizer. An regularizer config is a Python dictionary (serializable) containing all configuration parameters of the regularizer. The same regularizer can be reinstantiated later (without any saved state) from this configuration. This method is optional if you are just training and executing models, exporting to and from SavedModels, or using weight checkpoints. This method is required for Keras model_to_estimator , saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON. Returns: Type Description dict Python dictionary. Source code in indl/regularizers.py def get_config ( self ) -> dict : return { 'kernel_size' : self . kernel_size , 'window_scale' : self . window_scale , 'window_func' : self . window_func , 'poly_exp' : self . poly_exp , 'threshold' : self . threshold }","title":"regularizers"},{"location":"API/regularizers/#regularizers","text":"","title":"regularizers"},{"location":"API/regularizers/#indl.regularizers","text":"","title":"indl.regularizers"},{"location":"API/regularizers/#indl.regularizers.KernelLengthRegularizer","text":"Regularize a kernel by its length. Added loss is a int mask of 1s where abs(weight) is above threshold, and 0s otherwise, multiplied by a window. The window is typically shaped to penalize the presence of non-zero weights further away from the middle of the kernel. Use this regularizer if you want to try to find a minimal-length kernel. (after training, kernel can be shortened for faster inference).","title":"KernelLengthRegularizer"},{"location":"API/regularizers/#indl.regularizers.KernelLengthRegularizer.__init__","text":"Parameters: Name Type Description Default kernel_size Iterable[int] length(s) of kernel(s) required window_scale float scale factor to apply to window 0.01 window_func str 'hann', 'hamming', 'poly' (default) 'poly' poly_exp int exponent used when window_func=='poly' 2 threshold float weight threshold, below which weights will not be penalized. 1e-07 Source code in indl/regularizers.py def __init__ ( self , kernel_size : Iterable [ int ], window_scale : float = 1e-2 , window_func : str = 'poly' , poly_exp : int = 2 , threshold : float = tf . keras . backend . epsilon ()): \"\"\" Args: kernel_size: length(s) of kernel(s) window_scale: scale factor to apply to window window_func: 'hann', 'hamming', 'poly' (default) poly_exp: exponent used when window_func=='poly' threshold: weight threshold, below which weights will not be penalized. \"\"\" self . kernel_size = kernel_size self . window_scale = window_scale self . window_func = window_func self . poly_exp = poly_exp self . threshold = threshold self . rebuild_window ()","title":"__init__()"},{"location":"API/regularizers/#indl.regularizers.KernelLengthRegularizer.get_config","text":"Returns the config of the regularizer. An regularizer config is a Python dictionary (serializable) containing all configuration parameters of the regularizer. The same regularizer can be reinstantiated later (without any saved state) from this configuration. This method is optional if you are just training and executing models, exporting to and from SavedModels, or using weight checkpoints. This method is required for Keras model_to_estimator , saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON. Returns: Type Description dict Python dictionary. Source code in indl/regularizers.py def get_config ( self ) -> dict : return { 'kernel_size' : self . kernel_size , 'window_scale' : self . window_scale , 'window_func' : self . window_func , 'poly_exp' : self . poly_exp , 'threshold' : self . threshold }","title":"get_config()"},{"location":"Miscellaneous/junk_model_inspect/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); model.inspect causes problems when working in model namespace because import inspect breaks things. if False: # Temporarily disable this test until I can recreate the model from pathlib import Path import os from tensorflow.keras.models import load_model if Path.cwd().stem == 'indl': os.chdir(Path.cwd().parent) layer_idx = 20 # [2, 6, 10, 14] n_steps = 100 max_n_filters = 25 model_file = Path.cwd() / 'data' / 'kjm_ecog' / 'converted' / 'faces_basic' / 'mv_model_full.h5' model = load_model(str(model_file)) # When processing softmax classification layer, # second last dense layer should be converted from relu to linear. if (layer_idx == len(model.layers) - 1) and (model.layers[-2].activation != tf.keras.activations.linear): model.layers[-2].activation = tf.keras.activations.linear import tempfile # Save and load the model to actually apply the change. tmp_path = Path(tempfile.gettempdir()) / (next(tempfile._get_candidate_names()) + '.h5') try: model.save(str(tmp_path)) model = load_model(str(tmp_path)) finally: tmp_path.unlink() model.summary() maximizing_activations = visualize_layer(model, layer_idx, epochs=n_steps, loss_as_exclusive=True, upsampling_steps=1, upsampling_factor=1, filter_range=(0, max_n_filters), output_dim=(701, model.get_input_shape_at(0)[-1])) # Stitch timeseries together into one mega timeseries with NaN gaps. stitched_data = _stitch_filters(maximizing_activations, n=2, sort_by_activation=False) import matplotlib.pyplot as plt # Create a colour code cycler e.g. 'C0', 'C1', etc. from itertools import cycle colour_codes = map('C{}'.format, cycle(range(10))) plt.figure() for chan_ix in [15, 9, 8]: plt.plot(stitched_data[:, :, chan_ix], color=next(colour_codes)) plt.show()","title":"Junk model inspect"},{"location":"Miscellaneous/kernels/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Kernels Some useful kernels. import numpy as np import scipy.signal import matplotlib.pyplot as plt srate = 1000 spk_rate = 13.0 # Avg 30 spikes per second tvec = np.arange(srate) / srate spikeevents = (np.random.rand(srate) < (spk_rate / srate)).astype(np.float32) spiketimes = tvec[spikeevents.nonzero()] from indl.misc.kernels import sskernel, Gauss # Shimazaki et al. auto-kernel-width kernel_width = sskernel(spiketimes - spiketimes[0], nbs=0)[2] kernel_param = 1 / (2.0 * 2.7) * kernel_width span_fac = 3.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Gauss(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.plot(t_kern, kernel) plt.xlim([-0.5, 0.5]) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd457d51910>] kernel_param = 0.050 # msec stdev span_fac = 3.0 # How many stdevs wide the kernel should be. Too short will truncate kernel. t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Gauss(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.plot(t_kern, kernel) plt.xlim([-0.5, 0.5]) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd455bcb0a0>] from indl.misc.kernels import Boxcar kernel_param = 0.05 # The width of the rectangle in seconds span_fac = np.sqrt(3.0) kernel_param /= (2*np.sqrt(3.0)) t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Boxcar(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd455a97c70>] from indl.misc.kernels import Alpha kernel_param = 0.03 # tau kernel_param *= np.sqrt(2) span_fac = 6.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Alpha(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') print(np.sum(spikeevents), np.mean(spikerates)) plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) 10.0 9.538681202301868 [<matplotlib.lines.Line2D at 0x7fd4559f6ac0>] from indl.misc.kernels import Exponential kernel_param = 0.05 # the time constant tau when the kernel reaches 1/e the maximum. span_fac = 6.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Exponential(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd45594adf0>]","title":"Kernels"},{"location":"Miscellaneous/kernels/#kernels","text":"Some useful kernels. import numpy as np import scipy.signal import matplotlib.pyplot as plt srate = 1000 spk_rate = 13.0 # Avg 30 spikes per second tvec = np.arange(srate) / srate spikeevents = (np.random.rand(srate) < (spk_rate / srate)).astype(np.float32) spiketimes = tvec[spikeevents.nonzero()] from indl.misc.kernels import sskernel, Gauss # Shimazaki et al. auto-kernel-width kernel_width = sskernel(spiketimes - spiketimes[0], nbs=0)[2] kernel_param = 1 / (2.0 * 2.7) * kernel_width span_fac = 3.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Gauss(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.plot(t_kern, kernel) plt.xlim([-0.5, 0.5]) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd457d51910>] kernel_param = 0.050 # msec stdev span_fac = 3.0 # How many stdevs wide the kernel should be. Too short will truncate kernel. t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Gauss(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.plot(t_kern, kernel) plt.xlim([-0.5, 0.5]) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd455bcb0a0>] from indl.misc.kernels import Boxcar kernel_param = 0.05 # The width of the rectangle in seconds span_fac = np.sqrt(3.0) kernel_param /= (2*np.sqrt(3.0)) t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Boxcar(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd455a97c70>] from indl.misc.kernels import Alpha kernel_param = 0.03 # tau kernel_param *= np.sqrt(2) span_fac = 6.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Alpha(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') print(np.sum(spikeevents), np.mean(spikerates)) plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) 10.0 9.538681202301868 [<matplotlib.lines.Line2D at 0x7fd4559f6ac0>] from indl.misc.kernels import Exponential kernel_param = 0.05 # the time constant tau when the kernel reaches 1/e the maximum. span_fac = 6.0 t_kern = np.arange(-span_fac * kernel_param, span_fac * kernel_param + (1 / srate), 1 / srate) kernel = Exponential(t_kern, kernel_param) spikerates = scipy.signal.convolve(spikeevents, kernel, 'same') plt.subplot(3, 1, 1) plt.plot(tvec, spikeevents) plt.subplot(3, 1, 2) plt.xlim([-0.5, 0.5]) plt.plot(t_kern, kernel) plt.subplot(3, 1, 3) plt.plot(tvec, spikerates) [<matplotlib.lines.Line2D at 0x7fd45594adf0>]","title":"Kernels"},{"location":"Miscellaneous/metrics/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Custom Metrics dprime [ src ] import numpy as np from indl.metrics import dprime y_true = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]) y_pred = np.array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]) dprime(y_true, y_pred) (1.4847266404679265, 1.5839845538068775, 33.333333333333336)","title":"Metrics"},{"location":"Miscellaneous/metrics/#custom-metrics","text":"","title":"Custom Metrics"},{"location":"Miscellaneous/metrics/#dprime","text":"[ src ] import numpy as np from indl.metrics import dprime y_true = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]) y_pred = np.array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]) dprime(y_true, y_pred) (1.4847266404679265, 1.5839845538068775, 33.333333333333336)","title":"dprime"},{"location":"Miscellaneous/sigfuncs/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Signal Functions Some useful signal functions. * sigmoid * minimum_jerk from indl.misc.sigfuncs import sigmoid import numpy as np import matplotlib.pyplot as plt x = np . arange ( - 6 , 6 , 0.1 ) plt . subplot ( 1 , 2 , 1 ) for B in [ 0.5 , 1 , 2 , 5 , 10 ]: plt . plot ( x , sigmoid ( x , B = B ), label = f \"B= { B } \" ) plt . legend () plt . subplot ( 1 , 2 , 2 ) for x_offset in [ - 3.0 , - 1.5 , 0 , 1.5 , 3.0 ]: plt . plot ( x , sigmoid ( x , x_offset = x_offset ), label = f \" { x_offset } \" ) plt . legend () plt . show () a = np . array ([[ 0.2 , 0.5 ]]) . T sigmoid ( x , A = a ) . shape (2, 120) from indl.misc.sigfuncs import minimum_jerk x = np . arange ( 0 , 6.0 , 0.1 ) for degree in [ 0 , 1 , 2 ]: plt . plot ( x , minimum_jerk ( x , degree = degree ), label = f \" { degree } \" ) plt . legend () plt . show () a = np . random . rand ( 5 , 2 ) Y = minimum_jerk ( x , a0 = a [:, 0 ], af = a [:, 1 ], degree = 0 ) plt . plot ( x , Y ) plt . show ()","title":"Sigfuncs"},{"location":"Miscellaneous/sigfuncs/#signal-functions","text":"Some useful signal functions. * sigmoid * minimum_jerk from indl.misc.sigfuncs import sigmoid import numpy as np import matplotlib.pyplot as plt x = np . arange ( - 6 , 6 , 0.1 ) plt . subplot ( 1 , 2 , 1 ) for B in [ 0.5 , 1 , 2 , 5 , 10 ]: plt . plot ( x , sigmoid ( x , B = B ), label = f \"B= { B } \" ) plt . legend () plt . subplot ( 1 , 2 , 2 ) for x_offset in [ - 3.0 , - 1.5 , 0 , 1.5 , 3.0 ]: plt . plot ( x , sigmoid ( x , x_offset = x_offset ), label = f \" { x_offset } \" ) plt . legend () plt . show () a = np . array ([[ 0.2 , 0.5 ]]) . T sigmoid ( x , A = a ) . shape (2, 120) from indl.misc.sigfuncs import minimum_jerk x = np . arange ( 0 , 6.0 , 0.1 ) for degree in [ 0 , 1 , 2 ]: plt . plot ( x , minimum_jerk ( x , degree = degree ), label = f \" { degree } \" ) plt . legend () plt . show () a = np . random . rand ( 5 , 2 ) Y = minimum_jerk ( x , a0 = a [:, 0 ], af = a [:, 1 ], degree = 0 ) plt . plot ( x , Y ) plt . show ()","title":"Signal Functions"},{"location":"bVAE/LFADS_Complex_Cell/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); LFADS Complex Cell For explanation forthcoming. For now see the src. if use_ar_prior : autocorrelation_taus = [ hps . prior_ar_atau for _ in range ( hps . co_dim )] noise_variances = [ hps . prior_ar_nvar for _ in range ( hps . co_dim )] cos_prior = LearnableAutoRegressive1Prior ( graph_batch_size , hps . co_dim , autocorrelation_taus , noise_variances , hps . do_train_prior_ar_atau , hps . do_train_prior_ar_nvar , \"u_prior_ar1\" ) kl_cost_co_b_t = KLCost_GaussianGaussianProcessSampled ( cos_posterior , cos_prior ) . kl_cost_b else : cos_prior = LearnableDiagonalGaussian ( batch_size = graph_batch_size , z_size = [ hps [ 'num_steps' ], hps [ 'co_dim' ]], name = 'cos_prior' , var = hps [ 'co_prior_var' ], trainable_mean = False , trainable_var = True ) # CO KL cost per timestep kl_cost_co_b_t = KLCost_GaussianGaussian ( self . cos_posterior , self . cos_prior ) . kl_cost_b","title":"LFADS Complex Cell"},{"location":"bVAE/LFADS_Complex_Cell/#lfads-complex-cell","text":"For explanation forthcoming. For now see the src. if use_ar_prior : autocorrelation_taus = [ hps . prior_ar_atau for _ in range ( hps . co_dim )] noise_variances = [ hps . prior_ar_nvar for _ in range ( hps . co_dim )] cos_prior = LearnableAutoRegressive1Prior ( graph_batch_size , hps . co_dim , autocorrelation_taus , noise_variances , hps . do_train_prior_ar_atau , hps . do_train_prior_ar_nvar , \"u_prior_ar1\" ) kl_cost_co_b_t = KLCost_GaussianGaussianProcessSampled ( cos_posterior , cos_prior ) . kl_cost_b else : cos_prior = LearnableDiagonalGaussian ( batch_size = graph_batch_size , z_size = [ hps [ 'num_steps' ], hps [ 'co_dim' ]], name = 'cos_prior' , var = hps [ 'co_prior_var' ], trainable_mean = False , trainable_var = True ) # CO KL cost per timestep kl_cost_co_b_t = KLCost_GaussianGaussian ( self . cos_posterior , self . cos_prior ) . kl_cost_b","title":"LFADS Complex Cell"},{"location":"bVAE/LFADS_utils/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); LFADS Utilities See the src. gen_ics_prior = LearnableDiagonalGaussian ( batch_size = graph_batch_size , z_size = [ 1 , hps [ 'ic_dim' ]], name = 'gen_ics_prior' , var = hps [ 'ic_prior_var' ], trainable_mean = True , trainable_var = False ) kl_cost_g0_b = KLCost_GaussianGaussian ( gen_ics_posterior , gen_ics_prior ) . kl_cost_b test_GRUClipCell () Model: \"GRUClip\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ bidirectional (Bidirectional (None, 256) 127488 ================================================================= Total params: 127,488 Trainable params: 127,488 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[-0.23070076 0.2536174 0.09853305 ... -0.08522673 -0.20194843 0.2024224 ] [-0.25721148 0.339842 0.0416486 ... -0.18940908 -0.30409572 0.03010286] [-0.23013729 0.38037577 0.09360252 ... -0.049852 -0.18864658 0.1707485 ] ... [-0.24810478 0.25258836 0.21201608 ... -0.10185561 -0.23760056 0.24850681] [-0.23509276 0.2597233 0.15127341 ... -0.1369887 -0.30795547 0.06462491] [-0.3083426 0.3344782 0.22309446 ... -0.20601623 -0.340707 0.17592245]], shape=(16, 256), dtype=float32) test_GRUClipLayer () Model: \"GRUClip\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ gru_clip (GRUClip) (None, 128) 63360 ================================================================= Total params: 63,360 Trainable params: 63,360 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[-0.145829 -0.0738971 -0.10125014 ... -0.3566203 -0.28160632 0.12751628] [-0.1078586 -0.03819858 -0.0784252 ... -0.30388248 -0.26838285 0.13112734] [-0.20403881 -0.04190751 -0.10297713 ... -0.1784311 -0.29298502 0.19574839] ... [-0.17381434 -0.02557798 -0.15238568 ... -0.38625315 -0.29328454 0.21715674] [-0.10311585 -0.09402761 0.00203776 ... -0.11177816 -0.27137804 0.18426083] [-0.26278093 0.04026812 -0.09496458 ... -0.3568564 -0.2787963 0.22160673]], shape=(16, 128), dtype=float32) test_GRUClipLayer_in_Bidirectional () Model: \"GRUClipBi\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ bidirectional (Bidirectional (None, 256) 126720 ================================================================= Total params: 126,720 Trainable params: 126,720 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[ 0.05232625 0.20494887 -0.12492953 ... 0.11413126 0.25303578 -0.31101322] [ 0.03256029 0.2671073 -0.00039157 ... 0.11336662 0.3193527 -0.30593503] [ 0.01949142 0.3231842 -0.01736745 ... 0.15450042 0.17337021 -0.21200603] ... [-0.06974471 0.27610314 -0.08454704 ... 0.16733271 0.25611082 -0.2095642 ] [ 0.10193326 0.17880175 -0.04680509 ... 0.20970306 0.13739228 -0.22986582] [ 0.07257779 0.1905019 -0.08863862 ... 0.12622494 0.16717374 -0.27231687]], shape=(16, 256), dtype=float32)","title":"LFADS utils"},{"location":"bVAE/LFADS_utils/#lfads-utilities","text":"See the src. gen_ics_prior = LearnableDiagonalGaussian ( batch_size = graph_batch_size , z_size = [ 1 , hps [ 'ic_dim' ]], name = 'gen_ics_prior' , var = hps [ 'ic_prior_var' ], trainable_mean = True , trainable_var = False ) kl_cost_g0_b = KLCost_GaussianGaussian ( gen_ics_posterior , gen_ics_prior ) . kl_cost_b test_GRUClipCell () Model: \"GRUClip\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ bidirectional (Bidirectional (None, 256) 127488 ================================================================= Total params: 127,488 Trainable params: 127,488 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[-0.23070076 0.2536174 0.09853305 ... -0.08522673 -0.20194843 0.2024224 ] [-0.25721148 0.339842 0.0416486 ... -0.18940908 -0.30409572 0.03010286] [-0.23013729 0.38037577 0.09360252 ... -0.049852 -0.18864658 0.1707485 ] ... [-0.24810478 0.25258836 0.21201608 ... -0.10185561 -0.23760056 0.24850681] [-0.23509276 0.2597233 0.15127341 ... -0.1369887 -0.30795547 0.06462491] [-0.3083426 0.3344782 0.22309446 ... -0.20601623 -0.340707 0.17592245]], shape=(16, 256), dtype=float32) test_GRUClipLayer () Model: \"GRUClip\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ gru_clip (GRUClip) (None, 128) 63360 ================================================================= Total params: 63,360 Trainable params: 63,360 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[-0.145829 -0.0738971 -0.10125014 ... -0.3566203 -0.28160632 0.12751628] [-0.1078586 -0.03819858 -0.0784252 ... -0.30388248 -0.26838285 0.13112734] [-0.20403881 -0.04190751 -0.10297713 ... -0.1784311 -0.29298502 0.19574839] ... [-0.17381434 -0.02557798 -0.15238568 ... -0.38625315 -0.29328454 0.21715674] [-0.10311585 -0.09402761 0.00203776 ... -0.11177816 -0.27137804 0.18426083] [-0.26278093 0.04026812 -0.09496458 ... -0.3568564 -0.2787963 0.22160673]], shape=(16, 128), dtype=float32) test_GRUClipLayer_in_Bidirectional () Model: \"GRUClipBi\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 246, 36)] 0 _________________________________________________________________ bidirectional (Bidirectional (None, 256) 126720 ================================================================= Total params: 126,720 Trainable params: 126,720 Non-trainable params: 0 _________________________________________________________________ tf.Tensor( [[ 0.05232625 0.20494887 -0.12492953 ... 0.11413126 0.25303578 -0.31101322] [ 0.03256029 0.2671073 -0.00039157 ... 0.11336662 0.3193527 -0.30593503] [ 0.01949142 0.3231842 -0.01736745 ... 0.15450042 0.17337021 -0.21200603] ... [-0.06974471 0.27610314 -0.08454704 ... 0.16733271 0.25611082 -0.2095642 ] [ 0.10193326 0.17880175 -0.04680509 ... 0.20970306 0.13739228 -0.22986582] [ 0.07257779 0.1905019 -0.08863862 ... 0.12622494 0.16717374 -0.27231687]], shape=(16, 256), dtype=float32)","title":"LFADS Utilities"},{"location":"bVAE/beta_vae/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); \\(\\beta\\) Variational Autoencoders to Disentangle Multi-channel Spiking Data I already have quite a few general notes on \\(\\beta\\) Variational Autoencoders with Tensorflow Probability in the IntracranialNeurophysDL repository here . This notebook provides \\(\\beta\\) -VAE component implementations that are more useful with our macaque PFC data. Here we define a series of model-builder functions. Each function takes params , a dictionary of hyperparameters, and inputs containing one or more Keras tensors, and each returns the model outputs and other intermediate variables that need to be tracked. We have generic functions to create the graphs for f- and z-encoders; we have a function to create the first part of the decoder graph; and a function to complete the decoder graph. We also provide an end-to-end model to show how to use it. These components are exported to our indl library. Our separate data analysis notebooks import this module, and possibly others (e.g., LFADS) to build models for analyzing our data. We don't do significant data analysis here. As much as possible, we try to make the functions generic enough that we can use parameters to switch between different \\(\\beta\\) -VAE implementations. We identify 4 different VAE models for consideration: * disentangled sequential autoencoders (DSAE) ( Li and Mandt, ICML 2018 ) - \"Full\" model. * Same as above - \"Factorized' model * FHVAE ( Hsu and Glass ) * LFADS (latest: AutoLFADS) ( Keshtkaran, ..., Pandarinath, 2021 ) The below table provides some differences between the models, but is perhaps incorrect and needs to be updated. Please do not rely on it. LFADS DSAE full DSAE factorized FHVAE f RNN Bidir. GRU Bidir. LSTM -- LSTM x2 f prior \\(\\mathcal{N}(0,\\kappa I)\\) \\(\\mathcal{N}(\\mu_z,\\sigma_z I)\\) -- \\(\\mathcal{N}(\\mu_2,0.5^2I)\\) z RNN A: Bidir. GRU, B:GRU Bdir. LSTM -> RNN MLP LSTM x2 z RNN input A: x; B: (A(x), fac) concat(x, tile(f)) \\(x_t\\) concat(input, tile(f)) z prior LearnableAutoRegressive1Prior LSTM(0) -- \\(\\mathcal{N}(0,I)\\) Decoder RNN GRU ?? ?? LSTM x2 RNN input0 0 / z ?? ?? concat(f, z) RNN state0 f ?? ?? 0 RNN output -MLP-> fac -MLP-> rates ?? ?? (x_mu, x_logvar) Decoder loss -log(p spike|Poisson(rates)) ?? ?? sparse sce with logits Learning rate 1e-2 decay 0.95 every 6 ?? ?? ?? Hyperparameters We separate our hyperparameters into non-tunable 'arguments' and tunable 'parameters'. This helps with the hyperparameter optimization framework. Prepare inputs Apply dropout, split f_encoder inputs off from inputs to prevent acausal modeling (optional), coordinated dropout (optional), CV mask (not implemented yet), Dense to input factors (optional). test_prepare_inputs ( n_times = 246 ) Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 246, 36)] 0 __________________________________________________________________________________________________ dropout (Dropout) (None, 246, 36) 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 246, 36)] 0 dropout[0][0] __________________________________________________________________________________________________ coordinated_dropout (Coordinate ((None, 246, 36), (N 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None, 0, 36)] 0 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 246, 36)] 0 tf_op_layer_strided_slice[0][0] coordinated_dropout[0][0] ================================================================================================== Total params: 0 Trainable params: 0 Non-trainable params: 0 __________________________________________________________________________________________________ [(TensorShape([16, 246, 36]), tf.float32), (TensorShape([16, 246, 36]), tf.float32), (TensorShape([16, 246, 36]), tf.bool)] f -Encoder Transform full sequence of \"features\" ( inputs or ReadIn(inputs) ) through (1) RNN then (2) affine to yield parameters of latent posterior distribution: \\( \\(q(f | x_{1:T})\\) \\) This distribution is a multivariate normal, optionally with off-diagonal elements allowed. Model loss will include the KL divergence between the static latent posterior and a prior. The prior is a learnable multivariate normal diagonal. The prior is initialized with a mean of 0 and a stddev of 1 but these are trainable by default. test_create_f_encoder () Model: \"f_encoder_model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, None, 36)] 0 __________________________________________________________________________________________________ f_rnn_0 (Bidirectional) (None, 256) 126720 input_1[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 256) 0 f_rnn_0[0][0] __________________________________________________________________________________________________ f_scale (Dense) (None, 10) 2570 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 10)] 0 f_scale[0][0] __________________________________________________________________________________________________ tf_op_layer_Softplus (TensorFlo [(None, 10)] 0 tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ f_loc (Dense) (None, 10) 2570 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2_1 (TensorFlow [(None, 10)] 0 tf_op_layer_Softplus[0][0] __________________________________________________________________________________________________ q_f (DistributionLambda) ((None, 10), (None, 0 f_loc[0][0] tf_op_layer_AddV2_1[0][0] ================================================================================================== Total params: 131,860 Trainable params: 131,860 Non-trainable params: 0 __________________________________________________________________________________________________ tfp.distributions.MultivariateNormalDiag(\"f_encoder_model_q_f_MultivariateNormalDiag\", batch_shape=[16], event_shape=[10], dtype=float32) tf.Tensor( [ 93.19866 81.839386 73.49664 45.901627 50.906715 36.570427 62.996284 89.13544 78.416565 38.75402 85.56836 71.12454 51.789825 100.711395 86.70568 89.89396 ], shape=(16,), dtype=float32) z -Encoder \\(q(z_t | x_{1:T})\\) I have also seen this called the \"Dynamic Encoder\", or in LFADS the \"Controller Input\" encoder. The z -Encoder varies quite a bit between the different Disentangling/ \\(\\beta\\) Variational Autoencoder implementations. Indeed, in some formulations it isn't used at all, such as the LFADS model without inferred controller input. The inputs are the original data sequences ( \\(x_t\\) ). Unlike the f -encoder, here we output full sequences. The output sequences parameterize a multivariate normal distribution at each timestep The encoder itself has as its first layer a RNN (LSTM, GRU), often bidirectional, or a simple MLP as in the DHSAE Factorized model If the first layer is an RNN then there is usually a second layer forward-only RNN. Extra Details - DHSAE Full The inputs are concatenated with a tiled sample from \\(q(f)\\) . We've added the option to instead concatenate on the inputs into the second RNN. Extra Details - LFADS Like its f-Encoder, the RNN cells are a GRU with clipping. The secondary RNN input is the output from the primary RNN concatenated with the decoder RNN's previous step + transformed through the factor Dense layer . Because the LFADS secondary RNN is so complicated, it is integrated into the decoder RNN itself in a \"complex cell\". The complex cell includes the z2-cell, making the z2 outputs variational in \\(q(z_t)\\) , sampling \\(q(z_t)\\) for the inputs to the generative RNN cell, passing the output of the generative RNN step through a Dense to-factors layer, and finally using that output as one of the inputs to the z2 cell. If params['gen_cell_type'] is \"Complex\" , then we assume that LFADS is being used and we thus skip the second RNN in create_z_encoder and we skip making the latents variational in make_z_variational . # TODO: Rework this # TODO: Compare to LFADS' prior on enc_z. def sample_dynamic_prior ( self , timesteps , samples = 1 , batches = 1 , fixed = False ): \"\"\" Samples from self.dynamic_prior_cell `timesteps` times. On each step, the previous (sample, state) is fed back into the cell (zero_state used for 0th step). The cell returns a multivariate normal diagonal distribution for each timestep. We collect each timestep-dist's params (loc and scale), then use them to create the return value: a single MVN diag dist that has a dimension for timesteps. The cell returns a full dist for each timestep so that we can 'sample' it. If our sample size is 1, and our cell is an RNN cell, then this is roughly equivalent to doing a generative RNN (init state = zeros, return_sequences=True) then passing those values through a pair of Dense layers to parameterize a single MVNDiag. :param timesteps: Number of timesteps to sample for each sequence. :param samples: Number of samples to draw from the latent distribution. :param batches: Number of sequences to sample. :param fixed: Boolean for whether or not to share the same random sample across all sequences in batch. \"\"\" if fixed : sample_batch_size = 1 else : sample_batch_size = batches sample , state = self . dynamic_prior_cell . zero_state ([ samples , sample_batch_size ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( timesteps ): dist , state = self . dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : # tile along the batch axis sample = sample + tf . zeros ([ batches , 1 , 1 ]) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) # TODO: Move 1 of the batch dims into event dims test_create_z_encoder () Model: \"z_encoder_model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, None, 36)] 0 _________________________________________________________________ z_rnn_1 (Bidirectional) (None, None, 32) 5088 ================================================================= Total params: 5,088 Trainable params: 5,088 Non-trainable params: 0 _________________________________________________________________ <class 'tensorflow.python.framework.ops.EagerTensor'> (16, 246, 32) dynamic_prior_cell = LearnableMultivariateNormalDiagCell ( 3 , 4 , cell_type = 'gru' ) sample , state = dynamic_prior_cell . zero_state ([ 1 , 1 ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( 161 ): dist , state = dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample ) Generator (Decoder part 1) \\(p(x_t | z_t, f)\\) The generator is an RNN that outputs full sequences from the encoded latents which comprise a single-timestep latent vector ( f ) and optionally a low-dimensional sequence ( \\(z_t\\) ). Note that these latents are distributions, and therefore must be sampled from to get the initial state and/or the inputs to the generative RNN. The generative RNN outputs a sequence. This sequence is typically transformed through a Dense layer to yield the \"factors\". However, in LFADS the factors are fedback to the z2_encoder step-by-step, and this cannot be accomplished in a normal sequential layer connection. Instead, LFADS includes the dense layer inside a \"ComplexCell\". To be consistent with the LFADS implementation we need to include the to-dense layer in other create_generator_ functions. from indl.model.lfads.complex_cell import ComplexCell def create_generator_LFADS (): \"\"\" units_gen, units_con, factors_dim, co_dim, ext_input_dim, inject_ext_input_to_gen, \"\"\" # TODO: Sample/Mean from $q(f)$. This will replace the first element in generator init_states # TODO: need a custom function for sample-during-train-mean-during-test. See nn.dropout for inspiration. # TODO: Sample from $q(z_t)$, and optionally concat with ext_input, to build generator inputs. # TODO: continue generator from lfads-cd/lfadslite.py start at 495 custom_cell = ComplexCell ( params [ 'gen_dim' ], # Units in generator GRU con_hidden_state_dim , # Units in controller GRU params [ 'factors_dim' ], params [ 'co_dim' ], params [ 'ext_input_dim' ], True , ) generator = tfkl . RNN ( custom_cell , return_sequences = True , # recurrent_regularizer=tf.keras.regularizers.l2(l=gen_l2_reg), name = 'gen_rnn' ) init_states = generator . get_initial_state ( gen_input ) gen_output = generator ( gen_input , initial_state = init_states ) factors = gen_output [ - 1 ] return factors Probabilistic Reconstructed Input (Decoder part 2) The factors are passed through a Dense layer and the outputs are the same dimensionality as the inputs, but instead of reconstructing the inputs, they parameterize a distribution representing the inputs. This distribution can be Gaussian or Poisson, with the latter being more appropriate for (binned) spike counts.","title":"Beta vae"},{"location":"bVAE/beta_vae/#beta-variational-autoencoders-to-disentangle-multi-channel-spiking-data","text":"I already have quite a few general notes on \\(\\beta\\) Variational Autoencoders with Tensorflow Probability in the IntracranialNeurophysDL repository here . This notebook provides \\(\\beta\\) -VAE component implementations that are more useful with our macaque PFC data. Here we define a series of model-builder functions. Each function takes params , a dictionary of hyperparameters, and inputs containing one or more Keras tensors, and each returns the model outputs and other intermediate variables that need to be tracked. We have generic functions to create the graphs for f- and z-encoders; we have a function to create the first part of the decoder graph; and a function to complete the decoder graph. We also provide an end-to-end model to show how to use it. These components are exported to our indl library. Our separate data analysis notebooks import this module, and possibly others (e.g., LFADS) to build models for analyzing our data. We don't do significant data analysis here. As much as possible, we try to make the functions generic enough that we can use parameters to switch between different \\(\\beta\\) -VAE implementations. We identify 4 different VAE models for consideration: * disentangled sequential autoencoders (DSAE) ( Li and Mandt, ICML 2018 ) - \"Full\" model. * Same as above - \"Factorized' model * FHVAE ( Hsu and Glass ) * LFADS (latest: AutoLFADS) ( Keshtkaran, ..., Pandarinath, 2021 ) The below table provides some differences between the models, but is perhaps incorrect and needs to be updated. Please do not rely on it. LFADS DSAE full DSAE factorized FHVAE f RNN Bidir. GRU Bidir. LSTM -- LSTM x2 f prior \\(\\mathcal{N}(0,\\kappa I)\\) \\(\\mathcal{N}(\\mu_z,\\sigma_z I)\\) -- \\(\\mathcal{N}(\\mu_2,0.5^2I)\\) z RNN A: Bidir. GRU, B:GRU Bdir. LSTM -> RNN MLP LSTM x2 z RNN input A: x; B: (A(x), fac) concat(x, tile(f)) \\(x_t\\) concat(input, tile(f)) z prior LearnableAutoRegressive1Prior LSTM(0) -- \\(\\mathcal{N}(0,I)\\) Decoder RNN GRU ?? ?? LSTM x2 RNN input0 0 / z ?? ?? concat(f, z) RNN state0 f ?? ?? 0 RNN output -MLP-> fac -MLP-> rates ?? ?? (x_mu, x_logvar) Decoder loss -log(p spike|Poisson(rates)) ?? ?? sparse sce with logits Learning rate 1e-2 decay 0.95 every 6 ?? ?? ??","title":"\\(\\beta\\) Variational Autoencoders to Disentangle Multi-channel Spiking Data"},{"location":"bVAE/beta_vae/#hyperparameters","text":"We separate our hyperparameters into non-tunable 'arguments' and tunable 'parameters'. This helps with the hyperparameter optimization framework.","title":"Hyperparameters"},{"location":"bVAE/beta_vae/#prepare-inputs","text":"Apply dropout, split f_encoder inputs off from inputs to prevent acausal modeling (optional), coordinated dropout (optional), CV mask (not implemented yet), Dense to input factors (optional). test_prepare_inputs ( n_times = 246 ) Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 246, 36)] 0 __________________________________________________________________________________________________ dropout (Dropout) (None, 246, 36) 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 246, 36)] 0 dropout[0][0] __________________________________________________________________________________________________ coordinated_dropout (Coordinate ((None, 246, 36), (N 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None, 0, 36)] 0 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 246, 36)] 0 tf_op_layer_strided_slice[0][0] coordinated_dropout[0][0] ================================================================================================== Total params: 0 Trainable params: 0 Non-trainable params: 0 __________________________________________________________________________________________________ [(TensorShape([16, 246, 36]), tf.float32), (TensorShape([16, 246, 36]), tf.float32), (TensorShape([16, 246, 36]), tf.bool)]","title":"Prepare inputs"},{"location":"bVAE/beta_vae/#f-encoder","text":"Transform full sequence of \"features\" ( inputs or ReadIn(inputs) ) through (1) RNN then (2) affine to yield parameters of latent posterior distribution: \\( \\(q(f | x_{1:T})\\) \\) This distribution is a multivariate normal, optionally with off-diagonal elements allowed. Model loss will include the KL divergence between the static latent posterior and a prior. The prior is a learnable multivariate normal diagonal. The prior is initialized with a mean of 0 and a stddev of 1 but these are trainable by default. test_create_f_encoder () Model: \"f_encoder_model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, None, 36)] 0 __________________________________________________________________________________________________ f_rnn_0 (Bidirectional) (None, 256) 126720 input_1[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 256) 0 f_rnn_0[0][0] __________________________________________________________________________________________________ f_scale (Dense) (None, 10) 2570 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 10)] 0 f_scale[0][0] __________________________________________________________________________________________________ tf_op_layer_Softplus (TensorFlo [(None, 10)] 0 tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ f_loc (Dense) (None, 10) 2570 dropout[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2_1 (TensorFlow [(None, 10)] 0 tf_op_layer_Softplus[0][0] __________________________________________________________________________________________________ q_f (DistributionLambda) ((None, 10), (None, 0 f_loc[0][0] tf_op_layer_AddV2_1[0][0] ================================================================================================== Total params: 131,860 Trainable params: 131,860 Non-trainable params: 0 __________________________________________________________________________________________________ tfp.distributions.MultivariateNormalDiag(\"f_encoder_model_q_f_MultivariateNormalDiag\", batch_shape=[16], event_shape=[10], dtype=float32) tf.Tensor( [ 93.19866 81.839386 73.49664 45.901627 50.906715 36.570427 62.996284 89.13544 78.416565 38.75402 85.56836 71.12454 51.789825 100.711395 86.70568 89.89396 ], shape=(16,), dtype=float32)","title":"f-Encoder"},{"location":"bVAE/beta_vae/#z-encoder","text":"\\(q(z_t | x_{1:T})\\) I have also seen this called the \"Dynamic Encoder\", or in LFADS the \"Controller Input\" encoder. The z -Encoder varies quite a bit between the different Disentangling/ \\(\\beta\\) Variational Autoencoder implementations. Indeed, in some formulations it isn't used at all, such as the LFADS model without inferred controller input. The inputs are the original data sequences ( \\(x_t\\) ). Unlike the f -encoder, here we output full sequences. The output sequences parameterize a multivariate normal distribution at each timestep The encoder itself has as its first layer a RNN (LSTM, GRU), often bidirectional, or a simple MLP as in the DHSAE Factorized model If the first layer is an RNN then there is usually a second layer forward-only RNN.","title":"z-Encoder"},{"location":"bVAE/beta_vae/#extra-details-dhsae-full","text":"The inputs are concatenated with a tiled sample from \\(q(f)\\) . We've added the option to instead concatenate on the inputs into the second RNN.","title":"Extra Details - DHSAE Full"},{"location":"bVAE/beta_vae/#extra-details-lfads","text":"Like its f-Encoder, the RNN cells are a GRU with clipping. The secondary RNN input is the output from the primary RNN concatenated with the decoder RNN's previous step + transformed through the factor Dense layer . Because the LFADS secondary RNN is so complicated, it is integrated into the decoder RNN itself in a \"complex cell\". The complex cell includes the z2-cell, making the z2 outputs variational in \\(q(z_t)\\) , sampling \\(q(z_t)\\) for the inputs to the generative RNN cell, passing the output of the generative RNN step through a Dense to-factors layer, and finally using that output as one of the inputs to the z2 cell. If params['gen_cell_type'] is \"Complex\" , then we assume that LFADS is being used and we thus skip the second RNN in create_z_encoder and we skip making the latents variational in make_z_variational . # TODO: Rework this # TODO: Compare to LFADS' prior on enc_z. def sample_dynamic_prior ( self , timesteps , samples = 1 , batches = 1 , fixed = False ): \"\"\" Samples from self.dynamic_prior_cell `timesteps` times. On each step, the previous (sample, state) is fed back into the cell (zero_state used for 0th step). The cell returns a multivariate normal diagonal distribution for each timestep. We collect each timestep-dist's params (loc and scale), then use them to create the return value: a single MVN diag dist that has a dimension for timesteps. The cell returns a full dist for each timestep so that we can 'sample' it. If our sample size is 1, and our cell is an RNN cell, then this is roughly equivalent to doing a generative RNN (init state = zeros, return_sequences=True) then passing those values through a pair of Dense layers to parameterize a single MVNDiag. :param timesteps: Number of timesteps to sample for each sequence. :param samples: Number of samples to draw from the latent distribution. :param batches: Number of sequences to sample. :param fixed: Boolean for whether or not to share the same random sample across all sequences in batch. \"\"\" if fixed : sample_batch_size = 1 else : sample_batch_size = batches sample , state = self . dynamic_prior_cell . zero_state ([ samples , sample_batch_size ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( timesteps ): dist , state = self . dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : # tile along the batch axis sample = sample + tf . zeros ([ batches , 1 , 1 ]) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) # TODO: Move 1 of the batch dims into event dims test_create_z_encoder () Model: \"z_encoder_model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, None, 36)] 0 _________________________________________________________________ z_rnn_1 (Bidirectional) (None, None, 32) 5088 ================================================================= Total params: 5,088 Trainable params: 5,088 Non-trainable params: 0 _________________________________________________________________ <class 'tensorflow.python.framework.ops.EagerTensor'> (16, 246, 32) dynamic_prior_cell = LearnableMultivariateNormalDiagCell ( 3 , 4 , cell_type = 'gru' ) sample , state = dynamic_prior_cell . zero_state ([ 1 , 1 ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( 161 ): dist , state = dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample )","title":"Extra Details - LFADS"},{"location":"bVAE/beta_vae/#generator-decoder-part-1","text":"\\(p(x_t | z_t, f)\\) The generator is an RNN that outputs full sequences from the encoded latents which comprise a single-timestep latent vector ( f ) and optionally a low-dimensional sequence ( \\(z_t\\) ). Note that these latents are distributions, and therefore must be sampled from to get the initial state and/or the inputs to the generative RNN. The generative RNN outputs a sequence. This sequence is typically transformed through a Dense layer to yield the \"factors\". However, in LFADS the factors are fedback to the z2_encoder step-by-step, and this cannot be accomplished in a normal sequential layer connection. Instead, LFADS includes the dense layer inside a \"ComplexCell\". To be consistent with the LFADS implementation we need to include the to-dense layer in other create_generator_ functions. from indl.model.lfads.complex_cell import ComplexCell def create_generator_LFADS (): \"\"\" units_gen, units_con, factors_dim, co_dim, ext_input_dim, inject_ext_input_to_gen, \"\"\" # TODO: Sample/Mean from $q(f)$. This will replace the first element in generator init_states # TODO: need a custom function for sample-during-train-mean-during-test. See nn.dropout for inspiration. # TODO: Sample from $q(z_t)$, and optionally concat with ext_input, to build generator inputs. # TODO: continue generator from lfads-cd/lfadslite.py start at 495 custom_cell = ComplexCell ( params [ 'gen_dim' ], # Units in generator GRU con_hidden_state_dim , # Units in controller GRU params [ 'factors_dim' ], params [ 'co_dim' ], params [ 'ext_input_dim' ], True , ) generator = tfkl . RNN ( custom_cell , return_sequences = True , # recurrent_regularizer=tf.keras.regularizers.l2(l=gen_l2_reg), name = 'gen_rnn' ) init_states = generator . get_initial_state ( gen_input ) gen_output = generator ( gen_input , initial_state = init_states ) factors = gen_output [ - 1 ] return factors","title":"Generator (Decoder part 1)"},{"location":"bVAE/beta_vae/#probabilistic-reconstructed-input-decoder-part-2","text":"The factors are passed through a Dense layer and the outputs are the same dimensionality as the inputs, but instead of reconstructing the inputs, they parameterize a distribution representing the inputs. This distribution can be Gaussian or Poisson, with the latter being more appropriate for (binned) spike counts.","title":"Probabilistic Reconstructed Input (Decoder part 2)"},{"location":"bVAE/recurrent_layers/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Custom Recurrent Layers A bit of a mess. Also check out tfp_utils, lfads_utils, and lfads_complex_cell. Test GenerativeRNN N_IN = 8 N_UNITS = 24 N_OUT_TIMESTEPS = 115 cell = tfkl.GRUCell # LSTMCell or GRUCell # Test regular RNN with zeros input reg_rnn_layer = tfkl.RNN(cell(N_UNITS), return_state=True, return_sequences=True) in_ = tf.zeros((1, 115, 16)) x_ = reg_rnn_layer(in_) print(K.any(x_[0])) # Just to remind myself that input zeros and state zeros will yield output zeros. tf.Tensor(False, shape=(), dtype=bool) # Test placeholder tensor with no timesteps K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tfkl.Input(shape=(N_IN,)) x_, cell_state_ = gen_rnn_layer(in_) print(\"Test placeholder tensor\") model = tf.keras.Model(inputs=in_, outputs=x_) model.summary() Test placeholder tensor Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 8)] 0 __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None, 1, 8)] 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 8)] 0 tf_op_layer_strided_slice[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike (TensorFl [(None, 8)] 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_2 (Te [(None, 1, 8)] 0 tf_op_layer_ZerosLike[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 8)] 0 tf_op_layer_strided_slice_2[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 115, 8)] 0 tf_op_layer_strided_slice[0][0] tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ generative_rnn (GenerativeRNN) [(None, 115, 24), (N 2448 tf_op_layer_concat[0][0] ================================================================================================== Total params: 2,448 Trainable params: 2,448 Non-trainable params: 0 __________________________________________________________________________________________________ # Test placeholder tensor with no timesteps as initial state K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tfkl.Input(shape=(N_UNITS,)) x_, cell_state_ = gen_rnn_layer(None, initial_state=in_) print(\"Test placeholder tensor\") model = tf.keras.Model(inputs=in_, outputs=x_) model.summary() Test placeholder tensor Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 24)] 0 __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None,)] 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 1)] 0 tf_op_layer_strided_slice[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike (TensorFl [(None, 1)] 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_2 (Te [(None, 1, 1)] 0 tf_op_layer_ZerosLike[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_3 (Te [(None, 1)] 0 tf_op_layer_strided_slice_2[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike_1 (Tensor [(None, 1)] 0 tf_op_layer_strided_slice_3[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_4 (Te [(None, 1, 1)] 0 tf_op_layer_ZerosLike_1[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 1)] 0 tf_op_layer_strided_slice_4[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 115, 1)] 0 tf_op_layer_strided_slice_2[0][0] tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ generative_rnn (GenerativeRNN) [(None, 115, 24), (N 1944 tf_op_layer_concat[0][0] input_1[0][0] ================================================================================================== Total params: 1,944 Trainable params: 1,944 Non-trainable params: 0 __________________________________________________________________________________________________ # Test None input --> uses zeros K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) print(\"Test None input\") x_, cell_state_ = gen_rnn_layer() print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? Test None input (1, 115, 24) (1, 24) tf.Tensor(False, shape=(), dtype=bool) tf.Tensor(False, shape=(), dtype=bool) # Test random input K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tf.random.uniform((1, 8, N_UNITS), minval=-1.0, maxval=1.0) print(\"Test zeros input\") x_, cell_state_ = gen_rnn_layer(in_) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? Test zeros input (1, 115, 24) (1, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Test random states K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) print(gen_rnn_layer.compute_output_shape()) init_states = [tf.random.uniform((1, N_UNITS), minval=-1.0, maxval=1.0) for _ in range(1)] x_, cell_states_ = gen_rnn_layer(initial_state=init_states) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? [TensorShape([None, 115, 24]), TensorShape([None, 24])] (1, 115, 24) (1, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Test masking K.clear_session() tmp = tf.range(N_OUT_TIMESTEPS)[tf.newaxis, :, tf.newaxis] mask = tf.math.logical_or(tmp < 5, tmp > 100) gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS, tile_input=True) in_ = tf.random.uniform((5, N_OUT_TIMESTEPS, N_UNITS), minval=-1.0, maxval=1.0) x_, cell_state_ = gen_rnn_layer(in_, mask=mask) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? (5, 115, 24) (5, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Garbage code I don't want to throw out yet. if False: def call(self, inputs, mask=None, training=None, initial_state=None, constants=None): assert(mask is None), \"mask not supported.\" # First part copied from super call() # The input should be dense, padded with zeros. If a ragged input is fed # into the layer, it is padded and the row lengths are used for masking. inputs, row_lengths = K.convert_inputs_if_ragged(inputs) is_ragged_input = (row_lengths is not None) self._validate_args_if_ragged(is_ragged_input, mask) # Get initial_state. Merge provided initial_state and preserved if self.stateful, # otherwise use provided or zeros if provided is None. inputs, initial_state, constants = self._process_inputs( inputs, initial_state, constants) self._maybe_reset_cell_dropout_mask(self.cell) if isinstance(self.cell, tfkl.StackedRNNCells): for cell in self.cell.cells: self._maybe_reset_cell_dropout_mask(cell) kwargs = {} if generic_utils.has_arg(self.cell.call, 'training'): kwargs['training'] = training # TF RNN cells expect single tensor as state instead of list wrapped tensor. is_tf_rnn_cell = getattr(self.cell, '_is_tf_rnn_cell', None) is not None if constants: if not generic_utils.has_arg(self.cell.call, 'constants'): raise ValueError('RNN cell does not support constants') def step(inputs, states): constants = states[-self._num_constants:] # pylint: disable=invalid-unary-operand-type states = states[:-self._num_constants] # pylint: disable=invalid-unary-operand-type states = states[0] if len(states) == 1 and is_tf_rnn_cell else states output, new_states = self.cell.call( inputs, states, constants=constants, **kwargs) if not nest.is_sequence(new_states): new_states = [new_states] return output, new_states else: def step(inputs, states): states = states[0] if len(states) == 1 and is_tf_rnn_cell else states output, new_states = self.cell.call(inputs, states, **kwargs) if not nest.is_sequence(new_states): new_states = [new_states] return output, new_states # Begin deviation from super call() # ##################################### # We do not do K.rnn because it does not support feeding the output back as the input to the next step. def _process_single_input_t(input_t): input_t = tf.unstack(input_t, axis=-2) # unstack for time_step dim if self.go_backwards: input_t.reverse() return input_t if nest.is_sequence(inputs): processed_input = nest.map_structure(_process_single_input_t, inputs) else: processed_input = (_process_single_input_t(inputs),) cell_input = nest.pack_sequence_as(inputs, [_[0] for _ in processed_input]) cell_state = tuple(initial_state) out_states = [] out_inputs = [] for step_ix in range(self.timesteps): cell_input, new_states = step(cell_input, cell_state) flat_new_states = nest.flatten(new_states) cell_state = nest.pack_sequence_as(cell_state, flat_new_states) out_states.append(cell_state) out_inputs.append(cell_input) out_inputs = tf.stack(out_inputs, axis=-2) # if cell outputs a distribution, then we might do the following, but base class # would have to change. if False: if hasattr(out_inputs[0], 'parameters') and 'distribution' in out_inputs[0].parameters: dist0_parms = out_inputs[0].parameters['distribution'].parameters coll_parms = {} for p_name, p_val in dist0_parms.items(): if K.tensor_util.is_tensor(p_val): coll_parms[p_name] = [] for dist in out_inputs: for p_name in coll_parms.keys(): coll_parms[p_name].append(dist.parameters['distribution'].parameters[p_name]) for p_name in coll_parms.keys(): coll_parms[p_name] = tf.stack(coll_parms[p_name], axis=-2) dist_class = out_inputs[0].parameters['distribution'].__class__ out_inputs = dist_class(**coll_parms) # Warning! time dimension lost in batch with None out_inputs = tfp.distributions.Independent(out_inputs, reinterpreted_batch_ndims=1) out_states = tf.stack(out_states, axis=-2) out_states = tf.unstack(out_states, axis=0) if not hasattr(self.cell.state_size, '__len__'): out_states = out_states[0] if not self.return_sequences: out_inputs = out_inputs[..., -1, :] out_states = [_[..., -1, :] for _ in out_states] if isinstance(out_states, list) else out_states[..., -1, :] if self.return_state: return out_inputs, out_states return out_inputs","title":"Recurrent layers"},{"location":"bVAE/recurrent_layers/#custom-recurrent-layers","text":"A bit of a mess. Also check out tfp_utils, lfads_utils, and lfads_complex_cell.","title":"Custom Recurrent Layers"},{"location":"bVAE/recurrent_layers/#test-generativernn","text":"N_IN = 8 N_UNITS = 24 N_OUT_TIMESTEPS = 115 cell = tfkl.GRUCell # LSTMCell or GRUCell # Test regular RNN with zeros input reg_rnn_layer = tfkl.RNN(cell(N_UNITS), return_state=True, return_sequences=True) in_ = tf.zeros((1, 115, 16)) x_ = reg_rnn_layer(in_) print(K.any(x_[0])) # Just to remind myself that input zeros and state zeros will yield output zeros. tf.Tensor(False, shape=(), dtype=bool) # Test placeholder tensor with no timesteps K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tfkl.Input(shape=(N_IN,)) x_, cell_state_ = gen_rnn_layer(in_) print(\"Test placeholder tensor\") model = tf.keras.Model(inputs=in_, outputs=x_) model.summary() Test placeholder tensor Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 8)] 0 __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None, 1, 8)] 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 8)] 0 tf_op_layer_strided_slice[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike (TensorFl [(None, 8)] 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_2 (Te [(None, 1, 8)] 0 tf_op_layer_ZerosLike[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 8)] 0 tf_op_layer_strided_slice_2[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 115, 8)] 0 tf_op_layer_strided_slice[0][0] tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ generative_rnn (GenerativeRNN) [(None, 115, 24), (N 2448 tf_op_layer_concat[0][0] ================================================================================================== Total params: 2,448 Trainable params: 2,448 Non-trainable params: 0 __________________________________________________________________________________________________ # Test placeholder tensor with no timesteps as initial state K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tfkl.Input(shape=(N_UNITS,)) x_, cell_state_ = gen_rnn_layer(None, initial_state=in_) print(\"Test placeholder tensor\") model = tf.keras.Model(inputs=in_, outputs=x_) model.summary() Test placeholder tensor Model: \"model\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 24)] 0 __________________________________________________________________________________________________ tf_op_layer_strided_slice (Tens [(None,)] 0 input_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_1 (Te [(None, 1)] 0 tf_op_layer_strided_slice[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike (TensorFl [(None, 1)] 0 tf_op_layer_strided_slice_1[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_2 (Te [(None, 1, 1)] 0 tf_op_layer_ZerosLike[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_3 (Te [(None, 1)] 0 tf_op_layer_strided_slice_2[0][0] __________________________________________________________________________________________________ tf_op_layer_ZerosLike_1 (Tensor [(None, 1)] 0 tf_op_layer_strided_slice_3[0][0] __________________________________________________________________________________________________ tf_op_layer_strided_slice_4 (Te [(None, 1, 1)] 0 tf_op_layer_ZerosLike_1[0][0] __________________________________________________________________________________________________ tf_op_layer_AddV2 (TensorFlowOp [(None, 114, 1)] 0 tf_op_layer_strided_slice_4[0][0] __________________________________________________________________________________________________ tf_op_layer_concat (TensorFlowO [(None, 115, 1)] 0 tf_op_layer_strided_slice_2[0][0] tf_op_layer_AddV2[0][0] __________________________________________________________________________________________________ generative_rnn (GenerativeRNN) [(None, 115, 24), (N 1944 tf_op_layer_concat[0][0] input_1[0][0] ================================================================================================== Total params: 1,944 Trainable params: 1,944 Non-trainable params: 0 __________________________________________________________________________________________________ # Test None input --> uses zeros K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) print(\"Test None input\") x_, cell_state_ = gen_rnn_layer() print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? Test None input (1, 115, 24) (1, 24) tf.Tensor(False, shape=(), dtype=bool) tf.Tensor(False, shape=(), dtype=bool) # Test random input K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) in_ = tf.random.uniform((1, 8, N_UNITS), minval=-1.0, maxval=1.0) print(\"Test zeros input\") x_, cell_state_ = gen_rnn_layer(in_) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? Test zeros input (1, 115, 24) (1, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Test random states K.clear_session() gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS) print(gen_rnn_layer.compute_output_shape()) init_states = [tf.random.uniform((1, N_UNITS), minval=-1.0, maxval=1.0) for _ in range(1)] x_, cell_states_ = gen_rnn_layer(initial_state=init_states) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? [TensorShape([None, 115, 24]), TensorShape([None, 24])] (1, 115, 24) (1, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Test masking K.clear_session() tmp = tf.range(N_OUT_TIMESTEPS)[tf.newaxis, :, tf.newaxis] mask = tf.math.logical_or(tmp < 5, tmp > 100) gen_rnn_layer = GenerativeRNN(cell(N_UNITS), return_sequences=True, return_state=True, timesteps=N_OUT_TIMESTEPS, tile_input=True) in_ = tf.random.uniform((5, N_OUT_TIMESTEPS, N_UNITS), minval=-1.0, maxval=1.0) x_, cell_state_ = gen_rnn_layer(in_, mask=mask) print(x_.shape, cell_state_.shape) print(K.any(x_), K.any(cell_state_)) # <- any non-zero values? (5, 115, 24) (5, 24) tf.Tensor(True, shape=(), dtype=bool) tf.Tensor(True, shape=(), dtype=bool) # Garbage code I don't want to throw out yet. if False: def call(self, inputs, mask=None, training=None, initial_state=None, constants=None): assert(mask is None), \"mask not supported.\" # First part copied from super call() # The input should be dense, padded with zeros. If a ragged input is fed # into the layer, it is padded and the row lengths are used for masking. inputs, row_lengths = K.convert_inputs_if_ragged(inputs) is_ragged_input = (row_lengths is not None) self._validate_args_if_ragged(is_ragged_input, mask) # Get initial_state. Merge provided initial_state and preserved if self.stateful, # otherwise use provided or zeros if provided is None. inputs, initial_state, constants = self._process_inputs( inputs, initial_state, constants) self._maybe_reset_cell_dropout_mask(self.cell) if isinstance(self.cell, tfkl.StackedRNNCells): for cell in self.cell.cells: self._maybe_reset_cell_dropout_mask(cell) kwargs = {} if generic_utils.has_arg(self.cell.call, 'training'): kwargs['training'] = training # TF RNN cells expect single tensor as state instead of list wrapped tensor. is_tf_rnn_cell = getattr(self.cell, '_is_tf_rnn_cell', None) is not None if constants: if not generic_utils.has_arg(self.cell.call, 'constants'): raise ValueError('RNN cell does not support constants') def step(inputs, states): constants = states[-self._num_constants:] # pylint: disable=invalid-unary-operand-type states = states[:-self._num_constants] # pylint: disable=invalid-unary-operand-type states = states[0] if len(states) == 1 and is_tf_rnn_cell else states output, new_states = self.cell.call( inputs, states, constants=constants, **kwargs) if not nest.is_sequence(new_states): new_states = [new_states] return output, new_states else: def step(inputs, states): states = states[0] if len(states) == 1 and is_tf_rnn_cell else states output, new_states = self.cell.call(inputs, states, **kwargs) if not nest.is_sequence(new_states): new_states = [new_states] return output, new_states # Begin deviation from super call() # ##################################### # We do not do K.rnn because it does not support feeding the output back as the input to the next step. def _process_single_input_t(input_t): input_t = tf.unstack(input_t, axis=-2) # unstack for time_step dim if self.go_backwards: input_t.reverse() return input_t if nest.is_sequence(inputs): processed_input = nest.map_structure(_process_single_input_t, inputs) else: processed_input = (_process_single_input_t(inputs),) cell_input = nest.pack_sequence_as(inputs, [_[0] for _ in processed_input]) cell_state = tuple(initial_state) out_states = [] out_inputs = [] for step_ix in range(self.timesteps): cell_input, new_states = step(cell_input, cell_state) flat_new_states = nest.flatten(new_states) cell_state = nest.pack_sequence_as(cell_state, flat_new_states) out_states.append(cell_state) out_inputs.append(cell_input) out_inputs = tf.stack(out_inputs, axis=-2) # if cell outputs a distribution, then we might do the following, but base class # would have to change. if False: if hasattr(out_inputs[0], 'parameters') and 'distribution' in out_inputs[0].parameters: dist0_parms = out_inputs[0].parameters['distribution'].parameters coll_parms = {} for p_name, p_val in dist0_parms.items(): if K.tensor_util.is_tensor(p_val): coll_parms[p_name] = [] for dist in out_inputs: for p_name in coll_parms.keys(): coll_parms[p_name].append(dist.parameters['distribution'].parameters[p_name]) for p_name in coll_parms.keys(): coll_parms[p_name] = tf.stack(coll_parms[p_name], axis=-2) dist_class = out_inputs[0].parameters['distribution'].__class__ out_inputs = dist_class(**coll_parms) # Warning! time dimension lost in batch with None out_inputs = tfp.distributions.Independent(out_inputs, reinterpreted_batch_ndims=1) out_states = tf.stack(out_states, axis=-2) out_states = tf.unstack(out_states, axis=0) if not hasattr(self.cell.state_size, '__len__'): out_states = out_states[0] if not self.return_sequences: out_inputs = out_inputs[..., -1, :] out_states = [_[..., -1, :] for _ in out_states] if isinstance(out_states, list) else out_states[..., -1, :] if self.return_state: return out_inputs, out_states return out_inputs","title":"Test GenerativeRNN"},{"location":"bVAE/tfp_utils/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Tensorflow Probability Utilities This notebook is a bit of a mess after the refactor. # https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/disentangled_vae.py K . clear_session () K . set_floatx ( 'float32' ) def test_make_mvn_prior ( latent_size = 4 , init_std = 0.1 , trainable_mean = True , trainable_var = True , prior_offdiag = False ): prior = make_mvn_prior ( latent_size , init_std = init_std , trainable_mean = trainable_mean , trainable_var = trainable_var , offdiag = prior_offdiag ) assert ( isinstance ( prior . loc , tf . Variable ) == trainable_mean ) if prior_offdiag : assert ( hasattr ( prior . scale_tril , 'trainable_variables' ) == trainable_var ) else : assert (( len ( prior . scale . trainable_variables ) > 0 ) == trainable_var ) print ( prior . mean (), prior . stddev ()) for trainable_mean in [ True , False ]: for trainable_var in [ True , False ]: for prior_offdiag in [ True , False ]: test_make_mvn_prior ( trainable_mean = trainable_mean , trainable_var = trainable_var , prior_offdiag = prior_offdiag ) tf.Tensor([-0.08141219 -0.0387085 -0.05836845 -0.00457149], shape=(4,), dtype=float32) tf.Tensor([0.13052301 0.23032661 0.31772745 0.2098567 ], shape=(4,), dtype=float32) tf.Tensor([-0.07178359 -0.07984201 0.07635907 -0.09406286], shape=(4,), dtype=float32) tf.Tensor([0.10905064 0.103502 0.09993087 0.10073646], shape=(4,), dtype=float32) tf.Tensor([-0.1242888 0.05545203 -0.18831097 -0.02664005], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0.18107729 0.14778917 0.1890041 0.20096827], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.10471911 0.23243366 0.32148966 0.18541414], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.11602824 0.11143655 0.09993648 0.09902043], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) K . clear_session () test_make_mvn_dist_fn () q_dist: tfp.distributions.MultivariateNormalDiag(\"model_distribution_lambda_MultivariateNormalDiag\", batch_shape=[6], event_shape=[4], dtype=float32) q_dist stddev: tf.Tensor( [[0.12080275 0.16515851 0.13489766 0.06180337] [0.0738408 0.12705617 0.09915603 0.06687246] [0.10365839 0.16512334 0.09549066 0.15812276] [0.09845882 0.1333666 0.1458401 0.08260284] [0.10875075 0.14236958 0.21499178 0.09950284] [0.15298602 0.13276467 0.1905963 0.18955594]], shape=(6, 4), dtype=float32) q_dist sample: tf.Tensor( [[ 1.2572958 -1.079319 0.05739563 0.27565804] [ 0.9092588 -0.37485668 0.31609604 0.25275287] [ 0.19276637 -0.7746772 -0.3904564 0.34959695] [ 0.48656625 -0.78282547 -0.2158188 0.271042 ] [ 1.3095855 -0.9227902 0.38678312 0.59275925] [ 0.83026767 -0.9097708 0.10051005 0.40007213]], shape=(6, 4), dtype=float32) K . clear_session () test_make_variational () --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op (graph, node_def, inputs, control_inputs, op_def) 1653 try : -> 1654 c_op = pywrap_tf_session . TF_FinishOperation ( op_desc ) 1655 except errors . InvalidArgumentError as e : InvalidArgumentError : Duplicate node name in graph: 'concat/values_0' During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 408 dtype = dtypes . as_dtype ( dtype ) . base_dtype --> 409 values = ops.internal_convert_n_to_tensor( 410 values , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor (values, dtype, name, as_ref, preferred_dtype, ctx) 1402 ret.append( -> 1403 convert_to_tensor( 1404 value , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor (value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types) 1340 if ret is None : -> 1341 ret = conversion_func ( value , dtype = dtype , name = name , as_ref = as_ref ) 1342 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function (v, dtype, name, as_ref) 1454 v = nest . map_structure ( _cast_nested_seqs_to_dtype ( dtype ) , v ) -> 1455 return _autopacking_helper ( v , dtype , name or \"packed\" ) 1456 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper (list_or_tuple, dtype, name) 1390 constant_op.constant(elem, dtype=dtype, name=str(i))) -> 1391 return gen_array_ops . pack ( elems_as_tensors , name = scope ) 1392 else : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in pack (values, axis, name) 6346 axis = _execute . make_int ( axis , \"axis\" ) -> 6347 _, _, _op, _outputs = _op_def_library._apply_op_helper( 6348 \"Pack\", values=values, axis=axis, name=name) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 741 # pylint: disable=protected-access --> 742 op = g._create_op_internal(op_type_name, inputs, dtypes=None, 743 name = scope , input_types = input_types , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in _create_op_internal (self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 592 inputs [ i ] = inp --> 593 return super(FuncGraph, self)._create_op_internal( # pylint: disable=protected-access 594 op_type , inputs , dtypes , input_types , name , attrs , op_def , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_op_internal (self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 3318 with self . _mutation_lock ( ) : -> 3319 ret = Operation( 3320 node_def , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __init__ (self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def) 1815 op_def = self . _graph . _get_op_def ( node_def . op ) -> 1816 self._c_op = _create_c_op(self._graph, node_def, inputs, 1817 control_input_ops, op_def) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op (graph, node_def, inputs, control_inputs, op_def) 1656 # Convert to ValueError for backwards compatibility. -> 1657 raise ValueError ( str ( e ) ) 1658 ValueError : Duplicate node name in graph: 'concat/values_0' During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) <ipython-input-60-f531da3a8495> in <module> 1 K . clear_session ( ) ----> 2 test_make_variational ( ) <ipython-input-59-7ba46b6f4e26> in test_make_variational (input_dim, dist_dim, batch_size) 3 inputs = tfkl . Input ( shape = ( input_dim , ) ) 4 q_dist = make_variational ( inputs , dist_dim , init_std = 0.1 ) ----> 5 print ( q_dist , q_dist . sample ( ) . shape ) 6 7 #model = tf.keras.Model(inputs=inputs, outputs=q_dist) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in sample (self, sample_shape, seed, name, **kwargs) 935 samples : a ` Tensor ` with prepended dimensions ` sample_shape ` . 936 \"\"\" --> 937 return self . _call_sample_n ( sample_shape , seed , name , ** kwargs ) 938 939 def _call_log_prob ( self , value , name , ** kwargs ) : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _call_sample_n (self, sample_shape, seed, name, **kwargs) 464 # event that we need to reinterpret the samples as part of the 465 # event_shape. --> 466 x = self . _sample_n ( n , seed , ** distribution_kwargs ) 467 468 # Next, we reshape `x` into its final form. We do this prior to the call ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _sample_n (self, n, seed, **distribution_kwargs) 442 distribution_util . pick_vector ( needs_rotation , [ n ] , self . _empty ) , 443 ], axis=0) --> 444 x = self.distribution.sample(sample_shape=sample_shape, seed=seed, 445 **distribution_kwargs) 446 x = self._maybe_rotate_dims( ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in sample (self, sample_shape, seed, name, **kwargs) 935 samples : a ` Tensor ` with prepended dimensions ` sample_shape ` . 936 \"\"\" --> 937 return self . _call_sample_n ( sample_shape , seed , name , ** kwargs ) 938 939 def _call_log_prob ( self , value , name , ** kwargs ) : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in _call_sample_n (self, sample_shape, seed, name, **kwargs) 912 sample_shape, n = self._expand_sample_shape_to_vector( 913 sample_shape, 'sample_shape') --> 914 samples = self._sample_n( 915 n, seed=seed() if callable(seed) else seed, **kwargs) 916 batch_event_shape = tf . shape ( samples ) [ 1 : ] ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/normal.py in _sample_n (self, n, seed) 184 loc = tf . convert_to_tensor ( self . loc ) 185 scale = tf . convert_to_tensor ( self . scale ) --> 186 shape = tf.concat([[n], self._batch_shape_tensor(loc=loc, scale=scale)], 187 axis=0) 188 sampled = tf.random.normal( ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper (*args, **kwargs) 178 \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\" 179 try : --> 180 return target ( * args , ** kwargs ) 181 except ( TypeError , ValueError ) : 182 # Note: convert_to_eager_tensor currently raises a ValueError, not a ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in concat (values, axis, name) 1604 dtype=dtypes.int32).get_shape().assert_has_rank(0) 1605 return identity ( values [ 0 ] , name = name ) -> 1606 return gen_array_ops . concat_v2 ( values = values , axis = axis , name = name ) 1607 1608 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2 (values, axis, name) 1186 \"'concat_v2' Op, not %r.\" % values) 1187 _attr_N = len ( values ) -> 1188 _, _, _op, _outputs = _op_def_library._apply_op_helper( 1189 \"ConcatV2\", values=values, axis=axis, name=name) 1190 _result = _outputs [ : ] ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 440 (prefix, dtype.name)) 441 else : --> 442 raise TypeError ( \"%s that don't all match.\" % prefix ) 443 else : 444 raise TypeError( TypeError : Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, int32] that don't all match. # An example of how this would work in a variational autoencoder. N_TIMES = 10 N_SENSORS = 8 N_SAMPLES = 2 N_HIDDEN = 5 KL_WEIGHT = 0.05 t_vec = tf . range ( N_TIMES , dtype = tf . float32 ) / N_TIMES sig_vec = 1 + tf . exp ( - 10 * ( t_vec - 0.5 )) def make_model ( prior ): input_ = tfkl . Input ( shape = ( LATENT_SIZE ,)) # Encoder make_latent_dist_fn , latent_params = make_mvn_dist_fn ( input_ , LATENT_SIZE , offdiag = True , loc_name = \"latent_loc\" ) q_latent = tfpl . DistributionLambda ( name = \"q_latent\" , make_distribution_fn = make_latent_dist_fn , convert_to_tensor_fn = lambda s : s . sample ( N_SAMPLES ), activity_regularizer = tfpl . KLDivergenceRegularizer ( prior , use_exact_kl = True , weight = KL_WEIGHT ) )( latent_params ) # Decoder y_ = q_latent [ ... , tf . newaxis , :] / sig_vec [:, tf . newaxis ] # broadcast-add zeros to restore timesteps #y_ = q_latent[..., tf.newaxis, :] + tf.zeros([N_TIMES, 1]) #y_ = tf.reshape(y_, [-1, N_TIMES, LATENT_SIZE]) #y_ = tfkl.LSTM(N_HIDDEN, return_sequences=True)(y_) #y_ = tf.reshape(y_, [N_SAMPLES, -1, N_TIMES, N_HIDDEN]) make_out_dist_fn , out_dist_params = make_mvn_dist_fn ( y_ , N_SENSORS , loc_name = \"out_loc\" ) p_out = tfpl . DistributionLambda ( make_distribution_fn = make_out_dist_fn , name = \"p_out\" )( out_dist_params ) # no prior on the output. # Model model = tf . keras . Model ( inputs = input_ , outputs = [ q_latent , p_out ]) return model # Create a fake dataset to train the model. LATENT_SIZE = 4 BATCH_SIZE = 6 # The latents are sampled from a distribution with known parameters. true_dist = tfd . MultivariateNormalDiag ( loc = [ - 1. , 1. , 5 , - 5 ], # must have length == LATENT_SIZE scale_diag = [ 0.5 , 0.5 , 0.9 , 0.2 ] ) # They parameterize sigmoid end points, from indl.misc.sigfuncs import sigmoid from functools import partial t_vec = ( np . arange ( N_TIMES , dtype = np . float32 ) / N_TIMES )[ None , :] f_sig = partial ( sigmoid , t_vec , B = 10 , x_offset = 0.5 ) # which are then mixed with a known mixing matrix mix_mat = np . array ([ [ - 0.3 , -. 28 , - 0.38 , - 0.45 , - 0.02 , - 0.12 , - 0.05 , - 0.48 ], [ 0.27 , 0.29 , - 0.34 , 0.2 , 0.41 , 0.08 , 0.11 , 0.13 ], [ - 0.14 , 0.26 , - 0.28 , - 0.14 , 0.1 , - 0.2 , 0.4 , 0.11 ], [ - 0.05 , - 0.12 , 0.28 , 0.49 , - 0.12 , 0.1 , 0.17 , 0.22 ] ], dtype = np . float32 ) . T #mix_mat = tf.convert_to_tensor(mix_mat) def gen_ds ( n_iters = 1e2 , latent_size = LATENT_SIZE ): iter_ix = 0 while iter_ix < n_iters : _input = tf . ones (( latent_size ,), dtype = tf . float32 ) latent = true_dist . sample () . numpy () _y = np . reshape ( latent , [ latent_size , 1 ]) _y = f_sig ( K = _y ) _y = mix_mat @ _y _y = _y . T yield _input , _y iter_ix += 1 ds = tf . data . Dataset . from_generator ( gen_ds , args = [ 1e2 ], output_types = ( tf . float32 , tf . float32 ), output_shapes = (( LATENT_SIZE ,), ( N_TIMES , N_SENSORS ))) ds = ds . map ( lambda x , y : ( x , ( tf . zeros ( 0 , dtype = tf . float32 ), y ))) . batch ( BATCH_SIZE ) # Train the model. # Try playing around with the 2nd loss_weights (below) and KL_WEIGHT (above). N_EPOCHS = 100 K . clear_session () prior = make_mvn_prior ( LATENT_SIZE , trainable_mean = True , trainable_var = True , offdiag = False ) model_ = make_model ( prior ) model_ . compile ( optimizer = 'adam' , loss = [ lambda _ , model_latent : tfd . kl_divergence ( model_latent , prior ), lambda y_true , model_out : - model_out . log_prob ( y_true )], loss_weights = [ 0.0 , 1.0 ]) hist = model_ . fit ( ds , epochs = N_EPOCHS , verbose = 2 ) Epoch 1/100 17/17 - 0s - loss: 91.9389 - q_latent_loss: 4.6523 - p_out_loss: 90.5613 Epoch 2/100 17/17 - 0s - loss: 11531.2471 - q_latent_loss: 4.3536 - p_out_loss: 11529.9590 Epoch 3/100 17/17 - 0s - loss: 743.5315 - q_latent_loss: 4.1376 - p_out_loss: 742.3065 Epoch 4/100 17/17 - 0s - loss: 137048.2969 - q_latent_loss: 3.9767 - p_out_loss: 137047.1250 Epoch 5/100 17/17 - 0s - loss: 113.2025 - q_latent_loss: 3.8272 - p_out_loss: 112.0695 Epoch 6/100 17/17 - 0s - loss: 74.9116 - q_latent_loss: 3.7352 - p_out_loss: 73.8058 Epoch 7/100 17/17 - 0s - loss: 79.9207 - q_latent_loss: 3.6573 - p_out_loss: 78.8380 Epoch 8/100 17/17 - 0s - loss: 372.0323 - q_latent_loss: 3.5850 - p_out_loss: 370.9711 Epoch 9/100 17/17 - 0s - loss: 60.3690 - q_latent_loss: 3.5167 - p_out_loss: 59.3279 Epoch 10/100 17/17 - 0s - loss: 7418.3252 - q_latent_loss: 3.4512 - p_out_loss: 7417.3027 Epoch 11/100 17/17 - 0s - loss: 3513.5659 - q_latent_loss: 3.3824 - p_out_loss: 3512.5649 Epoch 12/100 17/17 - 0s - loss: 53.7796 - q_latent_loss: 3.3220 - p_out_loss: 52.7962 Epoch 13/100 17/17 - 0s - loss: 20.4849 - q_latent_loss: 3.2672 - p_out_loss: 19.5177 Epoch 14/100 17/17 - 0s - loss: 1562.3738 - q_latent_loss: 3.2157 - p_out_loss: 1561.4218 Epoch 15/100 17/17 - 0s - loss: 963.6710 - q_latent_loss: 3.1652 - p_out_loss: 962.7339 Epoch 16/100 17/17 - 0s - loss: 2838.6292 - q_latent_loss: 3.1165 - p_out_loss: 2837.7068 Epoch 17/100 17/17 - 0s - loss: 35.5000 - q_latent_loss: 3.0708 - p_out_loss: 34.5910 Epoch 18/100 17/17 - 0s - loss: 706.3328 - q_latent_loss: 3.0287 - p_out_loss: 705.4363 Epoch 19/100 17/17 - 0s - loss: 52.2510 - q_latent_loss: 2.9887 - p_out_loss: 51.3663 Epoch 20/100 17/17 - 0s - loss: 7950.6792 - q_latent_loss: 2.9512 - p_out_loss: 7949.8062 Epoch 21/100 17/17 - 0s - loss: 190.5290 - q_latent_loss: 2.9112 - p_out_loss: 189.6672 Epoch 22/100 17/17 - 0s - loss: 27.1404 - q_latent_loss: 2.8756 - p_out_loss: 26.2891 Epoch 23/100 17/17 - 0s - loss: 222.1122 - q_latent_loss: 2.8434 - p_out_loss: 221.2705 Epoch 24/100 17/17 - 0s - loss: 26.6817 - q_latent_loss: 2.8132 - p_out_loss: 25.8489 Epoch 25/100 17/17 - 0s - loss: 301.0465 - q_latent_loss: 2.7844 - p_out_loss: 300.2223 Epoch 26/100 17/17 - 0s - loss: 17.3472 - q_latent_loss: 2.7568 - p_out_loss: 16.5312 Epoch 27/100 17/17 - 0s - loss: 47.1069 - q_latent_loss: 2.7306 - p_out_loss: 46.2986 Epoch 28/100 17/17 - 0s - loss: 34.0318 - q_latent_loss: 2.7057 - p_out_loss: 33.2309 Epoch 29/100 17/17 - 0s - loss: 25.4790 - q_latent_loss: 2.6819 - p_out_loss: 24.6851 Epoch 30/100 17/17 - 0s - loss: 35.2237 - q_latent_loss: 2.6592 - p_out_loss: 34.4365 Epoch 31/100 17/17 - 0s - loss: 17.7497 - q_latent_loss: 2.6375 - p_out_loss: 16.9690 Epoch 32/100 17/17 - 0s - loss: 19.0096 - q_latent_loss: 2.6168 - p_out_loss: 18.2350 Epoch 33/100 17/17 - 0s - loss: 17.3889 - q_latent_loss: 2.5969 - p_out_loss: 16.6202 Epoch 34/100 17/17 - 0s - loss: 3164.2163 - q_latent_loss: 2.5783 - p_out_loss: 3163.4539 Epoch 35/100 17/17 - 0s - loss: 318.1325 - q_latent_loss: 2.5614 - p_out_loss: 317.3743 Epoch 36/100 17/17 - 0s - loss: 496.7764 - q_latent_loss: 2.5442 - p_out_loss: 496.0233 Epoch 37/100 17/17 - 0s - loss: 43.8554 - q_latent_loss: 2.5274 - p_out_loss: 43.1073 Epoch 38/100 17/17 - 0s - loss: 127.4449 - q_latent_loss: 2.5114 - p_out_loss: 126.7015 Epoch 39/100 17/17 - 0s - loss: 34.3881 - q_latent_loss: 2.4962 - p_out_loss: 33.6492 Epoch 40/100 17/17 - 0s - loss: 92.4194 - q_latent_loss: 2.4816 - p_out_loss: 91.6848 Epoch 41/100 17/17 - 0s - loss: 2711.8994 - q_latent_loss: 2.4669 - p_out_loss: 2711.1697 Epoch 42/100 17/17 - 0s - loss: 32.7170 - q_latent_loss: 2.4484 - p_out_loss: 31.9922 Epoch 43/100 17/17 - 0s - loss: 368.3795 - q_latent_loss: 2.4338 - p_out_loss: 367.6590 Epoch 44/100 17/17 - 0s - loss: 185.1732 - q_latent_loss: 2.4207 - p_out_loss: 184.4566 Epoch 45/100 17/17 - 0s - loss: 1496.3325 - q_latent_loss: 2.4090 - p_out_loss: 1495.6195 Epoch 46/100 17/17 - 0s - loss: 40.2950 - q_latent_loss: 2.3979 - p_out_loss: 39.5852 Epoch 47/100 17/17 - 0s - loss: 3341.2693 - q_latent_loss: 2.3869 - p_out_loss: 3340.5627 Epoch 48/100 17/17 - 0s - loss: 317.7818 - q_latent_loss: 2.3705 - p_out_loss: 317.0801 Epoch 49/100 17/17 - 0s - loss: 129.0085 - q_latent_loss: 2.3577 - p_out_loss: 128.3107 Epoch 50/100 17/17 - 0s - loss: 677.0181 - q_latent_loss: 2.3475 - p_out_loss: 676.3232 Epoch 51/100 17/17 - 0s - loss: 44.6301 - q_latent_loss: 2.3379 - p_out_loss: 43.9381 Epoch 52/100 17/17 - 0s - loss: 13.6527 - q_latent_loss: 2.3290 - p_out_loss: 12.9633 Epoch 53/100 17/17 - 0s - loss: 2174.0337 - q_latent_loss: 2.3206 - p_out_loss: 2173.3469 Epoch 54/100 17/17 - 0s - loss: 85.2265 - q_latent_loss: 2.3118 - p_out_loss: 84.5422 Epoch 55/100 17/17 - 0s - loss: 13.3743 - q_latent_loss: 2.3037 - p_out_loss: 12.6924 Epoch 56/100 17/17 - 0s - loss: 46996.4805 - q_latent_loss: 2.2929 - p_out_loss: 46995.8008 Epoch 57/100 17/17 - 0s - loss: 23.0730 - q_latent_loss: 2.2527 - p_out_loss: 22.4061 Epoch 58/100 17/17 - 0s - loss: 11.3692 - q_latent_loss: 2.2361 - p_out_loss: 10.7074 Epoch 59/100 17/17 - 0s - loss: 25.0329 - q_latent_loss: 2.2285 - p_out_loss: 24.3733 Epoch 60/100 17/17 - 0s - loss: 17.7706 - q_latent_loss: 2.2227 - p_out_loss: 17.1126 Epoch 61/100 17/17 - 0s - loss: 10.4981 - q_latent_loss: 2.2176 - p_out_loss: 9.8417 Epoch 62/100 17/17 - 0s - loss: 32.7279 - q_latent_loss: 2.2127 - p_out_loss: 32.0729 Epoch 63/100 17/17 - 0s - loss: 30.3548 - q_latent_loss: 2.2081 - p_out_loss: 29.7012 Epoch 64/100 17/17 - 0s - loss: 30.8280 - q_latent_loss: 2.2037 - p_out_loss: 30.1757 Epoch 65/100 17/17 - 0s - loss: 41.9700 - q_latent_loss: 2.1996 - p_out_loss: 41.3189 Epoch 66/100 17/17 - 0s - loss: 54.7435 - q_latent_loss: 2.1956 - p_out_loss: 54.0936 Epoch 67/100 17/17 - 0s - loss: 43.0750 - q_latent_loss: 2.1918 - p_out_loss: 42.4262 Epoch 68/100 17/17 - 0s - loss: 243.1561 - q_latent_loss: 2.1882 - p_out_loss: 242.5083 Epoch 69/100 17/17 - 0s - loss: 22.0350 - q_latent_loss: 2.1842 - p_out_loss: 21.3885 Epoch 70/100 17/17 - 0s - loss: 99.8952 - q_latent_loss: 2.1805 - p_out_loss: 99.2498 Epoch 71/100 17/17 - 0s - loss: 31.7489 - q_latent_loss: 2.1772 - p_out_loss: 31.1044 Epoch 72/100 17/17 - 0s - loss: 10.3156 - q_latent_loss: 2.1741 - p_out_loss: 9.6721 Epoch 73/100 17/17 - 0s - loss: 10.8083 - q_latent_loss: 2.1710 - p_out_loss: 10.1656 Epoch 74/100 17/17 - 0s - loss: 149.1215 - q_latent_loss: 2.1682 - p_out_loss: 148.4797 Epoch 75/100 17/17 - 0s - loss: 11.4474 - q_latent_loss: 2.1655 - p_out_loss: 10.8064 Epoch 76/100 17/17 - 0s - loss: 11.3532 - q_latent_loss: 2.1629 - p_out_loss: 10.7130 Epoch 77/100 17/17 - 0s - loss: 9.8263 - q_latent_loss: 2.1603 - p_out_loss: 9.1868 Epoch 78/100 17/17 - 0s - loss: 12.8481 - q_latent_loss: 2.1578 - p_out_loss: 12.2094 Epoch 79/100 17/17 - 0s - loss: 10.0819 - q_latent_loss: 2.1554 - p_out_loss: 9.4439 Epoch 80/100 17/17 - 0s - loss: 9.9589 - q_latent_loss: 2.1531 - p_out_loss: 9.3216 Epoch 81/100 17/17 - 0s - loss: 148.1879 - q_latent_loss: 2.1508 - p_out_loss: 147.5513 Epoch 82/100 17/17 - 0s - loss: 13.6919 - q_latent_loss: 2.1483 - p_out_loss: 13.0560 Epoch 83/100 17/17 - 0s - loss: 11.4130 - q_latent_loss: 2.1462 - p_out_loss: 10.7777 Epoch 84/100 17/17 - 0s - loss: 62.1876 - q_latent_loss: 2.1442 - p_out_loss: 61.5529 Epoch 85/100 17/17 - 0s - loss: 37.1363 - q_latent_loss: 2.1424 - p_out_loss: 36.5021 Epoch 86/100 17/17 - 0s - loss: 9.0239 - q_latent_loss: 2.1406 - p_out_loss: 8.3903 Epoch 87/100 17/17 - 0s - loss: 9.6573 - q_latent_loss: 2.1389 - p_out_loss: 9.0242 Epoch 88/100 17/17 - 0s - loss: 16.5087 - q_latent_loss: 2.1371 - p_out_loss: 15.8761 Epoch 89/100 17/17 - 0s - loss: 9.0933 - q_latent_loss: 2.1355 - p_out_loss: 8.4612 Epoch 90/100 17/17 - 0s - loss: 11.1045 - q_latent_loss: 2.1339 - p_out_loss: 10.4729 Epoch 91/100 17/17 - 0s - loss: 14.6627 - q_latent_loss: 2.1323 - p_out_loss: 14.0315 Epoch 92/100 17/17 - 0s - loss: 12.7246 - q_latent_loss: 2.1308 - p_out_loss: 12.0939 Epoch 93/100 17/17 - 0s - loss: 18.2482 - q_latent_loss: 2.1294 - p_out_loss: 17.6179 Epoch 94/100 17/17 - 0s - loss: 12.0889 - q_latent_loss: 2.1280 - p_out_loss: 11.4590 Epoch 95/100 17/17 - 0s - loss: 35.7015 - q_latent_loss: 2.1267 - p_out_loss: 35.0720 Epoch 96/100 17/17 - 0s - loss: 156.0881 - q_latent_loss: 2.1253 - p_out_loss: 155.4590 Epoch 97/100 17/17 - 0s - loss: 8.8568 - q_latent_loss: 2.1240 - p_out_loss: 8.2280 Epoch 98/100 17/17 - 0s - loss: 10.9563 - q_latent_loss: 2.1228 - p_out_loss: 10.3279 Epoch 99/100 17/17 - 0s - loss: 10.1586 - q_latent_loss: 2.1217 - p_out_loss: 9.5306 Epoch 100/100 17/17 - 0s - loss: 14.0194 - q_latent_loss: 2.1206 - p_out_loss: 13.3917 lat_wts = model_ . get_layer ( \"latent_loc\" ) . weights lat_locs = np . ones (( 1 , LATENT_SIZE )) @ lat_wts [ 0 ] . numpy () + lat_wts [ 1 ] . numpy () mix_wts = model_ . get_layer ( \"out_loc\" ) . weights model_out = lat_locs @ mix_wts [ 0 ] . numpy () + mix_wts [ 1 ] . numpy () true_out = mix_mat @ true_dist . mean () . numpy () print ( f \"Model est lat: { lat_locs } \" ) print ( f \"Model est out: { model_out } \" ) print ( f \"prior mean: { prior . mean () . numpy () } \" ) print ( f \"true lat: { true_dist . mean () . numpy () } \" ) print ( f \"true out: { true_out . T } \" ) Model est lat: [[ 0.51195845 -1.02728739 0.67845761 -0.11073773]] Model est out: [[ 0.10048061 1.36010552 0.30864524 -0.09840383 1.17217551 -0.67099368 0.95406085 0.03414997]] prior mean: [ 0.5117957 -0.8991166 0.66152537 -0.11197621] true lat: [-1. 1. 5. -5.] true out: [ 0.12000006 2.4699998 -2.76 -2.5 1.53 -1.3 1.31 0.05999994] # test LearnableMultivariateNormalDiag prior_factory = LearnableMultivariateNormalDiag ( LATENT_SIZE ) learnable_prior = prior_factory () sample = learnable_prior . sample (( 100 , 64 )) print ( sample . shape ) print ( learnable_prior . trainable_variables ) (100, 64, 4) (<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 , 0.0387747 , 0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143, 0.06062925, 0.02439827, -0.01735771], dtype=float32)>) K . clear_session () model_ = make_model ( learnable_prior ) model_ . compile ( optimizer = 'adam' , loss = [ lambda _ , model_latent : tfd . kl_divergence ( model_latent , learnable_prior ), lambda y_true , model_out : - model_out . log_prob ( y_true )], loss_weights = [ 0.0 , 1.0 ]) print ( learnable_prior . trainable_variables ) print ([ _ . name for _ in model_ . trainable_variables ]) hist = model_ . fit ( ds , epochs = N_EPOCHS , verbose = 2 ) lat_wts = model_ . get_layer ( \"latent_loc\" ) . weights lat_locs = np . ones (( 1 , LATENT_SIZE )) @ lat_wts [ 0 ] . numpy () + lat_wts [ 1 ] . numpy () mix_wts = model_ . get_layer ( \"out_loc\" ) . weights model_out = lat_locs @ mix_wts [ 0 ] . numpy () + mix_wts [ 1 ] . numpy () true_out = mix_mat @ true_dist . mean () . numpy () print ( f \"Model est lat: { lat_locs } \" ) print ( f \"Model est out: { model_out } \" ) print ( f \"prior mean: { learnable_prior . mean () . numpy () } \" ) print ( f \"true lat: { true_dist . mean () . numpy () } \" ) print ( f \"true out: { true_out . T } \" ) (<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 , 0.0387747 , 0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143, 0.06062925, 0.02439827, -0.01735771], dtype=float32)>) ['dense/kernel:0', 'dense/bias:0', 'latent_loc/kernel:0', 'latent_loc/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'out_loc/kernel:0', 'out_loc/bias:0', 'learnable_multivariate_normal_diag_2/mean:0', 'learnable_multivariate_normal_diag_2/transformed_scale:0'] Epoch 1/100 17/17 - 0s - loss: 266.6295 - q_latent_loss: 6.5640 - p_out_loss: 264.6859 Epoch 2/100 17/17 - 0s - loss: 2032.0966 - q_latent_loss: 6.2853 - p_out_loss: 2030.2358 Epoch 3/100 17/17 - 0s - loss: 83.5799 - q_latent_loss: 6.0718 - p_out_loss: 81.7824 Epoch 4/100 17/17 - 0s - loss: 82.7522 - q_latent_loss: 5.8942 - p_out_loss: 81.0072 Epoch 5/100 17/17 - 0s - loss: 59.2224 - q_latent_loss: 5.7269 - p_out_loss: 57.5269 Epoch 6/100 17/17 - 0s - loss: 38.8948 - q_latent_loss: 5.5706 - p_out_loss: 37.2456 Epoch 7/100 17/17 - 0s - loss: 47.8537 - q_latent_loss: 5.4227 - p_out_loss: 46.2483 Epoch 8/100 17/17 - 0s - loss: 60.9186 - q_latent_loss: 5.2828 - p_out_loss: 59.3546 Epoch 9/100 17/17 - 0s - loss: 80.7008 - q_latent_loss: 5.1479 - p_out_loss: 79.1768 Epoch 10/100 17/17 - 0s - loss: 29.5548 - q_latent_loss: 5.0204 - p_out_loss: 28.0686 Epoch 11/100 17/17 - 0s - loss: 100.5337 - q_latent_loss: 4.9013 - p_out_loss: 99.0827 Epoch 12/100 17/17 - 0s - loss: 208.5356 - q_latent_loss: 4.7856 - p_out_loss: 207.1189 Epoch 13/100 17/17 - 0s - loss: 47.4895 - q_latent_loss: 4.6692 - p_out_loss: 46.1072 Epoch 14/100 17/17 - 0s - loss: 51.8070 - q_latent_loss: 4.5624 - p_out_loss: 50.4563 Epoch 15/100 17/17 - 0s - loss: 49.2825 - q_latent_loss: 4.4640 - p_out_loss: 47.9610 Epoch 16/100 17/17 - 0s - loss: 63.7341 - q_latent_loss: 4.3716 - p_out_loss: 62.4399 Epoch 17/100 17/17 - 0s - loss: 35.2299 - q_latent_loss: 4.2837 - p_out_loss: 33.9617 Epoch 18/100 17/17 - 0s - loss: 45.8432 - q_latent_loss: 4.2006 - p_out_loss: 44.5997 Epoch 19/100 17/17 - 0s - loss: 25.7876 - q_latent_loss: 4.1215 - p_out_loss: 24.5675 Epoch 20/100 17/17 - 0s - loss: 268.8558 - q_latent_loss: 4.0396 - p_out_loss: 267.6599 Epoch 21/100 17/17 - 0s - loss: 45.2869 - q_latent_loss: 3.9513 - p_out_loss: 44.1171 Epoch 22/100 17/17 - 0s - loss: 30.1766 - q_latent_loss: 3.8787 - p_out_loss: 29.0284 Epoch 23/100 17/17 - 0s - loss: 32.2969 - q_latent_loss: 3.8108 - p_out_loss: 31.1688 Epoch 24/100 17/17 - 0s - loss: 64.0437 - q_latent_loss: 3.7457 - p_out_loss: 62.9348 Epoch 25/100 17/17 - 0s - loss: 39.9464 - q_latent_loss: 3.6825 - p_out_loss: 38.8562 Epoch 26/100 17/17 - 0s - loss: 33.5094 - q_latent_loss: 3.6220 - p_out_loss: 32.4372 Epoch 27/100 17/17 - 0s - loss: 31.4306 - q_latent_loss: 3.5643 - p_out_loss: 30.3755 Epoch 28/100 17/17 - 0s - loss: 27.8061 - q_latent_loss: 3.5087 - p_out_loss: 26.7674 Epoch 29/100 17/17 - 0s - loss: 65.8272 - q_latent_loss: 3.4540 - p_out_loss: 64.8047 Epoch 30/100 17/17 - 0s - loss: 25.9475 - q_latent_loss: 3.4009 - p_out_loss: 24.9408 Epoch 31/100 17/17 - 0s - loss: 30.2780 - q_latent_loss: 3.3515 - p_out_loss: 29.2859 Epoch 32/100 17/17 - 0s - loss: 21.8850 - q_latent_loss: 3.3044 - p_out_loss: 20.9068 Epoch 33/100 17/17 - 0s - loss: 36.6851 - q_latent_loss: 3.2587 - p_out_loss: 35.7204 Epoch 34/100 17/17 - 0s - loss: 25.5569 - q_latent_loss: 3.2120 - p_out_loss: 24.6061 Epoch 35/100 17/17 - 0s - loss: 24.6902 - q_latent_loss: 3.1682 - p_out_loss: 23.7523 Epoch 36/100 17/17 - 0s - loss: 116.0450 - q_latent_loss: 3.1269 - p_out_loss: 115.1194 Epoch 37/100 17/17 - 0s - loss: 20.0418 - q_latent_loss: 3.0908 - p_out_loss: 19.1268 Epoch 38/100 17/17 - 0s - loss: 56.1398 - q_latent_loss: 3.0497 - p_out_loss: 55.2370 Epoch 39/100 17/17 - 0s - loss: 27.5171 - q_latent_loss: 3.0067 - p_out_loss: 26.6270 Epoch 40/100 17/17 - 0s - loss: 20.7006 - q_latent_loss: 2.9684 - p_out_loss: 19.8219 Epoch 41/100 17/17 - 0s - loss: 26.9046 - q_latent_loss: 2.9328 - p_out_loss: 26.0364 Epoch 42/100 17/17 - 0s - loss: 18.7693 - q_latent_loss: 2.8996 - p_out_loss: 17.9110 Epoch 43/100 17/17 - 0s - loss: 22.3650 - q_latent_loss: 2.8670 - p_out_loss: 21.5163 Epoch 44/100 17/17 - 0s - loss: 32.9155 - q_latent_loss: 2.8352 - p_out_loss: 32.0763 Epoch 45/100 17/17 - 0s - loss: 19.9130 - q_latent_loss: 2.8037 - p_out_loss: 19.0830 Epoch 46/100 17/17 - 0s - loss: 19.9001 - q_latent_loss: 2.7740 - p_out_loss: 19.0789 Epoch 47/100 17/17 - 0s - loss: 25.4838 - q_latent_loss: 2.7436 - p_out_loss: 24.6716 Epoch 48/100 17/17 - 0s - loss: 23.9622 - q_latent_loss: 2.7135 - p_out_loss: 23.1589 Epoch 49/100 17/17 - 0s - loss: 20.7703 - q_latent_loss: 2.6849 - p_out_loss: 19.9756 Epoch 50/100 17/17 - 0s - loss: 19.6302 - q_latent_loss: 2.6576 - p_out_loss: 18.8435 Epoch 51/100 17/17 - 0s - loss: 18.7125 - q_latent_loss: 2.6321 - p_out_loss: 17.9334 Epoch 52/100 17/17 - 0s - loss: 21.4065 - q_latent_loss: 2.6073 - p_out_loss: 20.6347 Epoch 53/100 17/17 - 0s - loss: 37.3685 - q_latent_loss: 2.5831 - p_out_loss: 36.6039 Epoch 54/100 17/17 - 0s - loss: 15.8975 - q_latent_loss: 2.5606 - p_out_loss: 15.1395 Epoch 55/100 17/17 - 0s - loss: 15.6574 - q_latent_loss: 2.5387 - p_out_loss: 14.9059 Epoch 56/100 17/17 - 0s - loss: 28.7901 - q_latent_loss: 2.5174 - p_out_loss: 28.0449 Epoch 57/100 17/17 - 0s - loss: 99.3240 - q_latent_loss: 2.4972 - p_out_loss: 98.5848 Epoch 58/100 17/17 - 0s - loss: 19.6783 - q_latent_loss: 2.4761 - p_out_loss: 18.9453 Epoch 59/100 17/17 - 0s - loss: 18.9958 - q_latent_loss: 2.4563 - p_out_loss: 18.2688 Epoch 60/100 17/17 - 0s - loss: 21.3663 - q_latent_loss: 2.4364 - p_out_loss: 20.6451 Epoch 61/100 17/17 - 0s - loss: 26.8008 - q_latent_loss: 2.4179 - p_out_loss: 26.0850 Epoch 62/100 17/17 - 0s - loss: 13.9355 - q_latent_loss: 2.3984 - p_out_loss: 13.2256 Epoch 63/100 17/17 - 0s - loss: 14.0786 - q_latent_loss: 2.3803 - p_out_loss: 13.3740 Epoch 64/100 17/17 - 0s - loss: 20.6991 - q_latent_loss: 2.3634 - p_out_loss: 19.9995 Epoch 65/100 17/17 - 0s - loss: 33.9438 - q_latent_loss: 2.3476 - p_out_loss: 33.2488 Epoch 66/100 17/17 - 0s - loss: 19.5023 - q_latent_loss: 2.3325 - p_out_loss: 18.8118 Epoch 67/100 17/17 - 0s - loss: 16.1214 - q_latent_loss: 2.3179 - p_out_loss: 15.4353 Epoch 68/100 17/17 - 0s - loss: 33.3983 - q_latent_loss: 2.3044 - p_out_loss: 32.7162 Epoch 69/100 17/17 - 0s - loss: 14.1833 - q_latent_loss: 2.2933 - p_out_loss: 13.5045 Epoch 70/100 17/17 - 0s - loss: 33.0913 - q_latent_loss: 2.2802 - p_out_loss: 32.4163 Epoch 71/100 17/17 - 0s - loss: 15.5565 - q_latent_loss: 2.2661 - p_out_loss: 14.8857 Epoch 72/100 17/17 - 0s - loss: 23.7552 - q_latent_loss: 2.2522 - p_out_loss: 23.0885 Epoch 73/100 17/17 - 0s - loss: 15.8186 - q_latent_loss: 2.2402 - p_out_loss: 15.1554 Epoch 74/100 17/17 - 0s - loss: 15.8109 - q_latent_loss: 2.2277 - p_out_loss: 15.1514 Epoch 75/100 17/17 - 0s - loss: 23.2216 - q_latent_loss: 2.2153 - p_out_loss: 22.5659 Epoch 76/100 17/17 - 0s - loss: 17.1244 - q_latent_loss: 2.2021 - p_out_loss: 16.4725 Epoch 77/100 17/17 - 0s - loss: 20.2818 - q_latent_loss: 2.1875 - p_out_loss: 19.6343 Epoch 78/100 17/17 - 0s - loss: 20.1146 - q_latent_loss: 2.1751 - p_out_loss: 19.4708 Epoch 79/100 17/17 - 0s - loss: 12.0626 - q_latent_loss: 2.1647 - p_out_loss: 11.4218 Epoch 80/100 17/17 - 0s - loss: 17.5959 - q_latent_loss: 2.1547 - p_out_loss: 16.9580 Epoch 81/100 17/17 - 0s - loss: 46.9798 - q_latent_loss: 2.1428 - p_out_loss: 46.3454 Epoch 82/100 17/17 - 0s - loss: 20.0080 - q_latent_loss: 2.1244 - p_out_loss: 19.3792 Epoch 83/100 17/17 - 0s - loss: 11.8902 - q_latent_loss: 2.1120 - p_out_loss: 11.2651 Epoch 84/100 17/17 - 0s - loss: 17.5359 - q_latent_loss: 2.1015 - p_out_loss: 16.9139 Epoch 85/100 17/17 - 0s - loss: 14.8084 - q_latent_loss: 2.0917 - p_out_loss: 14.1892 Epoch 86/100 17/17 - 0s - loss: 9.6016 - q_latent_loss: 2.0823 - p_out_loss: 8.9852 Epoch 87/100 17/17 - 0s - loss: 13.4432 - q_latent_loss: 2.0736 - p_out_loss: 12.8294 Epoch 88/100 17/17 - 0s - loss: 16.5978 - q_latent_loss: 2.0652 - p_out_loss: 15.9865 Epoch 89/100 17/17 - 0s - loss: 21.3484 - q_latent_loss: 2.0557 - p_out_loss: 20.7399 Epoch 90/100 17/17 - 0s - loss: 11.3400 - q_latent_loss: 2.0439 - p_out_loss: 10.7350 Epoch 91/100 17/17 - 0s - loss: 14.2551 - q_latent_loss: 2.0345 - p_out_loss: 13.6529 Epoch 92/100 17/17 - 0s - loss: 14.2384 - q_latent_loss: 2.0268 - p_out_loss: 13.6385 Epoch 93/100 17/17 - 0s - loss: 16.3489 - q_latent_loss: 2.0194 - p_out_loss: 15.7511 Epoch 94/100 17/17 - 0s - loss: 14.2265 - q_latent_loss: 2.0118 - p_out_loss: 13.6310 Epoch 95/100 17/17 - 0s - loss: 11.5992 - q_latent_loss: 2.0041 - p_out_loss: 11.0060 Epoch 96/100 17/17 - 0s - loss: 11.7333 - q_latent_loss: 1.9971 - p_out_loss: 11.1421 Epoch 97/100 17/17 - 0s - loss: 12.1329 - q_latent_loss: 1.9904 - p_out_loss: 11.5438 Epoch 98/100 17/17 - 0s - loss: 13.7211 - q_latent_loss: 1.9832 - p_out_loss: 13.1341 Epoch 99/100 17/17 - 0s - loss: 37.2112 - q_latent_loss: 1.9745 - p_out_loss: 36.6267 Epoch 100/100 17/17 - 0s - loss: 9.3972 - q_latent_loss: 1.9630 - p_out_loss: 8.8161 Model est lat: [[ 2.51292503 -1.26424221 -1.11180196 -0.02588509]] Model est out: [[-0.27425705 2.39304429 -2.49596335 0.29950965 1.37276651 -0.43098222 -0.45473986 0.07839915]] prior mean: [ 1.3514248 -1.2266612 -0.8751619 -0.03475915] true lat: [-1. 1. 5. -5.] true out: [ 0.12000006 2.4699998 -2.76 -2.5 1.53 -1.3 1.31 0.05999994] Latent Dynamic Factor # Return 3 outputs, the first 2 are null #ds_dyn = ds.map(lambda x, y: (x, (y[0], y[0], y[1]))) ds_dyn = ds . map ( lambda x , y : ( x , y [ 1 ])) KL_WEIGHT = 0.001 LATENT_SIZE_DYNAMIC = 1 # Integer dimensionality of each dynamic, time-variant latent variable `z_t`. class VariationalLSTMCell ( tfkl . LSTMCell ): def __init__ ( self , units , make_dist_fn = None , make_dist_model = None , ** kwargs ): super ( VariationalLSTMCell , self ) . __init__ ( units , ** kwargs ) self . make_dist_fn = make_dist_fn self . make_dist_model = make_dist_model # For some reason the below code doesn't work during build. # So I don't know how to use the outer VariationalRNN to set this cell's output_size if self . make_dist_fn is None : self . make_dist_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]) if self . make_dist_model is None : fake_cell_output = tfkl . Input (( self . units ,)) loc = tfkl . Dense ( self . output_size , name = \"VarLSTMCell_loc\" )( fake_cell_output ) scale = tfkl . Dense ( self . output_size , name = \"VarLSTMCell_scale\" )( fake_cell_output ) scale = tf . nn . softplus ( scale + scale_shift ) + 1e-5 dist_layer = tfpl . DistributionLambda ( make_distribution_fn = self . make_dist_fn , # TODO: convert_to_tensor_fn=lambda s: s.sample(N_SAMPLES) )([ loc , scale ]) self . make_dist_model = tf . keras . Model ( fake_cell_output , dist_layer ) def build ( self , input_shape ): super ( VariationalLSTMCell , self ) . build ( input_shape ) # It would be good to defer making self.make_dist_model until here, # but it doesn't work for some reason. #def input_zero(self, inputs_): # input0 = inputs_[..., -1, :] # input0 = tf.matmul(input0, tf.zeros((input0.shape[-1], self.units))) # dist0 = self.make_dist_model(input0) # return dist0 def call ( self , inputs , states , training = None ): inputs = tf . convert_to_tensor ( inputs ) output , state = super ( VariationalLSTMCell , self ) . call ( inputs , states , training = training ) dist = self . make_dist_model ( output ) return dist , state K . clear_session () tmp = LearnableMultivariateNormalDiagCell ( 3 , 4 ) #tmp.build((None, 10, 5)) #tmp.summary() class DynamicEncoder ( tf . keras . Model ): def __init__ ( self , units , n_times , output_dim , name = \"dynamic_encoder\" ): super ( DynamicEncoder , self ) . __init__ ( name = name ) self . dynamic_prior_cell = LearnableMultivariateNormalDiagCell ( units , output_dim ) self . n_times = n_times self . loc = tfkl . Dense ( output_dim , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( output_dim , name = \"scale\" ) self . q_z_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]), name = \"q_z\" ) def call ( self , inputs ): # Assume inputs doesn't have time-axis. Broadcast-add zeros to add time axis. inputs_ = inputs [ ... , tf . newaxis , :] + tf . zeros ([ self . n_times , 1 ]) loc = self . loc ( inputs_ ) unxf_scale = self . unxf_scale ( inputs_ ) scale = tf . math . softplus ( unxf_scale + scale_shift ) + 1e-5 q_z = self . q_z_layer ([ loc , scale ]) # _, dynamic_prior = self.sample_dynamic_prior(self.n_times) #kld = tfd.kl_divergence(q_z, dynamic_prior) #kld = tf.reduce_sum(kld, axis=-1) #kld = tf.reduce_mean(kld) #self.add_loss(KL_WEIGHT * kld) return q_z def sample_dynamic_prior ( self , steps , samples = 1 , batches = 1 , fixed = False ): \"\"\"Samples LSTM cell->MVNDiag for each steps Args: steps: Number of timesteps to sample for each sequence. samples: Number of samples to draw from the latent distribution. batch_size: Number of sequences to sample. fixed: Boolean for whether or not to share the same random sample across all sequences. Returns: A tuple of a sample tensor of shape [samples, batch_size, steps, latent_size], and a MultivariateNormalDiag distribution from which the tensor was sampled, with event shape [latent_size], and batch shape [samples, 1, length] if fixed or [samples, batch_size, length] otherwise. \"\"\" if fixed : sample_batch_size = 1 else : sample_batch_size = batches sample , state = self . dynamic_prior_cell . zero_state ([ samples , sample_batch_size ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( steps ): dist , state = self . dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : # tile along the batch axis sample = sample + tf . zeros ([ batches , 1 , 1 ]) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) # test DynamicEncoder and LearnableMultivariateNormalDiagCell K . clear_session () dynamic_encoder = DynamicEncoder ( N_HIDDEN , N_TIMES , LATENT_SIZE_DYNAMIC ) sample , dynamic_prior = dynamic_encoder . sample_dynamic_prior ( N_TIMES , samples = N_SAMPLES , batches = 1 ) print ( sample . shape ) print ( \"mean:\" , np . squeeze ( dynamic_prior . mean ())) print ( \"stddev:\" , np . squeeze ( dynamic_prior . stddev ())) print ([ _ . name for _ in dynamic_encoder . trainable_variables ]) (2, 1, 10, 1) mean: [[ 0. 0.42388976 0.45631832 0.365768 0.20130846 0.37873474 0.31262326 0.26073667 0.15399611 0.14049806] [ 0. 0.08804662 0.0361465 -0.03267653 -0.08733355 0.19941618 0.30335566 0.3730844 0.2744042 0.17948757]] stddev: [[1.00001 0.929902 0.9554563 0.99371654 1.0142238 0.97018814 1.0006421 1.0033575 1.0105829 1.0065393 ] [1.00001 0.9952911 1.0008274 1.0000954 0.99874425 0.98230976 0.9771384 0.9683764 1.000174 1.0049998 ]] ['learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/recurrent_kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/bias:0'] class StaticEncoder ( tf . keras . Model ): def __init__ ( self , latent_size , name = \"static_encoder\" ): super ( StaticEncoder , self ) . __init__ ( name = name ) self . static_prior_factory = LearnableMultivariateNormalDiag ( latent_size ) self . loc = tfkl . Dense ( latent_size , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( tfpl . MultivariateNormalTriL . params_size ( latent_size ) - latent_size , name = \"scale\" ) self . scale_bijector = tfp . bijectors . FillScaleTriL () self . q_f_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalTriL ( loc = t [ 0 ], scale_tril = t [ 1 ]), name = \"q_f\" ) def call ( self , inputs ): loc = self . loc ( inputs ) unxf_scale = self . unxf_scale ( inputs ) scale = self . scale_bijector ( unxf_scale ) q_f = self . q_f_layer ([ loc , scale ]) #static_prior = self.static_prior_factory() #kld = tfd.kl_divergence(q_f, static_prior) #kld = tf.reduce_mean(kld) #self.add_loss(KL_WEIGHT * kld) return q_f class Decoder ( tf . keras . Model ): def __init__ ( self , n_times , out_dim , name = 'decoder' ): super ( FactorizedDecoder , self ) . __init__ ( name = name ) self . n_times = n_times self . concat = tfkl . Concatenate () self . loc = tfkl . Dense ( out_dim , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( out_dim , name = \"scale\" ) self . q_z_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]), name = \"p_out\" ) def call ( self , inputs ): f_sample = inputs [ 0 ][ ... , tf . newaxis , :] + tf . zeros ([ self . n_times , 1 ]) z_sample = tf . convert_to_tensor ( inputs [ 1 ]) y = self . concat ([ z_sample , f_sample ]) loc = self . loc ( y ) unxf_scale = self . unxf_scale ( y ) scale = tf . math . softplus ( unxf_scale + scale_shift ) + 1e-5 p_out = self . q_z_layer ([ loc , scale ]) return p_out class FactorizedAutoEncoder ( tf . keras . Model ): def __init__ ( self , units , n_times , latent_size_static , latent_size_dynamic , n_out_dim , name = 'autoencoder' ): super ( FactorizedAutoEncoder , self ) . __init__ ( name = name ) self . static_encoder = StaticEncoder ( latent_size_static ) self . dynamic_encoder = DynamicEncoder ( units , n_times , latent_size_dynamic ) self . decoder = Decoder ( n_times , n_out_dim ) def call ( self , inputs ): q_f = self . static_encoder ( inputs ) q_z = self . dynamic_encoder ( inputs ) p_out = self . decoder ([ tf . convert_to_tensor ( q_f ), tf . convert_to_tensor ( q_z )]) return p_out K . clear_session () f_model = FactorizedAutoEncoder ( N_HIDDEN , N_TIMES , LATENT_SIZE , LATENT_SIZE_DYNAMIC , N_SENSORS ) # Most of the trainable variables don't present themselves until the model pieces are called. print ([ _ . name for _ in f_model . static_encoder . trainable_variables ]) print ([ _ . name for _ in f_model . dynamic_encoder . trainable_variables ]) print ([ _ . name for _ in f_model . decoder . trainable_variables ]) [] ['learnable_multivariate_normal_diag/mean:0', 'learnable_multivariate_normal_diag/untransformed_stddev:0'] [] [] N_EPOCHS = 200 if False : f_model . compile ( optimizer = 'adam' , loss = lambda y_true , model_out : - model_out . log_prob ( y_true )) hist = f_model . fit ( ds_dyn , epochs = N_EPOCHS , verbose = 2 ) else : @tf . function def grad ( model , inputs , preds ): with tf . GradientTape () as tape : q_f = model . static_encoder ( inputs ) q_z = model . dynamic_encoder ( inputs ) p_full = model . decoder ([ tf . convert_to_tensor ( q_f ), tf . convert_to_tensor ( q_z )]) # Reconstruction log-likelihood: p(output|input) recon_post_log_prob = p_full . log_prob ( preds ) recon_post_log_prob = tf . reduce_sum ( recon_post_log_prob , axis =- 1 ) # Sum over time axis recon_post_log_prob = tf . reduce_mean ( recon_post_log_prob ) # KL Divergence - analytical # Static static_prior = model . static_encoder . static_prior_factory () stat_kl = tfd . kl_divergence ( q_f , static_prior ) stat_kl = KL_WEIGHT * stat_kl stat_kl = tf . reduce_mean ( stat_kl ) # Dynamic _ , dynamic_prior = model . dynamic_encoder . sample_dynamic_prior ( N_TIMES , samples = 1 , batches = 1 ) dyn_kl = tfd . kl_divergence ( q_z , dynamic_prior ) dyn_kl = tf . reduce_sum ( dyn_kl , axis =- 1 ) dyn_kl = tf . squeeze ( dyn_kl ) dyn_kl = KL_WEIGHT * dyn_kl dyn_kl = tf . reduce_mean ( dyn_kl ) loss = - recon_post_log_prob + stat_kl + dyn_kl grads = tape . gradient ( loss , model . trainable_variables ) return loss , grads , ( - recon_post_log_prob , stat_kl , dyn_kl ) optim = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ) for epoch_ix in range ( N_EPOCHS ): for step_ix , batch in enumerate ( ds_dyn ): inputs , preds = batch loss , grads , loss_comps = grad ( f_model , inputs , preds ) optim . apply_gradients ( zip ( grads , f_model . trainable_variables )) if ( step_ix % 200 ) == 0 : print ( '.' ) print ( f \"Epoch { epoch_ix } / { N_EPOCHS } : \\t loss= { loss : .3f } ; \" f \"Losses: { [ _ . numpy () for _ in loss_comps ] } \" ) . Epoch 0/200: loss=3777.438; Losses: [3777.3972, 0.0028260916, 0.03819199] . Epoch 1/200: loss=908.620; Losses: [908.5834, 0.002681077, 0.034407526] . Epoch 2/200: loss=682.733; Losses: [682.7001, 0.0025911012, 0.030505255] . Epoch 3/200: loss=317.985; Losses: [317.9555, 0.0024803109, 0.027142774] . Epoch 4/200: loss=737.753; Losses: [737.7305, 0.0024128703, 0.019867169] . Epoch 5/200: loss=293.983; Losses: [293.9585, 0.0023704215, 0.022514882] . Epoch 6/200: loss=412.075; Losses: [412.0592, 0.002335041, 0.013460781] . Epoch 7/200: loss=306.618; Losses: [306.60327, 0.002284743, 0.012861049] . Epoch 8/200: loss=215.916; Losses: [215.9029, 0.0022479186, 0.0111077] . Epoch 9/200: loss=223.136; Losses: [223.12366, 0.0022122823, 0.010420884] . Epoch 10/200: loss=300.156; Losses: [300.14383, 0.0021807426, 0.009667382] . Epoch 11/200: loss=179.766; Losses: [179.75607, 0.0021481065, 0.007676568] . Epoch 12/200: loss=215.981; Losses: [215.97124, 0.0021149626, 0.008112778] . Epoch 13/200: loss=208.054; Losses: [208.04565, 0.0020882282, 0.006530414] . Epoch 14/200: loss=207.146; Losses: [207.13806, 0.0020594192, 0.0063194335] . Epoch 15/200: loss=261.805; Losses: [261.7982, 0.0020306753, 0.0045144106] . Epoch 16/200: loss=178.393; Losses: [178.38637, 0.0020005947, 0.0045015276] . Epoch 17/200: loss=284.872; Losses: [284.86642, 0.0019730518, 0.003997615] . Epoch 18/200: loss=212.054; Losses: [212.04866, 0.0019469936, 0.0034680453] . Epoch 19/200: loss=145.862; Losses: [145.85641, 0.001922644, 0.004011639] . Epoch 20/200: loss=182.153; Losses: [182.14563, 0.0018981744, 0.0059485184] . Epoch 21/200: loss=154.575; Losses: [154.56941, 0.0018728755, 0.0038126395] . Epoch 22/200: loss=214.752; Losses: [214.74734, 0.0018469194, 0.003135754] . Epoch 23/200: loss=179.347; Losses: [179.34248, 0.001825613, 0.0029100403] . Epoch 24/200: loss=354.274; Losses: [354.26898, 0.0018005224, 0.0028155171] . Epoch 25/200: loss=181.006; Losses: [181.0021, 0.0017763268, 0.0022955306] . Epoch 26/200: loss=142.006; Losses: [142.00166, 0.0017520584, 0.0022001117] . Epoch 27/200: loss=158.932; Losses: [158.92723, 0.0017273662, 0.0026816982] . Epoch 28/200: loss=175.159; Losses: [175.15494, 0.0017028436, 0.0019087334] . Epoch 29/200: loss=167.915; Losses: [167.91084, 0.0016797789, 0.002798946] . Epoch 30/200: loss=152.785; Losses: [152.78006, 0.001658167, 0.0029456303] . Epoch 31/200: loss=158.407; Losses: [158.40344, 0.0016350556, 0.0018697139] . Epoch 32/200: loss=151.065; Losses: [151.06094, 0.0016126116, 0.00282249] . Epoch 33/200: loss=181.075; Losses: [181.07072, 0.0015892526, 0.0025259533] . Epoch 34/200: loss=157.210; Losses: [157.20605, 0.0015682159, 0.0026225548] . Epoch 35/200: loss=151.200; Losses: [151.19623, 0.0015464819, 0.0017979483] . Epoch 36/200: loss=157.111; Losses: [157.10796, 0.0015237778, 0.0016808716] . Epoch 37/200: loss=160.398; Losses: [160.39449, 0.0015015532, 0.0018031715] . Epoch 38/200: loss=133.418; Losses: [133.41472, 0.0014805306, 0.0017712188] . Epoch 39/200: loss=161.662; Losses: [161.65845, 0.0014592485, 0.0018676165] . Epoch 40/200: loss=181.789; Losses: [181.78532, 0.0014379938, 0.0023018767] . Epoch 41/200: loss=183.568; Losses: [183.56451, 0.00142015, 0.001671126] . Epoch 42/200: loss=211.679; Losses: [211.67618, 0.0014024411, 0.001817507] . Epoch 43/200: loss=235.384; Losses: [235.38095, 0.0013794828, 0.0018946148] . Epoch 44/200: loss=139.088; Losses: [139.08447, 0.0013571468, 0.0019089653] . Epoch 45/200: loss=160.254; Losses: [160.25092, 0.0013359843, 0.0013933791] . Epoch 46/200: loss=142.206; Losses: [142.20291, 0.0013167453, 0.0014015753] . Epoch 47/200: loss=138.814; Losses: [138.8115, 0.0012925405, 0.0014298331] . Epoch 48/200: loss=130.677; Losses: [130.67343, 0.0012679367, 0.0021604125] . Epoch 49/200: loss=145.296; Losses: [145.29306, 0.0012432866, 0.0013332999] . Epoch 50/200: loss=133.338; Losses: [133.33516, 0.0012180805, 0.0012685797] . Epoch 51/200: loss=138.212; Losses: [138.20908, 0.0011976506, 0.0017989981] . Epoch 52/200: loss=136.139; Losses: [136.13644, 0.0011768966, 0.001470595] . Epoch 53/200: loss=141.839; Losses: [141.83646, 0.0011539405, 0.0017275128] . Epoch 54/200: loss=159.297; Losses: [159.29402, 0.0011311076, 0.0017414299] . Epoch 55/200: loss=125.919; Losses: [125.91669, 0.0011091225, 0.0013509693] . Epoch 56/200: loss=135.710; Losses: [135.7079, 0.0010871431, 0.0010571101] . Epoch 57/200: loss=124.548; Losses: [124.545654, 0.0010660599, 0.0011921946] . Epoch 58/200: loss=128.607; Losses: [128.60472, 0.0010461143, 0.0010578154] . Epoch 59/200: loss=200.879; Losses: [200.87674, 0.0010245068, 0.0014237395] . Epoch 60/200: loss=181.939; Losses: [181.93698, 0.0010024481, 0.0010916217] . Epoch 61/200: loss=161.069; Losses: [161.06737, 0.0009815091, 0.0009930782] . Epoch 62/200: loss=129.661; Losses: [129.65881, 0.0009596758, 0.001090972] . Epoch 63/200: loss=342.733; Losses: [342.73068, 0.000939325, 0.001181667] . Epoch 64/200: loss=160.802; Losses: [160.8003, 0.00091621274, 0.0012079075] . Epoch 65/200: loss=123.200; Losses: [123.19836, 0.00089694076, 0.0009983401] . Epoch 66/200: loss=134.465; Losses: [134.46295, 0.00087896077, 0.0009002821] . Epoch 67/200: loss=205.839; Losses: [205.83714, 0.0008604548, 0.001061962] . Epoch 68/200: loss=144.191; Losses: [144.18906, 0.0008421927, 0.0011492949] . Epoch 69/200: loss=164.397; Losses: [164.39539, 0.0008238552, 0.0010998722] . Epoch 70/200: loss=131.024; Losses: [131.02272, 0.00080561615, 0.000867295] . Epoch 71/200: loss=130.408; Losses: [130.40652, 0.0007878286, 0.0009589862] . Epoch 72/200: loss=120.511; Losses: [120.509705, 0.0007217523, 0.0008722847] . Epoch 73/200: loss=122.388; Losses: [122.38655, 0.00069110864, 0.00080618635] . Epoch 74/200: loss=123.200; Losses: [123.198654, 0.0006731994, 0.0008244566] . Epoch 75/200: loss=117.884; Losses: [117.88217, 0.00065816526, 0.00086460914] . Epoch 76/200: loss=123.508; Losses: [123.50694, 0.0006448629, 0.0008477152] . Epoch 77/200: loss=121.749; Losses: [121.74744, 0.0006321136, 0.0008150753] . Epoch 78/200: loss=145.549; Losses: [145.5473, 0.00061951997, 0.00082959904] . Epoch 79/200: loss=135.341; Losses: [135.33992, 0.00060778105, 0.0007312283] . Epoch 80/200: loss=131.476; Losses: [131.47452, 0.0005965105, 0.0008391714] . Epoch 81/200: loss=123.978; Losses: [123.976944, 0.00058624754, 0.0008059401] . Epoch 82/200: loss=136.084; Losses: [136.08298, 0.0005766748, 0.0007252015] . Epoch 83/200: loss=137.815; Losses: [137.81375, 0.00056776253, 0.0009108439] . Epoch 84/200: loss=116.955; Losses: [116.95401, 0.0005592232, 0.0008040637] . Epoch 85/200: loss=131.525; Losses: [131.52376, 0.00055153086, 0.0007604256] . Epoch 86/200: loss=135.716; Losses: [135.71432, 0.00054452394, 0.0006871256] . Epoch 87/200: loss=191.940; Losses: [191.93927, 0.0005378095, 0.0006448448] . Epoch 88/200: loss=170.746; Losses: [170.74509, 0.00053181086, 0.0006591724] . Epoch 89/200: loss=121.373; Losses: [121.37161, 0.00052640436, 0.00079500565] . Epoch 90/200: loss=126.909; Losses: [126.90761, 0.0005213883, 0.0006021685] . Epoch 91/200: loss=122.121; Losses: [122.120255, 0.0005157513, 0.0006873938] . Epoch 92/200: loss=129.156; Losses: [129.15442, 0.0005111307, 0.00064897194] . Epoch 93/200: loss=113.183; Losses: [113.18219, 0.0005073488, 0.000602577] . Epoch 94/200: loss=146.389; Losses: [146.38794, 0.00050397916, 0.0005585851] . Epoch 95/200: loss=130.446; Losses: [130.44531, 0.0005009744, 0.00056175387] . Epoch 96/200: loss=118.884; Losses: [118.88327, 0.0004950377, 0.0005437781] . Epoch 97/200: loss=114.319; Losses: [114.3183, 0.0004938163, 0.0006134198] . Epoch 98/200: loss=134.747; Losses: [134.74594, 0.0004912615, 0.0005468172] . Epoch 99/200: loss=127.224; Losses: [127.22336, 0.0004886875, 0.00050384714] . Epoch 100/200: loss=123.007; Losses: [123.00618, 0.0004862357, 0.00056520273] . Epoch 101/200: loss=130.374; Losses: [130.3726, 0.0004841056, 0.0005738659] . Epoch 102/200: loss=120.120; Losses: [120.11898, 0.00048118114, 0.00054365897] . Epoch 103/200: loss=107.167; Losses: [107.16606, 0.00047940924, 0.0004886348] . Epoch 104/200: loss=112.270; Losses: [112.268585, 0.00047771176, 0.0004745159] . Epoch 105/200: loss=125.537; Losses: [125.53565, 0.0004758754, 0.00047247266] . Epoch 106/200: loss=109.324; Losses: [109.32308, 0.00047426036, 0.00046340926] . Epoch 107/200: loss=113.328; Losses: [113.327286, 0.0004729222, 0.00046490182] . Epoch 108/200: loss=117.106; Losses: [117.10452, 0.000471713, 0.00062898267] . Epoch 109/200: loss=122.371; Losses: [122.37039, 0.0004705812, 0.00051386596] . Epoch 110/200: loss=119.422; Losses: [119.42122, 0.0004696009, 0.0005573735] . Epoch 111/200: loss=131.784; Losses: [131.78348, 0.000468354, 0.00041559048] . Epoch 112/200: loss=124.476; Losses: [124.475006, 0.00046699354, 0.00041292777] . Epoch 113/200: loss=104.487; Losses: [104.486534, 0.00046530017, 0.00042556314] . Epoch 114/200: loss=119.418; Losses: [119.41684, 0.0004641684, 0.00039730535] . Epoch 115/200: loss=117.776; Losses: [117.77547, 0.00046339296, 0.00056288636] . Epoch 116/200: loss=112.189; Losses: [112.18817, 0.0004628609, 0.00045002214] . Epoch 117/200: loss=116.317; Losses: [116.31613, 0.0004616853, 0.00036934114] . Epoch 118/200: loss=159.105; Losses: [159.10422, 0.00046110825, 0.0003716088] . Epoch 119/200: loss=116.958; Losses: [116.95712, 0.00045984008, 0.0003543413] . Epoch 120/200: loss=108.100; Losses: [108.09944, 0.00045923653, 0.00045118056] . Epoch 121/200: loss=107.565; Losses: [107.56447, 0.00045848632, 0.00033903003] . Epoch 122/200: loss=117.631; Losses: [117.62992, 0.0004574218, 0.00033954927] . Epoch 123/200: loss=116.075; Losses: [116.07385, 0.0004564249, 0.00036835673] . Epoch 124/200: loss=106.798; Losses: [106.79701, 0.0004566427, 0.00032678706] . Epoch 125/200: loss=113.363; Losses: [113.36252, 0.0004562596, 0.00042438688] . Epoch 126/200: loss=118.104; Losses: [118.10279, 0.00045581322, 0.00032525152] . Epoch 127/200: loss=113.516; Losses: [113.51486, 0.00045547614, 0.0005056638] . Epoch 128/200: loss=117.624; Losses: [117.62353, 0.00045549794, 0.00034517745] . Epoch 129/200: loss=112.273; Losses: [112.272316, 0.00045538746, 0.00029629772] . Epoch 130/200: loss=113.328; Losses: [113.32768, 0.0004549961, 0.0003071945] . Epoch 131/200: loss=111.756; Losses: [111.75513, 0.00045427072, 0.00032724047] . Epoch 132/200: loss=107.796; Losses: [107.795494, 0.00045377907, 0.00027464304] . Epoch 133/200: loss=150.595; Losses: [150.59428, 0.00045307074, 0.0003010978] . Epoch 134/200: loss=120.134; Losses: [120.13356, 0.00045292114, 0.00029552256] . Epoch 135/200: loss=120.130; Losses: [120.12947, 0.00045320712, 0.0002585037] . Epoch 136/200: loss=117.070; Losses: [117.06926, 0.0004533619, 0.00026611943] . Epoch 137/200: loss=111.006; Losses: [111.00518, 0.00045333267, 0.0002824646] . Epoch 138/200: loss=115.901; Losses: [115.90064, 0.00045347284, 0.00030576388] . Epoch 139/200: loss=111.147; Losses: [111.146286, 0.000453111, 0.0002558468] . Epoch 140/200: loss=103.128; Losses: [103.12687, 0.0004522237, 0.00038727812] . Epoch 141/200: loss=115.025; Losses: [115.024475, 0.00045187408, 0.0002339398] . Epoch 142/200: loss=117.170; Losses: [117.16969, 0.0004520767, 0.00023050333] . Epoch 143/200: loss=105.315; Losses: [105.31448, 0.0004517807, 0.00026579303] . Epoch 144/200: loss=114.114; Losses: [114.113625, 0.00045176974, 0.0002442702] . Epoch 145/200: loss=108.179; Losses: [108.178154, 0.00045183185, 0.00022483827] . Epoch 146/200: loss=119.408; Losses: [119.407074, 0.0004509559, 0.00021656642] . Epoch 147/200: loss=116.504; Losses: [116.50336, 0.000450821, 0.00021879328] . Epoch 148/200: loss=108.465; Losses: [108.464005, 0.0004506137, 0.00031175395] . Epoch 149/200: loss=99.284; Losses: [99.28293, 0.00045024417, 0.00022024836] . Epoch 150/200: loss=105.143; Losses: [105.14198, 0.00044986696, 0.00024956765] . Epoch 151/200: loss=107.015; Losses: [107.01389, 0.0004503439, 0.00019209863] . Epoch 152/200: loss=109.961; Losses: [109.96058, 0.00045052002, 0.00036505712] . Epoch 153/200: loss=110.943; Losses: [110.94235, 0.00045058262, 0.00019362733] . Epoch 154/200: loss=105.146; Losses: [105.14586, 0.0004505078, 0.00018350873] . Epoch 155/200: loss=153.239; Losses: [153.23862, 0.00045094165, 0.00018688777] . Epoch 156/200: loss=97.193; Losses: [97.192276, 0.0004497521, 0.0002643012] . Epoch 157/200: loss=116.076; Losses: [116.075356, 0.00044927179, 0.00018770616] . Epoch 158/200: loss=99.644; Losses: [99.64349, 0.00044897772, 0.00019957994] . Epoch 159/200: loss=101.686; Losses: [101.68573, 0.00044913022, 0.0001649716] . Epoch 160/200: loss=114.998; Losses: [114.99737, 0.00044872603, 0.0001598363] . Epoch 161/200: loss=126.449; Losses: [126.44795, 0.00044798924, 0.00017636445] . Epoch 162/200: loss=99.323; Losses: [99.32204, 0.00044971833, 0.0001718228] . Epoch 163/200: loss=118.403; Losses: [118.402115, 0.0004499098, 0.00016231032] . Epoch 164/200: loss=101.217; Losses: [101.21654, 0.00044922353, 0.00015090306] . Epoch 165/200: loss=132.002; Losses: [132.0016, 0.0004491811, 0.00018679435] . Epoch 166/200: loss=103.262; Losses: [103.26103, 0.00044870118, 0.00014671378] . Epoch 167/200: loss=98.593; Losses: [98.592026, 0.0004482735, 0.00017167021] . Epoch 168/200: loss=102.641; Losses: [102.64062, 0.00044823167, 0.0001966377] . Epoch 169/200: loss=110.199; Losses: [110.19867, 0.00044768318, 0.00014072815] . Epoch 170/200: loss=98.456; Losses: [98.45533, 0.00044640992, 0.00013351238] . Epoch 171/200: loss=107.700; Losses: [107.698944, 0.00044608887, 0.000128763] . Epoch 172/200: loss=110.314; Losses: [110.31317, 0.0004454155, 0.00015472547] . Epoch 173/200: loss=101.824; Losses: [101.82384, 0.00044520054, 0.00012376827] . Epoch 174/200: loss=100.615; Losses: [100.61448, 0.00044518447, 0.00012106997] . Epoch 175/200: loss=99.010; Losses: [99.00989, 0.00044499052, 0.0001265088] . Epoch 176/200: loss=104.999; Losses: [104.99841, 0.0004448745, 0.00017745573] . Epoch 177/200: loss=98.012; Losses: [98.0118, 0.0004448767, 0.00016406355] . Epoch 178/200: loss=99.610; Losses: [99.60982, 0.0004446858, 0.00011498472] . Epoch 179/200: loss=108.899; Losses: [108.89821, 0.00044491427, 0.00010847509] . Epoch 180/200: loss=118.136; Losses: [118.13521, 0.00044519745, 0.00010489453] . Epoch 181/200: loss=98.810; Losses: [98.80894, 0.000445671, 0.00012609128] . Epoch 182/200: loss=96.406; Losses: [96.40544, 0.0004462903, 0.00010188887] . Epoch 183/200: loss=98.501; Losses: [98.50068, 0.00044781342, 9.952572e-05] . Epoch 184/200: loss=97.149; Losses: [97.14829, 0.00044934792, 0.00010874969] . Epoch 185/200: loss=100.637; Losses: [100.63678, 0.00044996737, 9.560937e-05] . Epoch 186/200: loss=97.202; Losses: [97.201294, 0.00045056257, 9.1939364e-05] . Epoch 187/200: loss=99.839; Losses: [99.83876, 0.0004511009, 8.99044e-05] . Epoch 188/200: loss=118.457; Losses: [118.45691, 0.00045174357, 9.007633e-05] . Epoch 189/200: loss=102.419; Losses: [102.41877, 0.00045339097, 9.094182e-05] . Epoch 190/200: loss=110.813; Losses: [110.812386, 0.00045527803, 8.5699685e-05] . Epoch 191/200: loss=99.384; Losses: [99.38332, 0.000456504, 8.375079e-05] . Epoch 192/200: loss=103.581; Losses: [103.58075, 0.0004564843, 9.893996e-05] . Epoch 193/200: loss=97.621; Losses: [97.62078, 0.00045904273, 8.179152e-05] . Epoch 194/200: loss=94.909; Losses: [94.90842, 0.00046064917, 7.898855e-05] . Epoch 195/200: loss=100.840; Losses: [100.83898, 0.0004623049, 8.4420986e-05] . Epoch 196/200: loss=98.157; Losses: [98.15616, 0.00046370056, 8.1935854e-05] . Epoch 197/200: loss=95.283; Losses: [95.28199, 0.00046543585, 7.4171934e-05] . Epoch 198/200: loss=98.005; Losses: [98.00419, 0.0004669357, 7.385877e-05] . Epoch 199/200: loss=103.609; Losses: [103.608406, 0.00046879202, 7.1431816e-05] _ , dyn_prior = f_model . dynamic_encoder . sample_dynamic_prior ( 10 ) np . squeeze ( dyn_prior . mean () . numpy ()) array([-1.6796231, -2.5568666, -2.4644861, -2.4013069, -2.3772488, -2.3707619, -2.377105 , -2.332031 , -2.3327737, -2.360692 ], dtype=float32) class RNNMultivariateNormalDiag ( tfd . MultivariateNormalDiag ): def __init__ ( self , cell , n_timesteps = 1 , output_dim = None , name = \"rnn_mvn_diag\" , ** kwargs ): self . cell = cell if output_dim is not None and hasattr ( self . cell , 'output_dim' ): self . cell . output_dim = output_dim if hasattr ( self . cell , 'output_dim' ): output_dim = self . cell . output_dim else : output_dim = output_dim or self . cell . units h0 = tf . zeros ([ 1 , self . cell . units ]) c0 = tf . zeros ([ 1 , self . cell . units ]) input0 = tf . zeros (( 1 , output_dim )) if hasattr ( cell , 'reset_dropout_mask' ): self . cell . reset_dropout_mask () self . cell . reset_recurrent_dropout_mask () input_ = input0 states_ = ( h0 , c0 ) successive_outputs = [] for i in range ( n_timesteps ): input_ , states_ = self . cell ( input_ , states_ ) successive_outputs . append ( input_ ) loc = tf . concat ([ _ . parameters [ \"distribution\" ] . parameters [ \"loc\" ] for _ in successive_outputs ], axis = 0 ) scale_diag = tf . concat ([ _ . parameters [ \"distribution\" ] . parameters [ \"scale_diag\" ] for _ in successive_outputs ], axis = 0 ) super ( RNNMultivariateNormalDiag , self ) . __init__ ( loc = loc , scale_diag = scale_diag , name = name , ** kwargs ) K . clear_session () dynamic_prior = RNNMultivariateNormalDiag ( VariationalLSTMCell ( N_HIDDEN , output_dim = LATENT_SIZE_DYNAMIC ), n_timesteps = N_TIMES , output_dim = LATENT_SIZE_DYNAMIC ) sample = dynamic_prior . sample (( N_SAMPLES , BATCH_SIZE )) print ( sample . shape ) print ( dynamic_prior . mean ()) (2, 6, 10, 1) tf.Tensor( [[ 0. ] [ 0.04328619] [ 0.08498121] [-0.17377347] [-0.09743058] [-0.30255282] [-0.22110605] [-0.36379734] [-0.30933833] [-0.13590682]], shape=(10, 1), dtype=float32)","title":"Tfp utils"},{"location":"bVAE/tfp_utils/#tensorflow-probability-utilities","text":"This notebook is a bit of a mess after the refactor. # https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/disentangled_vae.py K . clear_session () K . set_floatx ( 'float32' ) def test_make_mvn_prior ( latent_size = 4 , init_std = 0.1 , trainable_mean = True , trainable_var = True , prior_offdiag = False ): prior = make_mvn_prior ( latent_size , init_std = init_std , trainable_mean = trainable_mean , trainable_var = trainable_var , offdiag = prior_offdiag ) assert ( isinstance ( prior . loc , tf . Variable ) == trainable_mean ) if prior_offdiag : assert ( hasattr ( prior . scale_tril , 'trainable_variables' ) == trainable_var ) else : assert (( len ( prior . scale . trainable_variables ) > 0 ) == trainable_var ) print ( prior . mean (), prior . stddev ()) for trainable_mean in [ True , False ]: for trainable_var in [ True , False ]: for prior_offdiag in [ True , False ]: test_make_mvn_prior ( trainable_mean = trainable_mean , trainable_var = trainable_var , prior_offdiag = prior_offdiag ) tf.Tensor([-0.08141219 -0.0387085 -0.05836845 -0.00457149], shape=(4,), dtype=float32) tf.Tensor([0.13052301 0.23032661 0.31772745 0.2098567 ], shape=(4,), dtype=float32) tf.Tensor([-0.07178359 -0.07984201 0.07635907 -0.09406286], shape=(4,), dtype=float32) tf.Tensor([0.10905064 0.103502 0.09993087 0.10073646], shape=(4,), dtype=float32) tf.Tensor([-0.1242888 0.05545203 -0.18831097 -0.02664005], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0.18107729 0.14778917 0.1890041 0.20096827], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.10471911 0.23243366 0.32148966 0.18541414], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.11602824 0.11143655 0.09993648 0.09902043], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32) tf.Tensor([0.1 0.1 0.1 0.1], shape=(4,), dtype=float32) K . clear_session () test_make_mvn_dist_fn () q_dist: tfp.distributions.MultivariateNormalDiag(\"model_distribution_lambda_MultivariateNormalDiag\", batch_shape=[6], event_shape=[4], dtype=float32) q_dist stddev: tf.Tensor( [[0.12080275 0.16515851 0.13489766 0.06180337] [0.0738408 0.12705617 0.09915603 0.06687246] [0.10365839 0.16512334 0.09549066 0.15812276] [0.09845882 0.1333666 0.1458401 0.08260284] [0.10875075 0.14236958 0.21499178 0.09950284] [0.15298602 0.13276467 0.1905963 0.18955594]], shape=(6, 4), dtype=float32) q_dist sample: tf.Tensor( [[ 1.2572958 -1.079319 0.05739563 0.27565804] [ 0.9092588 -0.37485668 0.31609604 0.25275287] [ 0.19276637 -0.7746772 -0.3904564 0.34959695] [ 0.48656625 -0.78282547 -0.2158188 0.271042 ] [ 1.3095855 -0.9227902 0.38678312 0.59275925] [ 0.83026767 -0.9097708 0.10051005 0.40007213]], shape=(6, 4), dtype=float32) K . clear_session () test_make_variational () --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op (graph, node_def, inputs, control_inputs, op_def) 1653 try : -> 1654 c_op = pywrap_tf_session . TF_FinishOperation ( op_desc ) 1655 except errors . InvalidArgumentError as e : InvalidArgumentError : Duplicate node name in graph: 'concat/values_0' During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 408 dtype = dtypes . as_dtype ( dtype ) . base_dtype --> 409 values = ops.internal_convert_n_to_tensor( 410 values , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor (values, dtype, name, as_ref, preferred_dtype, ctx) 1402 ret.append( -> 1403 convert_to_tensor( 1404 value , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor (value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types) 1340 if ret is None : -> 1341 ret = conversion_func ( value , dtype = dtype , name = name , as_ref = as_ref ) 1342 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function (v, dtype, name, as_ref) 1454 v = nest . map_structure ( _cast_nested_seqs_to_dtype ( dtype ) , v ) -> 1455 return _autopacking_helper ( v , dtype , name or \"packed\" ) 1456 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper (list_or_tuple, dtype, name) 1390 constant_op.constant(elem, dtype=dtype, name=str(i))) -> 1391 return gen_array_ops . pack ( elems_as_tensors , name = scope ) 1392 else : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in pack (values, axis, name) 6346 axis = _execute . make_int ( axis , \"axis\" ) -> 6347 _, _, _op, _outputs = _op_def_library._apply_op_helper( 6348 \"Pack\", values=values, axis=axis, name=name) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 741 # pylint: disable=protected-access --> 742 op = g._create_op_internal(op_type_name, inputs, dtypes=None, 743 name = scope , input_types = input_types , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in _create_op_internal (self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 592 inputs [ i ] = inp --> 593 return super(FuncGraph, self)._create_op_internal( # pylint: disable=protected-access 594 op_type , inputs , dtypes , input_types , name , attrs , op_def , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_op_internal (self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 3318 with self . _mutation_lock ( ) : -> 3319 ret = Operation( 3320 node_def , ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __init__ (self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def) 1815 op_def = self . _graph . _get_op_def ( node_def . op ) -> 1816 self._c_op = _create_c_op(self._graph, node_def, inputs, 1817 control_input_ops, op_def) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op (graph, node_def, inputs, control_inputs, op_def) 1656 # Convert to ValueError for backwards compatibility. -> 1657 raise ValueError ( str ( e ) ) 1658 ValueError : Duplicate node name in graph: 'concat/values_0' During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) <ipython-input-60-f531da3a8495> in <module> 1 K . clear_session ( ) ----> 2 test_make_variational ( ) <ipython-input-59-7ba46b6f4e26> in test_make_variational (input_dim, dist_dim, batch_size) 3 inputs = tfkl . Input ( shape = ( input_dim , ) ) 4 q_dist = make_variational ( inputs , dist_dim , init_std = 0.1 ) ----> 5 print ( q_dist , q_dist . sample ( ) . shape ) 6 7 #model = tf.keras.Model(inputs=inputs, outputs=q_dist) ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in sample (self, sample_shape, seed, name, **kwargs) 935 samples : a ` Tensor ` with prepended dimensions ` sample_shape ` . 936 \"\"\" --> 937 return self . _call_sample_n ( sample_shape , seed , name , ** kwargs ) 938 939 def _call_log_prob ( self , value , name , ** kwargs ) : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _call_sample_n (self, sample_shape, seed, name, **kwargs) 464 # event that we need to reinterpret the samples as part of the 465 # event_shape. --> 466 x = self . _sample_n ( n , seed , ** distribution_kwargs ) 467 468 # Next, we reshape `x` into its final form. We do this prior to the call ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _sample_n (self, n, seed, **distribution_kwargs) 442 distribution_util . pick_vector ( needs_rotation , [ n ] , self . _empty ) , 443 ], axis=0) --> 444 x = self.distribution.sample(sample_shape=sample_shape, seed=seed, 445 **distribution_kwargs) 446 x = self._maybe_rotate_dims( ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in sample (self, sample_shape, seed, name, **kwargs) 935 samples : a ` Tensor ` with prepended dimensions ` sample_shape ` . 936 \"\"\" --> 937 return self . _call_sample_n ( sample_shape , seed , name , ** kwargs ) 938 939 def _call_log_prob ( self , value , name , ** kwargs ) : ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py in _call_sample_n (self, sample_shape, seed, name, **kwargs) 912 sample_shape, n = self._expand_sample_shape_to_vector( 913 sample_shape, 'sample_shape') --> 914 samples = self._sample_n( 915 n, seed=seed() if callable(seed) else seed, **kwargs) 916 batch_event_shape = tf . shape ( samples ) [ 1 : ] ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow_probability/python/distributions/normal.py in _sample_n (self, n, seed) 184 loc = tf . convert_to_tensor ( self . loc ) 185 scale = tf . convert_to_tensor ( self . scale ) --> 186 shape = tf.concat([[n], self._batch_shape_tensor(loc=loc, scale=scale)], 187 axis=0) 188 sampled = tf.random.normal( ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper (*args, **kwargs) 178 \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\" 179 try : --> 180 return target ( * args , ** kwargs ) 181 except ( TypeError , ValueError ) : 182 # Note: convert_to_eager_tensor currently raises a ValueError, not a ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in concat (values, axis, name) 1604 dtype=dtypes.int32).get_shape().assert_has_rank(0) 1605 return identity ( values [ 0 ] , name = name ) -> 1606 return gen_array_ops . concat_v2 ( values = values , axis = axis , name = name ) 1607 1608 ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2 (values, axis, name) 1186 \"'concat_v2' Op, not %r.\" % values) 1187 _attr_N = len ( values ) -> 1188 _, _, _op, _outputs = _op_def_library._apply_op_helper( 1189 \"ConcatV2\", values=values, axis=axis, name=name) 1190 _result = _outputs [ : ] ~/miniconda3/envs/indl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper (op_type_name, name, **keywords) 440 (prefix, dtype.name)) 441 else : --> 442 raise TypeError ( \"%s that don't all match.\" % prefix ) 443 else : 444 raise TypeError( TypeError : Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, int32] that don't all match. # An example of how this would work in a variational autoencoder. N_TIMES = 10 N_SENSORS = 8 N_SAMPLES = 2 N_HIDDEN = 5 KL_WEIGHT = 0.05 t_vec = tf . range ( N_TIMES , dtype = tf . float32 ) / N_TIMES sig_vec = 1 + tf . exp ( - 10 * ( t_vec - 0.5 )) def make_model ( prior ): input_ = tfkl . Input ( shape = ( LATENT_SIZE ,)) # Encoder make_latent_dist_fn , latent_params = make_mvn_dist_fn ( input_ , LATENT_SIZE , offdiag = True , loc_name = \"latent_loc\" ) q_latent = tfpl . DistributionLambda ( name = \"q_latent\" , make_distribution_fn = make_latent_dist_fn , convert_to_tensor_fn = lambda s : s . sample ( N_SAMPLES ), activity_regularizer = tfpl . KLDivergenceRegularizer ( prior , use_exact_kl = True , weight = KL_WEIGHT ) )( latent_params ) # Decoder y_ = q_latent [ ... , tf . newaxis , :] / sig_vec [:, tf . newaxis ] # broadcast-add zeros to restore timesteps #y_ = q_latent[..., tf.newaxis, :] + tf.zeros([N_TIMES, 1]) #y_ = tf.reshape(y_, [-1, N_TIMES, LATENT_SIZE]) #y_ = tfkl.LSTM(N_HIDDEN, return_sequences=True)(y_) #y_ = tf.reshape(y_, [N_SAMPLES, -1, N_TIMES, N_HIDDEN]) make_out_dist_fn , out_dist_params = make_mvn_dist_fn ( y_ , N_SENSORS , loc_name = \"out_loc\" ) p_out = tfpl . DistributionLambda ( make_distribution_fn = make_out_dist_fn , name = \"p_out\" )( out_dist_params ) # no prior on the output. # Model model = tf . keras . Model ( inputs = input_ , outputs = [ q_latent , p_out ]) return model # Create a fake dataset to train the model. LATENT_SIZE = 4 BATCH_SIZE = 6 # The latents are sampled from a distribution with known parameters. true_dist = tfd . MultivariateNormalDiag ( loc = [ - 1. , 1. , 5 , - 5 ], # must have length == LATENT_SIZE scale_diag = [ 0.5 , 0.5 , 0.9 , 0.2 ] ) # They parameterize sigmoid end points, from indl.misc.sigfuncs import sigmoid from functools import partial t_vec = ( np . arange ( N_TIMES , dtype = np . float32 ) / N_TIMES )[ None , :] f_sig = partial ( sigmoid , t_vec , B = 10 , x_offset = 0.5 ) # which are then mixed with a known mixing matrix mix_mat = np . array ([ [ - 0.3 , -. 28 , - 0.38 , - 0.45 , - 0.02 , - 0.12 , - 0.05 , - 0.48 ], [ 0.27 , 0.29 , - 0.34 , 0.2 , 0.41 , 0.08 , 0.11 , 0.13 ], [ - 0.14 , 0.26 , - 0.28 , - 0.14 , 0.1 , - 0.2 , 0.4 , 0.11 ], [ - 0.05 , - 0.12 , 0.28 , 0.49 , - 0.12 , 0.1 , 0.17 , 0.22 ] ], dtype = np . float32 ) . T #mix_mat = tf.convert_to_tensor(mix_mat) def gen_ds ( n_iters = 1e2 , latent_size = LATENT_SIZE ): iter_ix = 0 while iter_ix < n_iters : _input = tf . ones (( latent_size ,), dtype = tf . float32 ) latent = true_dist . sample () . numpy () _y = np . reshape ( latent , [ latent_size , 1 ]) _y = f_sig ( K = _y ) _y = mix_mat @ _y _y = _y . T yield _input , _y iter_ix += 1 ds = tf . data . Dataset . from_generator ( gen_ds , args = [ 1e2 ], output_types = ( tf . float32 , tf . float32 ), output_shapes = (( LATENT_SIZE ,), ( N_TIMES , N_SENSORS ))) ds = ds . map ( lambda x , y : ( x , ( tf . zeros ( 0 , dtype = tf . float32 ), y ))) . batch ( BATCH_SIZE ) # Train the model. # Try playing around with the 2nd loss_weights (below) and KL_WEIGHT (above). N_EPOCHS = 100 K . clear_session () prior = make_mvn_prior ( LATENT_SIZE , trainable_mean = True , trainable_var = True , offdiag = False ) model_ = make_model ( prior ) model_ . compile ( optimizer = 'adam' , loss = [ lambda _ , model_latent : tfd . kl_divergence ( model_latent , prior ), lambda y_true , model_out : - model_out . log_prob ( y_true )], loss_weights = [ 0.0 , 1.0 ]) hist = model_ . fit ( ds , epochs = N_EPOCHS , verbose = 2 ) Epoch 1/100 17/17 - 0s - loss: 91.9389 - q_latent_loss: 4.6523 - p_out_loss: 90.5613 Epoch 2/100 17/17 - 0s - loss: 11531.2471 - q_latent_loss: 4.3536 - p_out_loss: 11529.9590 Epoch 3/100 17/17 - 0s - loss: 743.5315 - q_latent_loss: 4.1376 - p_out_loss: 742.3065 Epoch 4/100 17/17 - 0s - loss: 137048.2969 - q_latent_loss: 3.9767 - p_out_loss: 137047.1250 Epoch 5/100 17/17 - 0s - loss: 113.2025 - q_latent_loss: 3.8272 - p_out_loss: 112.0695 Epoch 6/100 17/17 - 0s - loss: 74.9116 - q_latent_loss: 3.7352 - p_out_loss: 73.8058 Epoch 7/100 17/17 - 0s - loss: 79.9207 - q_latent_loss: 3.6573 - p_out_loss: 78.8380 Epoch 8/100 17/17 - 0s - loss: 372.0323 - q_latent_loss: 3.5850 - p_out_loss: 370.9711 Epoch 9/100 17/17 - 0s - loss: 60.3690 - q_latent_loss: 3.5167 - p_out_loss: 59.3279 Epoch 10/100 17/17 - 0s - loss: 7418.3252 - q_latent_loss: 3.4512 - p_out_loss: 7417.3027 Epoch 11/100 17/17 - 0s - loss: 3513.5659 - q_latent_loss: 3.3824 - p_out_loss: 3512.5649 Epoch 12/100 17/17 - 0s - loss: 53.7796 - q_latent_loss: 3.3220 - p_out_loss: 52.7962 Epoch 13/100 17/17 - 0s - loss: 20.4849 - q_latent_loss: 3.2672 - p_out_loss: 19.5177 Epoch 14/100 17/17 - 0s - loss: 1562.3738 - q_latent_loss: 3.2157 - p_out_loss: 1561.4218 Epoch 15/100 17/17 - 0s - loss: 963.6710 - q_latent_loss: 3.1652 - p_out_loss: 962.7339 Epoch 16/100 17/17 - 0s - loss: 2838.6292 - q_latent_loss: 3.1165 - p_out_loss: 2837.7068 Epoch 17/100 17/17 - 0s - loss: 35.5000 - q_latent_loss: 3.0708 - p_out_loss: 34.5910 Epoch 18/100 17/17 - 0s - loss: 706.3328 - q_latent_loss: 3.0287 - p_out_loss: 705.4363 Epoch 19/100 17/17 - 0s - loss: 52.2510 - q_latent_loss: 2.9887 - p_out_loss: 51.3663 Epoch 20/100 17/17 - 0s - loss: 7950.6792 - q_latent_loss: 2.9512 - p_out_loss: 7949.8062 Epoch 21/100 17/17 - 0s - loss: 190.5290 - q_latent_loss: 2.9112 - p_out_loss: 189.6672 Epoch 22/100 17/17 - 0s - loss: 27.1404 - q_latent_loss: 2.8756 - p_out_loss: 26.2891 Epoch 23/100 17/17 - 0s - loss: 222.1122 - q_latent_loss: 2.8434 - p_out_loss: 221.2705 Epoch 24/100 17/17 - 0s - loss: 26.6817 - q_latent_loss: 2.8132 - p_out_loss: 25.8489 Epoch 25/100 17/17 - 0s - loss: 301.0465 - q_latent_loss: 2.7844 - p_out_loss: 300.2223 Epoch 26/100 17/17 - 0s - loss: 17.3472 - q_latent_loss: 2.7568 - p_out_loss: 16.5312 Epoch 27/100 17/17 - 0s - loss: 47.1069 - q_latent_loss: 2.7306 - p_out_loss: 46.2986 Epoch 28/100 17/17 - 0s - loss: 34.0318 - q_latent_loss: 2.7057 - p_out_loss: 33.2309 Epoch 29/100 17/17 - 0s - loss: 25.4790 - q_latent_loss: 2.6819 - p_out_loss: 24.6851 Epoch 30/100 17/17 - 0s - loss: 35.2237 - q_latent_loss: 2.6592 - p_out_loss: 34.4365 Epoch 31/100 17/17 - 0s - loss: 17.7497 - q_latent_loss: 2.6375 - p_out_loss: 16.9690 Epoch 32/100 17/17 - 0s - loss: 19.0096 - q_latent_loss: 2.6168 - p_out_loss: 18.2350 Epoch 33/100 17/17 - 0s - loss: 17.3889 - q_latent_loss: 2.5969 - p_out_loss: 16.6202 Epoch 34/100 17/17 - 0s - loss: 3164.2163 - q_latent_loss: 2.5783 - p_out_loss: 3163.4539 Epoch 35/100 17/17 - 0s - loss: 318.1325 - q_latent_loss: 2.5614 - p_out_loss: 317.3743 Epoch 36/100 17/17 - 0s - loss: 496.7764 - q_latent_loss: 2.5442 - p_out_loss: 496.0233 Epoch 37/100 17/17 - 0s - loss: 43.8554 - q_latent_loss: 2.5274 - p_out_loss: 43.1073 Epoch 38/100 17/17 - 0s - loss: 127.4449 - q_latent_loss: 2.5114 - p_out_loss: 126.7015 Epoch 39/100 17/17 - 0s - loss: 34.3881 - q_latent_loss: 2.4962 - p_out_loss: 33.6492 Epoch 40/100 17/17 - 0s - loss: 92.4194 - q_latent_loss: 2.4816 - p_out_loss: 91.6848 Epoch 41/100 17/17 - 0s - loss: 2711.8994 - q_latent_loss: 2.4669 - p_out_loss: 2711.1697 Epoch 42/100 17/17 - 0s - loss: 32.7170 - q_latent_loss: 2.4484 - p_out_loss: 31.9922 Epoch 43/100 17/17 - 0s - loss: 368.3795 - q_latent_loss: 2.4338 - p_out_loss: 367.6590 Epoch 44/100 17/17 - 0s - loss: 185.1732 - q_latent_loss: 2.4207 - p_out_loss: 184.4566 Epoch 45/100 17/17 - 0s - loss: 1496.3325 - q_latent_loss: 2.4090 - p_out_loss: 1495.6195 Epoch 46/100 17/17 - 0s - loss: 40.2950 - q_latent_loss: 2.3979 - p_out_loss: 39.5852 Epoch 47/100 17/17 - 0s - loss: 3341.2693 - q_latent_loss: 2.3869 - p_out_loss: 3340.5627 Epoch 48/100 17/17 - 0s - loss: 317.7818 - q_latent_loss: 2.3705 - p_out_loss: 317.0801 Epoch 49/100 17/17 - 0s - loss: 129.0085 - q_latent_loss: 2.3577 - p_out_loss: 128.3107 Epoch 50/100 17/17 - 0s - loss: 677.0181 - q_latent_loss: 2.3475 - p_out_loss: 676.3232 Epoch 51/100 17/17 - 0s - loss: 44.6301 - q_latent_loss: 2.3379 - p_out_loss: 43.9381 Epoch 52/100 17/17 - 0s - loss: 13.6527 - q_latent_loss: 2.3290 - p_out_loss: 12.9633 Epoch 53/100 17/17 - 0s - loss: 2174.0337 - q_latent_loss: 2.3206 - p_out_loss: 2173.3469 Epoch 54/100 17/17 - 0s - loss: 85.2265 - q_latent_loss: 2.3118 - p_out_loss: 84.5422 Epoch 55/100 17/17 - 0s - loss: 13.3743 - q_latent_loss: 2.3037 - p_out_loss: 12.6924 Epoch 56/100 17/17 - 0s - loss: 46996.4805 - q_latent_loss: 2.2929 - p_out_loss: 46995.8008 Epoch 57/100 17/17 - 0s - loss: 23.0730 - q_latent_loss: 2.2527 - p_out_loss: 22.4061 Epoch 58/100 17/17 - 0s - loss: 11.3692 - q_latent_loss: 2.2361 - p_out_loss: 10.7074 Epoch 59/100 17/17 - 0s - loss: 25.0329 - q_latent_loss: 2.2285 - p_out_loss: 24.3733 Epoch 60/100 17/17 - 0s - loss: 17.7706 - q_latent_loss: 2.2227 - p_out_loss: 17.1126 Epoch 61/100 17/17 - 0s - loss: 10.4981 - q_latent_loss: 2.2176 - p_out_loss: 9.8417 Epoch 62/100 17/17 - 0s - loss: 32.7279 - q_latent_loss: 2.2127 - p_out_loss: 32.0729 Epoch 63/100 17/17 - 0s - loss: 30.3548 - q_latent_loss: 2.2081 - p_out_loss: 29.7012 Epoch 64/100 17/17 - 0s - loss: 30.8280 - q_latent_loss: 2.2037 - p_out_loss: 30.1757 Epoch 65/100 17/17 - 0s - loss: 41.9700 - q_latent_loss: 2.1996 - p_out_loss: 41.3189 Epoch 66/100 17/17 - 0s - loss: 54.7435 - q_latent_loss: 2.1956 - p_out_loss: 54.0936 Epoch 67/100 17/17 - 0s - loss: 43.0750 - q_latent_loss: 2.1918 - p_out_loss: 42.4262 Epoch 68/100 17/17 - 0s - loss: 243.1561 - q_latent_loss: 2.1882 - p_out_loss: 242.5083 Epoch 69/100 17/17 - 0s - loss: 22.0350 - q_latent_loss: 2.1842 - p_out_loss: 21.3885 Epoch 70/100 17/17 - 0s - loss: 99.8952 - q_latent_loss: 2.1805 - p_out_loss: 99.2498 Epoch 71/100 17/17 - 0s - loss: 31.7489 - q_latent_loss: 2.1772 - p_out_loss: 31.1044 Epoch 72/100 17/17 - 0s - loss: 10.3156 - q_latent_loss: 2.1741 - p_out_loss: 9.6721 Epoch 73/100 17/17 - 0s - loss: 10.8083 - q_latent_loss: 2.1710 - p_out_loss: 10.1656 Epoch 74/100 17/17 - 0s - loss: 149.1215 - q_latent_loss: 2.1682 - p_out_loss: 148.4797 Epoch 75/100 17/17 - 0s - loss: 11.4474 - q_latent_loss: 2.1655 - p_out_loss: 10.8064 Epoch 76/100 17/17 - 0s - loss: 11.3532 - q_latent_loss: 2.1629 - p_out_loss: 10.7130 Epoch 77/100 17/17 - 0s - loss: 9.8263 - q_latent_loss: 2.1603 - p_out_loss: 9.1868 Epoch 78/100 17/17 - 0s - loss: 12.8481 - q_latent_loss: 2.1578 - p_out_loss: 12.2094 Epoch 79/100 17/17 - 0s - loss: 10.0819 - q_latent_loss: 2.1554 - p_out_loss: 9.4439 Epoch 80/100 17/17 - 0s - loss: 9.9589 - q_latent_loss: 2.1531 - p_out_loss: 9.3216 Epoch 81/100 17/17 - 0s - loss: 148.1879 - q_latent_loss: 2.1508 - p_out_loss: 147.5513 Epoch 82/100 17/17 - 0s - loss: 13.6919 - q_latent_loss: 2.1483 - p_out_loss: 13.0560 Epoch 83/100 17/17 - 0s - loss: 11.4130 - q_latent_loss: 2.1462 - p_out_loss: 10.7777 Epoch 84/100 17/17 - 0s - loss: 62.1876 - q_latent_loss: 2.1442 - p_out_loss: 61.5529 Epoch 85/100 17/17 - 0s - loss: 37.1363 - q_latent_loss: 2.1424 - p_out_loss: 36.5021 Epoch 86/100 17/17 - 0s - loss: 9.0239 - q_latent_loss: 2.1406 - p_out_loss: 8.3903 Epoch 87/100 17/17 - 0s - loss: 9.6573 - q_latent_loss: 2.1389 - p_out_loss: 9.0242 Epoch 88/100 17/17 - 0s - loss: 16.5087 - q_latent_loss: 2.1371 - p_out_loss: 15.8761 Epoch 89/100 17/17 - 0s - loss: 9.0933 - q_latent_loss: 2.1355 - p_out_loss: 8.4612 Epoch 90/100 17/17 - 0s - loss: 11.1045 - q_latent_loss: 2.1339 - p_out_loss: 10.4729 Epoch 91/100 17/17 - 0s - loss: 14.6627 - q_latent_loss: 2.1323 - p_out_loss: 14.0315 Epoch 92/100 17/17 - 0s - loss: 12.7246 - q_latent_loss: 2.1308 - p_out_loss: 12.0939 Epoch 93/100 17/17 - 0s - loss: 18.2482 - q_latent_loss: 2.1294 - p_out_loss: 17.6179 Epoch 94/100 17/17 - 0s - loss: 12.0889 - q_latent_loss: 2.1280 - p_out_loss: 11.4590 Epoch 95/100 17/17 - 0s - loss: 35.7015 - q_latent_loss: 2.1267 - p_out_loss: 35.0720 Epoch 96/100 17/17 - 0s - loss: 156.0881 - q_latent_loss: 2.1253 - p_out_loss: 155.4590 Epoch 97/100 17/17 - 0s - loss: 8.8568 - q_latent_loss: 2.1240 - p_out_loss: 8.2280 Epoch 98/100 17/17 - 0s - loss: 10.9563 - q_latent_loss: 2.1228 - p_out_loss: 10.3279 Epoch 99/100 17/17 - 0s - loss: 10.1586 - q_latent_loss: 2.1217 - p_out_loss: 9.5306 Epoch 100/100 17/17 - 0s - loss: 14.0194 - q_latent_loss: 2.1206 - p_out_loss: 13.3917 lat_wts = model_ . get_layer ( \"latent_loc\" ) . weights lat_locs = np . ones (( 1 , LATENT_SIZE )) @ lat_wts [ 0 ] . numpy () + lat_wts [ 1 ] . numpy () mix_wts = model_ . get_layer ( \"out_loc\" ) . weights model_out = lat_locs @ mix_wts [ 0 ] . numpy () + mix_wts [ 1 ] . numpy () true_out = mix_mat @ true_dist . mean () . numpy () print ( f \"Model est lat: { lat_locs } \" ) print ( f \"Model est out: { model_out } \" ) print ( f \"prior mean: { prior . mean () . numpy () } \" ) print ( f \"true lat: { true_dist . mean () . numpy () } \" ) print ( f \"true out: { true_out . T } \" ) Model est lat: [[ 0.51195845 -1.02728739 0.67845761 -0.11073773]] Model est out: [[ 0.10048061 1.36010552 0.30864524 -0.09840383 1.17217551 -0.67099368 0.95406085 0.03414997]] prior mean: [ 0.5117957 -0.8991166 0.66152537 -0.11197621] true lat: [-1. 1. 5. -5.] true out: [ 0.12000006 2.4699998 -2.76 -2.5 1.53 -1.3 1.31 0.05999994] # test LearnableMultivariateNormalDiag prior_factory = LearnableMultivariateNormalDiag ( LATENT_SIZE ) learnable_prior = prior_factory () sample = learnable_prior . sample (( 100 , 64 )) print ( sample . shape ) print ( learnable_prior . trainable_variables ) (100, 64, 4) (<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 , 0.0387747 , 0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143, 0.06062925, 0.02439827, -0.01735771], dtype=float32)>) K . clear_session () model_ = make_model ( learnable_prior ) model_ . compile ( optimizer = 'adam' , loss = [ lambda _ , model_latent : tfd . kl_divergence ( model_latent , learnable_prior ), lambda y_true , model_out : - model_out . log_prob ( y_true )], loss_weights = [ 0.0 , 1.0 ]) print ( learnable_prior . trainable_variables ) print ([ _ . name for _ in model_ . trainable_variables ]) hist = model_ . fit ( ds , epochs = N_EPOCHS , verbose = 2 ) lat_wts = model_ . get_layer ( \"latent_loc\" ) . weights lat_locs = np . ones (( 1 , LATENT_SIZE )) @ lat_wts [ 0 ] . numpy () + lat_wts [ 1 ] . numpy () mix_wts = model_ . get_layer ( \"out_loc\" ) . weights model_out = lat_locs @ mix_wts [ 0 ] . numpy () + mix_wts [ 1 ] . numpy () true_out = mix_mat @ true_dist . mean () . numpy () print ( f \"Model est lat: { lat_locs } \" ) print ( f \"Model est out: { model_out } \" ) print ( f \"prior mean: { learnable_prior . mean () . numpy () } \" ) print ( f \"true lat: { true_dist . mean () . numpy () } \" ) print ( f \"true out: { true_out . T } \" ) (<tf.Variable 'learnable_multivariate_normal_diag_2/mean:0' shape=(4,) dtype=float32, numpy=array([ 0.16748714, -0.1799583 , 0.0387747 , 0.11378615], dtype=float32)>, <tf.Variable 'learnable_multivariate_normal_diag_2/transformed_scale:0' shape=(4,) dtype=float32, numpy=array([-0.11407143, 0.06062925, 0.02439827, -0.01735771], dtype=float32)>) ['dense/kernel:0', 'dense/bias:0', 'latent_loc/kernel:0', 'latent_loc/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'out_loc/kernel:0', 'out_loc/bias:0', 'learnable_multivariate_normal_diag_2/mean:0', 'learnable_multivariate_normal_diag_2/transformed_scale:0'] Epoch 1/100 17/17 - 0s - loss: 266.6295 - q_latent_loss: 6.5640 - p_out_loss: 264.6859 Epoch 2/100 17/17 - 0s - loss: 2032.0966 - q_latent_loss: 6.2853 - p_out_loss: 2030.2358 Epoch 3/100 17/17 - 0s - loss: 83.5799 - q_latent_loss: 6.0718 - p_out_loss: 81.7824 Epoch 4/100 17/17 - 0s - loss: 82.7522 - q_latent_loss: 5.8942 - p_out_loss: 81.0072 Epoch 5/100 17/17 - 0s - loss: 59.2224 - q_latent_loss: 5.7269 - p_out_loss: 57.5269 Epoch 6/100 17/17 - 0s - loss: 38.8948 - q_latent_loss: 5.5706 - p_out_loss: 37.2456 Epoch 7/100 17/17 - 0s - loss: 47.8537 - q_latent_loss: 5.4227 - p_out_loss: 46.2483 Epoch 8/100 17/17 - 0s - loss: 60.9186 - q_latent_loss: 5.2828 - p_out_loss: 59.3546 Epoch 9/100 17/17 - 0s - loss: 80.7008 - q_latent_loss: 5.1479 - p_out_loss: 79.1768 Epoch 10/100 17/17 - 0s - loss: 29.5548 - q_latent_loss: 5.0204 - p_out_loss: 28.0686 Epoch 11/100 17/17 - 0s - loss: 100.5337 - q_latent_loss: 4.9013 - p_out_loss: 99.0827 Epoch 12/100 17/17 - 0s - loss: 208.5356 - q_latent_loss: 4.7856 - p_out_loss: 207.1189 Epoch 13/100 17/17 - 0s - loss: 47.4895 - q_latent_loss: 4.6692 - p_out_loss: 46.1072 Epoch 14/100 17/17 - 0s - loss: 51.8070 - q_latent_loss: 4.5624 - p_out_loss: 50.4563 Epoch 15/100 17/17 - 0s - loss: 49.2825 - q_latent_loss: 4.4640 - p_out_loss: 47.9610 Epoch 16/100 17/17 - 0s - loss: 63.7341 - q_latent_loss: 4.3716 - p_out_loss: 62.4399 Epoch 17/100 17/17 - 0s - loss: 35.2299 - q_latent_loss: 4.2837 - p_out_loss: 33.9617 Epoch 18/100 17/17 - 0s - loss: 45.8432 - q_latent_loss: 4.2006 - p_out_loss: 44.5997 Epoch 19/100 17/17 - 0s - loss: 25.7876 - q_latent_loss: 4.1215 - p_out_loss: 24.5675 Epoch 20/100 17/17 - 0s - loss: 268.8558 - q_latent_loss: 4.0396 - p_out_loss: 267.6599 Epoch 21/100 17/17 - 0s - loss: 45.2869 - q_latent_loss: 3.9513 - p_out_loss: 44.1171 Epoch 22/100 17/17 - 0s - loss: 30.1766 - q_latent_loss: 3.8787 - p_out_loss: 29.0284 Epoch 23/100 17/17 - 0s - loss: 32.2969 - q_latent_loss: 3.8108 - p_out_loss: 31.1688 Epoch 24/100 17/17 - 0s - loss: 64.0437 - q_latent_loss: 3.7457 - p_out_loss: 62.9348 Epoch 25/100 17/17 - 0s - loss: 39.9464 - q_latent_loss: 3.6825 - p_out_loss: 38.8562 Epoch 26/100 17/17 - 0s - loss: 33.5094 - q_latent_loss: 3.6220 - p_out_loss: 32.4372 Epoch 27/100 17/17 - 0s - loss: 31.4306 - q_latent_loss: 3.5643 - p_out_loss: 30.3755 Epoch 28/100 17/17 - 0s - loss: 27.8061 - q_latent_loss: 3.5087 - p_out_loss: 26.7674 Epoch 29/100 17/17 - 0s - loss: 65.8272 - q_latent_loss: 3.4540 - p_out_loss: 64.8047 Epoch 30/100 17/17 - 0s - loss: 25.9475 - q_latent_loss: 3.4009 - p_out_loss: 24.9408 Epoch 31/100 17/17 - 0s - loss: 30.2780 - q_latent_loss: 3.3515 - p_out_loss: 29.2859 Epoch 32/100 17/17 - 0s - loss: 21.8850 - q_latent_loss: 3.3044 - p_out_loss: 20.9068 Epoch 33/100 17/17 - 0s - loss: 36.6851 - q_latent_loss: 3.2587 - p_out_loss: 35.7204 Epoch 34/100 17/17 - 0s - loss: 25.5569 - q_latent_loss: 3.2120 - p_out_loss: 24.6061 Epoch 35/100 17/17 - 0s - loss: 24.6902 - q_latent_loss: 3.1682 - p_out_loss: 23.7523 Epoch 36/100 17/17 - 0s - loss: 116.0450 - q_latent_loss: 3.1269 - p_out_loss: 115.1194 Epoch 37/100 17/17 - 0s - loss: 20.0418 - q_latent_loss: 3.0908 - p_out_loss: 19.1268 Epoch 38/100 17/17 - 0s - loss: 56.1398 - q_latent_loss: 3.0497 - p_out_loss: 55.2370 Epoch 39/100 17/17 - 0s - loss: 27.5171 - q_latent_loss: 3.0067 - p_out_loss: 26.6270 Epoch 40/100 17/17 - 0s - loss: 20.7006 - q_latent_loss: 2.9684 - p_out_loss: 19.8219 Epoch 41/100 17/17 - 0s - loss: 26.9046 - q_latent_loss: 2.9328 - p_out_loss: 26.0364 Epoch 42/100 17/17 - 0s - loss: 18.7693 - q_latent_loss: 2.8996 - p_out_loss: 17.9110 Epoch 43/100 17/17 - 0s - loss: 22.3650 - q_latent_loss: 2.8670 - p_out_loss: 21.5163 Epoch 44/100 17/17 - 0s - loss: 32.9155 - q_latent_loss: 2.8352 - p_out_loss: 32.0763 Epoch 45/100 17/17 - 0s - loss: 19.9130 - q_latent_loss: 2.8037 - p_out_loss: 19.0830 Epoch 46/100 17/17 - 0s - loss: 19.9001 - q_latent_loss: 2.7740 - p_out_loss: 19.0789 Epoch 47/100 17/17 - 0s - loss: 25.4838 - q_latent_loss: 2.7436 - p_out_loss: 24.6716 Epoch 48/100 17/17 - 0s - loss: 23.9622 - q_latent_loss: 2.7135 - p_out_loss: 23.1589 Epoch 49/100 17/17 - 0s - loss: 20.7703 - q_latent_loss: 2.6849 - p_out_loss: 19.9756 Epoch 50/100 17/17 - 0s - loss: 19.6302 - q_latent_loss: 2.6576 - p_out_loss: 18.8435 Epoch 51/100 17/17 - 0s - loss: 18.7125 - q_latent_loss: 2.6321 - p_out_loss: 17.9334 Epoch 52/100 17/17 - 0s - loss: 21.4065 - q_latent_loss: 2.6073 - p_out_loss: 20.6347 Epoch 53/100 17/17 - 0s - loss: 37.3685 - q_latent_loss: 2.5831 - p_out_loss: 36.6039 Epoch 54/100 17/17 - 0s - loss: 15.8975 - q_latent_loss: 2.5606 - p_out_loss: 15.1395 Epoch 55/100 17/17 - 0s - loss: 15.6574 - q_latent_loss: 2.5387 - p_out_loss: 14.9059 Epoch 56/100 17/17 - 0s - loss: 28.7901 - q_latent_loss: 2.5174 - p_out_loss: 28.0449 Epoch 57/100 17/17 - 0s - loss: 99.3240 - q_latent_loss: 2.4972 - p_out_loss: 98.5848 Epoch 58/100 17/17 - 0s - loss: 19.6783 - q_latent_loss: 2.4761 - p_out_loss: 18.9453 Epoch 59/100 17/17 - 0s - loss: 18.9958 - q_latent_loss: 2.4563 - p_out_loss: 18.2688 Epoch 60/100 17/17 - 0s - loss: 21.3663 - q_latent_loss: 2.4364 - p_out_loss: 20.6451 Epoch 61/100 17/17 - 0s - loss: 26.8008 - q_latent_loss: 2.4179 - p_out_loss: 26.0850 Epoch 62/100 17/17 - 0s - loss: 13.9355 - q_latent_loss: 2.3984 - p_out_loss: 13.2256 Epoch 63/100 17/17 - 0s - loss: 14.0786 - q_latent_loss: 2.3803 - p_out_loss: 13.3740 Epoch 64/100 17/17 - 0s - loss: 20.6991 - q_latent_loss: 2.3634 - p_out_loss: 19.9995 Epoch 65/100 17/17 - 0s - loss: 33.9438 - q_latent_loss: 2.3476 - p_out_loss: 33.2488 Epoch 66/100 17/17 - 0s - loss: 19.5023 - q_latent_loss: 2.3325 - p_out_loss: 18.8118 Epoch 67/100 17/17 - 0s - loss: 16.1214 - q_latent_loss: 2.3179 - p_out_loss: 15.4353 Epoch 68/100 17/17 - 0s - loss: 33.3983 - q_latent_loss: 2.3044 - p_out_loss: 32.7162 Epoch 69/100 17/17 - 0s - loss: 14.1833 - q_latent_loss: 2.2933 - p_out_loss: 13.5045 Epoch 70/100 17/17 - 0s - loss: 33.0913 - q_latent_loss: 2.2802 - p_out_loss: 32.4163 Epoch 71/100 17/17 - 0s - loss: 15.5565 - q_latent_loss: 2.2661 - p_out_loss: 14.8857 Epoch 72/100 17/17 - 0s - loss: 23.7552 - q_latent_loss: 2.2522 - p_out_loss: 23.0885 Epoch 73/100 17/17 - 0s - loss: 15.8186 - q_latent_loss: 2.2402 - p_out_loss: 15.1554 Epoch 74/100 17/17 - 0s - loss: 15.8109 - q_latent_loss: 2.2277 - p_out_loss: 15.1514 Epoch 75/100 17/17 - 0s - loss: 23.2216 - q_latent_loss: 2.2153 - p_out_loss: 22.5659 Epoch 76/100 17/17 - 0s - loss: 17.1244 - q_latent_loss: 2.2021 - p_out_loss: 16.4725 Epoch 77/100 17/17 - 0s - loss: 20.2818 - q_latent_loss: 2.1875 - p_out_loss: 19.6343 Epoch 78/100 17/17 - 0s - loss: 20.1146 - q_latent_loss: 2.1751 - p_out_loss: 19.4708 Epoch 79/100 17/17 - 0s - loss: 12.0626 - q_latent_loss: 2.1647 - p_out_loss: 11.4218 Epoch 80/100 17/17 - 0s - loss: 17.5959 - q_latent_loss: 2.1547 - p_out_loss: 16.9580 Epoch 81/100 17/17 - 0s - loss: 46.9798 - q_latent_loss: 2.1428 - p_out_loss: 46.3454 Epoch 82/100 17/17 - 0s - loss: 20.0080 - q_latent_loss: 2.1244 - p_out_loss: 19.3792 Epoch 83/100 17/17 - 0s - loss: 11.8902 - q_latent_loss: 2.1120 - p_out_loss: 11.2651 Epoch 84/100 17/17 - 0s - loss: 17.5359 - q_latent_loss: 2.1015 - p_out_loss: 16.9139 Epoch 85/100 17/17 - 0s - loss: 14.8084 - q_latent_loss: 2.0917 - p_out_loss: 14.1892 Epoch 86/100 17/17 - 0s - loss: 9.6016 - q_latent_loss: 2.0823 - p_out_loss: 8.9852 Epoch 87/100 17/17 - 0s - loss: 13.4432 - q_latent_loss: 2.0736 - p_out_loss: 12.8294 Epoch 88/100 17/17 - 0s - loss: 16.5978 - q_latent_loss: 2.0652 - p_out_loss: 15.9865 Epoch 89/100 17/17 - 0s - loss: 21.3484 - q_latent_loss: 2.0557 - p_out_loss: 20.7399 Epoch 90/100 17/17 - 0s - loss: 11.3400 - q_latent_loss: 2.0439 - p_out_loss: 10.7350 Epoch 91/100 17/17 - 0s - loss: 14.2551 - q_latent_loss: 2.0345 - p_out_loss: 13.6529 Epoch 92/100 17/17 - 0s - loss: 14.2384 - q_latent_loss: 2.0268 - p_out_loss: 13.6385 Epoch 93/100 17/17 - 0s - loss: 16.3489 - q_latent_loss: 2.0194 - p_out_loss: 15.7511 Epoch 94/100 17/17 - 0s - loss: 14.2265 - q_latent_loss: 2.0118 - p_out_loss: 13.6310 Epoch 95/100 17/17 - 0s - loss: 11.5992 - q_latent_loss: 2.0041 - p_out_loss: 11.0060 Epoch 96/100 17/17 - 0s - loss: 11.7333 - q_latent_loss: 1.9971 - p_out_loss: 11.1421 Epoch 97/100 17/17 - 0s - loss: 12.1329 - q_latent_loss: 1.9904 - p_out_loss: 11.5438 Epoch 98/100 17/17 - 0s - loss: 13.7211 - q_latent_loss: 1.9832 - p_out_loss: 13.1341 Epoch 99/100 17/17 - 0s - loss: 37.2112 - q_latent_loss: 1.9745 - p_out_loss: 36.6267 Epoch 100/100 17/17 - 0s - loss: 9.3972 - q_latent_loss: 1.9630 - p_out_loss: 8.8161 Model est lat: [[ 2.51292503 -1.26424221 -1.11180196 -0.02588509]] Model est out: [[-0.27425705 2.39304429 -2.49596335 0.29950965 1.37276651 -0.43098222 -0.45473986 0.07839915]] prior mean: [ 1.3514248 -1.2266612 -0.8751619 -0.03475915] true lat: [-1. 1. 5. -5.] true out: [ 0.12000006 2.4699998 -2.76 -2.5 1.53 -1.3 1.31 0.05999994]","title":"Tensorflow Probability Utilities"},{"location":"bVAE/tfp_utils/#latent-dynamic-factor","text":"# Return 3 outputs, the first 2 are null #ds_dyn = ds.map(lambda x, y: (x, (y[0], y[0], y[1]))) ds_dyn = ds . map ( lambda x , y : ( x , y [ 1 ])) KL_WEIGHT = 0.001 LATENT_SIZE_DYNAMIC = 1 # Integer dimensionality of each dynamic, time-variant latent variable `z_t`. class VariationalLSTMCell ( tfkl . LSTMCell ): def __init__ ( self , units , make_dist_fn = None , make_dist_model = None , ** kwargs ): super ( VariationalLSTMCell , self ) . __init__ ( units , ** kwargs ) self . make_dist_fn = make_dist_fn self . make_dist_model = make_dist_model # For some reason the below code doesn't work during build. # So I don't know how to use the outer VariationalRNN to set this cell's output_size if self . make_dist_fn is None : self . make_dist_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]) if self . make_dist_model is None : fake_cell_output = tfkl . Input (( self . units ,)) loc = tfkl . Dense ( self . output_size , name = \"VarLSTMCell_loc\" )( fake_cell_output ) scale = tfkl . Dense ( self . output_size , name = \"VarLSTMCell_scale\" )( fake_cell_output ) scale = tf . nn . softplus ( scale + scale_shift ) + 1e-5 dist_layer = tfpl . DistributionLambda ( make_distribution_fn = self . make_dist_fn , # TODO: convert_to_tensor_fn=lambda s: s.sample(N_SAMPLES) )([ loc , scale ]) self . make_dist_model = tf . keras . Model ( fake_cell_output , dist_layer ) def build ( self , input_shape ): super ( VariationalLSTMCell , self ) . build ( input_shape ) # It would be good to defer making self.make_dist_model until here, # but it doesn't work for some reason. #def input_zero(self, inputs_): # input0 = inputs_[..., -1, :] # input0 = tf.matmul(input0, tf.zeros((input0.shape[-1], self.units))) # dist0 = self.make_dist_model(input0) # return dist0 def call ( self , inputs , states , training = None ): inputs = tf . convert_to_tensor ( inputs ) output , state = super ( VariationalLSTMCell , self ) . call ( inputs , states , training = training ) dist = self . make_dist_model ( output ) return dist , state K . clear_session () tmp = LearnableMultivariateNormalDiagCell ( 3 , 4 ) #tmp.build((None, 10, 5)) #tmp.summary() class DynamicEncoder ( tf . keras . Model ): def __init__ ( self , units , n_times , output_dim , name = \"dynamic_encoder\" ): super ( DynamicEncoder , self ) . __init__ ( name = name ) self . dynamic_prior_cell = LearnableMultivariateNormalDiagCell ( units , output_dim ) self . n_times = n_times self . loc = tfkl . Dense ( output_dim , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( output_dim , name = \"scale\" ) self . q_z_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]), name = \"q_z\" ) def call ( self , inputs ): # Assume inputs doesn't have time-axis. Broadcast-add zeros to add time axis. inputs_ = inputs [ ... , tf . newaxis , :] + tf . zeros ([ self . n_times , 1 ]) loc = self . loc ( inputs_ ) unxf_scale = self . unxf_scale ( inputs_ ) scale = tf . math . softplus ( unxf_scale + scale_shift ) + 1e-5 q_z = self . q_z_layer ([ loc , scale ]) # _, dynamic_prior = self.sample_dynamic_prior(self.n_times) #kld = tfd.kl_divergence(q_z, dynamic_prior) #kld = tf.reduce_sum(kld, axis=-1) #kld = tf.reduce_mean(kld) #self.add_loss(KL_WEIGHT * kld) return q_z def sample_dynamic_prior ( self , steps , samples = 1 , batches = 1 , fixed = False ): \"\"\"Samples LSTM cell->MVNDiag for each steps Args: steps: Number of timesteps to sample for each sequence. samples: Number of samples to draw from the latent distribution. batch_size: Number of sequences to sample. fixed: Boolean for whether or not to share the same random sample across all sequences. Returns: A tuple of a sample tensor of shape [samples, batch_size, steps, latent_size], and a MultivariateNormalDiag distribution from which the tensor was sampled, with event shape [latent_size], and batch shape [samples, 1, length] if fixed or [samples, batch_size, length] otherwise. \"\"\" if fixed : sample_batch_size = 1 else : sample_batch_size = batches sample , state = self . dynamic_prior_cell . zero_state ([ samples , sample_batch_size ]) locs = [] scale_diags = [] sample_list = [] for _ in range ( steps ): dist , state = self . dynamic_prior_cell ( sample , state ) sample = dist . sample () locs . append ( dist . parameters [ \"loc\" ]) scale_diags . append ( dist . parameters [ \"scale_diag\" ]) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : # tile along the batch axis sample = sample + tf . zeros ([ batches , 1 , 1 ]) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) # test DynamicEncoder and LearnableMultivariateNormalDiagCell K . clear_session () dynamic_encoder = DynamicEncoder ( N_HIDDEN , N_TIMES , LATENT_SIZE_DYNAMIC ) sample , dynamic_prior = dynamic_encoder . sample_dynamic_prior ( N_TIMES , samples = N_SAMPLES , batches = 1 ) print ( sample . shape ) print ( \"mean:\" , np . squeeze ( dynamic_prior . mean ())) print ( \"stddev:\" , np . squeeze ( dynamic_prior . stddev ())) print ([ _ . name for _ in dynamic_encoder . trainable_variables ]) (2, 1, 10, 1) mean: [[ 0. 0.42388976 0.45631832 0.365768 0.20130846 0.37873474 0.31262326 0.26073667 0.15399611 0.14049806] [ 0. 0.08804662 0.0361465 -0.03267653 -0.08733355 0.19941618 0.30335566 0.3730844 0.2744042 0.17948757]] stddev: [[1.00001 0.929902 0.9554563 0.99371654 1.0142238 0.97018814 1.0006421 1.0033575 1.0105829 1.0065393 ] [1.00001 0.9952911 1.0008274 1.0000954 0.99874425 0.98230976 0.9771384 0.9683764 1.000174 1.0049998 ]] ['learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/recurrent_kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_lstm/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_loc/bias:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/kernel:0', 'learnable_multivariate_normal_diag_cell/mvndiagcell_scale/bias:0'] class StaticEncoder ( tf . keras . Model ): def __init__ ( self , latent_size , name = \"static_encoder\" ): super ( StaticEncoder , self ) . __init__ ( name = name ) self . static_prior_factory = LearnableMultivariateNormalDiag ( latent_size ) self . loc = tfkl . Dense ( latent_size , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( tfpl . MultivariateNormalTriL . params_size ( latent_size ) - latent_size , name = \"scale\" ) self . scale_bijector = tfp . bijectors . FillScaleTriL () self . q_f_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalTriL ( loc = t [ 0 ], scale_tril = t [ 1 ]), name = \"q_f\" ) def call ( self , inputs ): loc = self . loc ( inputs ) unxf_scale = self . unxf_scale ( inputs ) scale = self . scale_bijector ( unxf_scale ) q_f = self . q_f_layer ([ loc , scale ]) #static_prior = self.static_prior_factory() #kld = tfd.kl_divergence(q_f, static_prior) #kld = tf.reduce_mean(kld) #self.add_loss(KL_WEIGHT * kld) return q_f class Decoder ( tf . keras . Model ): def __init__ ( self , n_times , out_dim , name = 'decoder' ): super ( FactorizedDecoder , self ) . __init__ ( name = name ) self . n_times = n_times self . concat = tfkl . Concatenate () self . loc = tfkl . Dense ( out_dim , name = \"loc\" ) self . unxf_scale = tfkl . Dense ( out_dim , name = \"scale\" ) self . q_z_layer = tfpl . DistributionLambda ( make_distribution_fn = lambda t : tfd . MultivariateNormalDiag ( loc = t [ 0 ], scale_diag = t [ 1 ]), name = \"p_out\" ) def call ( self , inputs ): f_sample = inputs [ 0 ][ ... , tf . newaxis , :] + tf . zeros ([ self . n_times , 1 ]) z_sample = tf . convert_to_tensor ( inputs [ 1 ]) y = self . concat ([ z_sample , f_sample ]) loc = self . loc ( y ) unxf_scale = self . unxf_scale ( y ) scale = tf . math . softplus ( unxf_scale + scale_shift ) + 1e-5 p_out = self . q_z_layer ([ loc , scale ]) return p_out class FactorizedAutoEncoder ( tf . keras . Model ): def __init__ ( self , units , n_times , latent_size_static , latent_size_dynamic , n_out_dim , name = 'autoencoder' ): super ( FactorizedAutoEncoder , self ) . __init__ ( name = name ) self . static_encoder = StaticEncoder ( latent_size_static ) self . dynamic_encoder = DynamicEncoder ( units , n_times , latent_size_dynamic ) self . decoder = Decoder ( n_times , n_out_dim ) def call ( self , inputs ): q_f = self . static_encoder ( inputs ) q_z = self . dynamic_encoder ( inputs ) p_out = self . decoder ([ tf . convert_to_tensor ( q_f ), tf . convert_to_tensor ( q_z )]) return p_out K . clear_session () f_model = FactorizedAutoEncoder ( N_HIDDEN , N_TIMES , LATENT_SIZE , LATENT_SIZE_DYNAMIC , N_SENSORS ) # Most of the trainable variables don't present themselves until the model pieces are called. print ([ _ . name for _ in f_model . static_encoder . trainable_variables ]) print ([ _ . name for _ in f_model . dynamic_encoder . trainable_variables ]) print ([ _ . name for _ in f_model . decoder . trainable_variables ]) [] ['learnable_multivariate_normal_diag/mean:0', 'learnable_multivariate_normal_diag/untransformed_stddev:0'] [] [] N_EPOCHS = 200 if False : f_model . compile ( optimizer = 'adam' , loss = lambda y_true , model_out : - model_out . log_prob ( y_true )) hist = f_model . fit ( ds_dyn , epochs = N_EPOCHS , verbose = 2 ) else : @tf . function def grad ( model , inputs , preds ): with tf . GradientTape () as tape : q_f = model . static_encoder ( inputs ) q_z = model . dynamic_encoder ( inputs ) p_full = model . decoder ([ tf . convert_to_tensor ( q_f ), tf . convert_to_tensor ( q_z )]) # Reconstruction log-likelihood: p(output|input) recon_post_log_prob = p_full . log_prob ( preds ) recon_post_log_prob = tf . reduce_sum ( recon_post_log_prob , axis =- 1 ) # Sum over time axis recon_post_log_prob = tf . reduce_mean ( recon_post_log_prob ) # KL Divergence - analytical # Static static_prior = model . static_encoder . static_prior_factory () stat_kl = tfd . kl_divergence ( q_f , static_prior ) stat_kl = KL_WEIGHT * stat_kl stat_kl = tf . reduce_mean ( stat_kl ) # Dynamic _ , dynamic_prior = model . dynamic_encoder . sample_dynamic_prior ( N_TIMES , samples = 1 , batches = 1 ) dyn_kl = tfd . kl_divergence ( q_z , dynamic_prior ) dyn_kl = tf . reduce_sum ( dyn_kl , axis =- 1 ) dyn_kl = tf . squeeze ( dyn_kl ) dyn_kl = KL_WEIGHT * dyn_kl dyn_kl = tf . reduce_mean ( dyn_kl ) loss = - recon_post_log_prob + stat_kl + dyn_kl grads = tape . gradient ( loss , model . trainable_variables ) return loss , grads , ( - recon_post_log_prob , stat_kl , dyn_kl ) optim = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ) for epoch_ix in range ( N_EPOCHS ): for step_ix , batch in enumerate ( ds_dyn ): inputs , preds = batch loss , grads , loss_comps = grad ( f_model , inputs , preds ) optim . apply_gradients ( zip ( grads , f_model . trainable_variables )) if ( step_ix % 200 ) == 0 : print ( '.' ) print ( f \"Epoch { epoch_ix } / { N_EPOCHS } : \\t loss= { loss : .3f } ; \" f \"Losses: { [ _ . numpy () for _ in loss_comps ] } \" ) . Epoch 0/200: loss=3777.438; Losses: [3777.3972, 0.0028260916, 0.03819199] . Epoch 1/200: loss=908.620; Losses: [908.5834, 0.002681077, 0.034407526] . Epoch 2/200: loss=682.733; Losses: [682.7001, 0.0025911012, 0.030505255] . Epoch 3/200: loss=317.985; Losses: [317.9555, 0.0024803109, 0.027142774] . Epoch 4/200: loss=737.753; Losses: [737.7305, 0.0024128703, 0.019867169] . Epoch 5/200: loss=293.983; Losses: [293.9585, 0.0023704215, 0.022514882] . Epoch 6/200: loss=412.075; Losses: [412.0592, 0.002335041, 0.013460781] . Epoch 7/200: loss=306.618; Losses: [306.60327, 0.002284743, 0.012861049] . Epoch 8/200: loss=215.916; Losses: [215.9029, 0.0022479186, 0.0111077] . Epoch 9/200: loss=223.136; Losses: [223.12366, 0.0022122823, 0.010420884] . Epoch 10/200: loss=300.156; Losses: [300.14383, 0.0021807426, 0.009667382] . Epoch 11/200: loss=179.766; Losses: [179.75607, 0.0021481065, 0.007676568] . Epoch 12/200: loss=215.981; Losses: [215.97124, 0.0021149626, 0.008112778] . Epoch 13/200: loss=208.054; Losses: [208.04565, 0.0020882282, 0.006530414] . Epoch 14/200: loss=207.146; Losses: [207.13806, 0.0020594192, 0.0063194335] . Epoch 15/200: loss=261.805; Losses: [261.7982, 0.0020306753, 0.0045144106] . Epoch 16/200: loss=178.393; Losses: [178.38637, 0.0020005947, 0.0045015276] . Epoch 17/200: loss=284.872; Losses: [284.86642, 0.0019730518, 0.003997615] . Epoch 18/200: loss=212.054; Losses: [212.04866, 0.0019469936, 0.0034680453] . Epoch 19/200: loss=145.862; Losses: [145.85641, 0.001922644, 0.004011639] . Epoch 20/200: loss=182.153; Losses: [182.14563, 0.0018981744, 0.0059485184] . Epoch 21/200: loss=154.575; Losses: [154.56941, 0.0018728755, 0.0038126395] . Epoch 22/200: loss=214.752; Losses: [214.74734, 0.0018469194, 0.003135754] . Epoch 23/200: loss=179.347; Losses: [179.34248, 0.001825613, 0.0029100403] . Epoch 24/200: loss=354.274; Losses: [354.26898, 0.0018005224, 0.0028155171] . Epoch 25/200: loss=181.006; Losses: [181.0021, 0.0017763268, 0.0022955306] . Epoch 26/200: loss=142.006; Losses: [142.00166, 0.0017520584, 0.0022001117] . Epoch 27/200: loss=158.932; Losses: [158.92723, 0.0017273662, 0.0026816982] . Epoch 28/200: loss=175.159; Losses: [175.15494, 0.0017028436, 0.0019087334] . Epoch 29/200: loss=167.915; Losses: [167.91084, 0.0016797789, 0.002798946] . Epoch 30/200: loss=152.785; Losses: [152.78006, 0.001658167, 0.0029456303] . Epoch 31/200: loss=158.407; Losses: [158.40344, 0.0016350556, 0.0018697139] . Epoch 32/200: loss=151.065; Losses: [151.06094, 0.0016126116, 0.00282249] . Epoch 33/200: loss=181.075; Losses: [181.07072, 0.0015892526, 0.0025259533] . Epoch 34/200: loss=157.210; Losses: [157.20605, 0.0015682159, 0.0026225548] . Epoch 35/200: loss=151.200; Losses: [151.19623, 0.0015464819, 0.0017979483] . Epoch 36/200: loss=157.111; Losses: [157.10796, 0.0015237778, 0.0016808716] . Epoch 37/200: loss=160.398; Losses: [160.39449, 0.0015015532, 0.0018031715] . Epoch 38/200: loss=133.418; Losses: [133.41472, 0.0014805306, 0.0017712188] . Epoch 39/200: loss=161.662; Losses: [161.65845, 0.0014592485, 0.0018676165] . Epoch 40/200: loss=181.789; Losses: [181.78532, 0.0014379938, 0.0023018767] . Epoch 41/200: loss=183.568; Losses: [183.56451, 0.00142015, 0.001671126] . Epoch 42/200: loss=211.679; Losses: [211.67618, 0.0014024411, 0.001817507] . Epoch 43/200: loss=235.384; Losses: [235.38095, 0.0013794828, 0.0018946148] . Epoch 44/200: loss=139.088; Losses: [139.08447, 0.0013571468, 0.0019089653] . Epoch 45/200: loss=160.254; Losses: [160.25092, 0.0013359843, 0.0013933791] . Epoch 46/200: loss=142.206; Losses: [142.20291, 0.0013167453, 0.0014015753] . Epoch 47/200: loss=138.814; Losses: [138.8115, 0.0012925405, 0.0014298331] . Epoch 48/200: loss=130.677; Losses: [130.67343, 0.0012679367, 0.0021604125] . Epoch 49/200: loss=145.296; Losses: [145.29306, 0.0012432866, 0.0013332999] . Epoch 50/200: loss=133.338; Losses: [133.33516, 0.0012180805, 0.0012685797] . Epoch 51/200: loss=138.212; Losses: [138.20908, 0.0011976506, 0.0017989981] . Epoch 52/200: loss=136.139; Losses: [136.13644, 0.0011768966, 0.001470595] . Epoch 53/200: loss=141.839; Losses: [141.83646, 0.0011539405, 0.0017275128] . Epoch 54/200: loss=159.297; Losses: [159.29402, 0.0011311076, 0.0017414299] . Epoch 55/200: loss=125.919; Losses: [125.91669, 0.0011091225, 0.0013509693] . Epoch 56/200: loss=135.710; Losses: [135.7079, 0.0010871431, 0.0010571101] . Epoch 57/200: loss=124.548; Losses: [124.545654, 0.0010660599, 0.0011921946] . Epoch 58/200: loss=128.607; Losses: [128.60472, 0.0010461143, 0.0010578154] . Epoch 59/200: loss=200.879; Losses: [200.87674, 0.0010245068, 0.0014237395] . Epoch 60/200: loss=181.939; Losses: [181.93698, 0.0010024481, 0.0010916217] . Epoch 61/200: loss=161.069; Losses: [161.06737, 0.0009815091, 0.0009930782] . Epoch 62/200: loss=129.661; Losses: [129.65881, 0.0009596758, 0.001090972] . Epoch 63/200: loss=342.733; Losses: [342.73068, 0.000939325, 0.001181667] . Epoch 64/200: loss=160.802; Losses: [160.8003, 0.00091621274, 0.0012079075] . Epoch 65/200: loss=123.200; Losses: [123.19836, 0.00089694076, 0.0009983401] . Epoch 66/200: loss=134.465; Losses: [134.46295, 0.00087896077, 0.0009002821] . Epoch 67/200: loss=205.839; Losses: [205.83714, 0.0008604548, 0.001061962] . Epoch 68/200: loss=144.191; Losses: [144.18906, 0.0008421927, 0.0011492949] . Epoch 69/200: loss=164.397; Losses: [164.39539, 0.0008238552, 0.0010998722] . Epoch 70/200: loss=131.024; Losses: [131.02272, 0.00080561615, 0.000867295] . Epoch 71/200: loss=130.408; Losses: [130.40652, 0.0007878286, 0.0009589862] . Epoch 72/200: loss=120.511; Losses: [120.509705, 0.0007217523, 0.0008722847] . Epoch 73/200: loss=122.388; Losses: [122.38655, 0.00069110864, 0.00080618635] . Epoch 74/200: loss=123.200; Losses: [123.198654, 0.0006731994, 0.0008244566] . Epoch 75/200: loss=117.884; Losses: [117.88217, 0.00065816526, 0.00086460914] . Epoch 76/200: loss=123.508; Losses: [123.50694, 0.0006448629, 0.0008477152] . Epoch 77/200: loss=121.749; Losses: [121.74744, 0.0006321136, 0.0008150753] . Epoch 78/200: loss=145.549; Losses: [145.5473, 0.00061951997, 0.00082959904] . Epoch 79/200: loss=135.341; Losses: [135.33992, 0.00060778105, 0.0007312283] . Epoch 80/200: loss=131.476; Losses: [131.47452, 0.0005965105, 0.0008391714] . Epoch 81/200: loss=123.978; Losses: [123.976944, 0.00058624754, 0.0008059401] . Epoch 82/200: loss=136.084; Losses: [136.08298, 0.0005766748, 0.0007252015] . Epoch 83/200: loss=137.815; Losses: [137.81375, 0.00056776253, 0.0009108439] . Epoch 84/200: loss=116.955; Losses: [116.95401, 0.0005592232, 0.0008040637] . Epoch 85/200: loss=131.525; Losses: [131.52376, 0.00055153086, 0.0007604256] . Epoch 86/200: loss=135.716; Losses: [135.71432, 0.00054452394, 0.0006871256] . Epoch 87/200: loss=191.940; Losses: [191.93927, 0.0005378095, 0.0006448448] . Epoch 88/200: loss=170.746; Losses: [170.74509, 0.00053181086, 0.0006591724] . Epoch 89/200: loss=121.373; Losses: [121.37161, 0.00052640436, 0.00079500565] . Epoch 90/200: loss=126.909; Losses: [126.90761, 0.0005213883, 0.0006021685] . Epoch 91/200: loss=122.121; Losses: [122.120255, 0.0005157513, 0.0006873938] . Epoch 92/200: loss=129.156; Losses: [129.15442, 0.0005111307, 0.00064897194] . Epoch 93/200: loss=113.183; Losses: [113.18219, 0.0005073488, 0.000602577] . Epoch 94/200: loss=146.389; Losses: [146.38794, 0.00050397916, 0.0005585851] . Epoch 95/200: loss=130.446; Losses: [130.44531, 0.0005009744, 0.00056175387] . Epoch 96/200: loss=118.884; Losses: [118.88327, 0.0004950377, 0.0005437781] . Epoch 97/200: loss=114.319; Losses: [114.3183, 0.0004938163, 0.0006134198] . Epoch 98/200: loss=134.747; Losses: [134.74594, 0.0004912615, 0.0005468172] . Epoch 99/200: loss=127.224; Losses: [127.22336, 0.0004886875, 0.00050384714] . Epoch 100/200: loss=123.007; Losses: [123.00618, 0.0004862357, 0.00056520273] . Epoch 101/200: loss=130.374; Losses: [130.3726, 0.0004841056, 0.0005738659] . Epoch 102/200: loss=120.120; Losses: [120.11898, 0.00048118114, 0.00054365897] . Epoch 103/200: loss=107.167; Losses: [107.16606, 0.00047940924, 0.0004886348] . Epoch 104/200: loss=112.270; Losses: [112.268585, 0.00047771176, 0.0004745159] . Epoch 105/200: loss=125.537; Losses: [125.53565, 0.0004758754, 0.00047247266] . Epoch 106/200: loss=109.324; Losses: [109.32308, 0.00047426036, 0.00046340926] . Epoch 107/200: loss=113.328; Losses: [113.327286, 0.0004729222, 0.00046490182] . Epoch 108/200: loss=117.106; Losses: [117.10452, 0.000471713, 0.00062898267] . Epoch 109/200: loss=122.371; Losses: [122.37039, 0.0004705812, 0.00051386596] . Epoch 110/200: loss=119.422; Losses: [119.42122, 0.0004696009, 0.0005573735] . Epoch 111/200: loss=131.784; Losses: [131.78348, 0.000468354, 0.00041559048] . Epoch 112/200: loss=124.476; Losses: [124.475006, 0.00046699354, 0.00041292777] . Epoch 113/200: loss=104.487; Losses: [104.486534, 0.00046530017, 0.00042556314] . Epoch 114/200: loss=119.418; Losses: [119.41684, 0.0004641684, 0.00039730535] . Epoch 115/200: loss=117.776; Losses: [117.77547, 0.00046339296, 0.00056288636] . Epoch 116/200: loss=112.189; Losses: [112.18817, 0.0004628609, 0.00045002214] . Epoch 117/200: loss=116.317; Losses: [116.31613, 0.0004616853, 0.00036934114] . Epoch 118/200: loss=159.105; Losses: [159.10422, 0.00046110825, 0.0003716088] . Epoch 119/200: loss=116.958; Losses: [116.95712, 0.00045984008, 0.0003543413] . Epoch 120/200: loss=108.100; Losses: [108.09944, 0.00045923653, 0.00045118056] . Epoch 121/200: loss=107.565; Losses: [107.56447, 0.00045848632, 0.00033903003] . Epoch 122/200: loss=117.631; Losses: [117.62992, 0.0004574218, 0.00033954927] . Epoch 123/200: loss=116.075; Losses: [116.07385, 0.0004564249, 0.00036835673] . Epoch 124/200: loss=106.798; Losses: [106.79701, 0.0004566427, 0.00032678706] . Epoch 125/200: loss=113.363; Losses: [113.36252, 0.0004562596, 0.00042438688] . Epoch 126/200: loss=118.104; Losses: [118.10279, 0.00045581322, 0.00032525152] . Epoch 127/200: loss=113.516; Losses: [113.51486, 0.00045547614, 0.0005056638] . Epoch 128/200: loss=117.624; Losses: [117.62353, 0.00045549794, 0.00034517745] . Epoch 129/200: loss=112.273; Losses: [112.272316, 0.00045538746, 0.00029629772] . Epoch 130/200: loss=113.328; Losses: [113.32768, 0.0004549961, 0.0003071945] . Epoch 131/200: loss=111.756; Losses: [111.75513, 0.00045427072, 0.00032724047] . Epoch 132/200: loss=107.796; Losses: [107.795494, 0.00045377907, 0.00027464304] . Epoch 133/200: loss=150.595; Losses: [150.59428, 0.00045307074, 0.0003010978] . Epoch 134/200: loss=120.134; Losses: [120.13356, 0.00045292114, 0.00029552256] . Epoch 135/200: loss=120.130; Losses: [120.12947, 0.00045320712, 0.0002585037] . Epoch 136/200: loss=117.070; Losses: [117.06926, 0.0004533619, 0.00026611943] . Epoch 137/200: loss=111.006; Losses: [111.00518, 0.00045333267, 0.0002824646] . Epoch 138/200: loss=115.901; Losses: [115.90064, 0.00045347284, 0.00030576388] . Epoch 139/200: loss=111.147; Losses: [111.146286, 0.000453111, 0.0002558468] . Epoch 140/200: loss=103.128; Losses: [103.12687, 0.0004522237, 0.00038727812] . Epoch 141/200: loss=115.025; Losses: [115.024475, 0.00045187408, 0.0002339398] . Epoch 142/200: loss=117.170; Losses: [117.16969, 0.0004520767, 0.00023050333] . Epoch 143/200: loss=105.315; Losses: [105.31448, 0.0004517807, 0.00026579303] . Epoch 144/200: loss=114.114; Losses: [114.113625, 0.00045176974, 0.0002442702] . Epoch 145/200: loss=108.179; Losses: [108.178154, 0.00045183185, 0.00022483827] . Epoch 146/200: loss=119.408; Losses: [119.407074, 0.0004509559, 0.00021656642] . Epoch 147/200: loss=116.504; Losses: [116.50336, 0.000450821, 0.00021879328] . Epoch 148/200: loss=108.465; Losses: [108.464005, 0.0004506137, 0.00031175395] . Epoch 149/200: loss=99.284; Losses: [99.28293, 0.00045024417, 0.00022024836] . Epoch 150/200: loss=105.143; Losses: [105.14198, 0.00044986696, 0.00024956765] . Epoch 151/200: loss=107.015; Losses: [107.01389, 0.0004503439, 0.00019209863] . Epoch 152/200: loss=109.961; Losses: [109.96058, 0.00045052002, 0.00036505712] . Epoch 153/200: loss=110.943; Losses: [110.94235, 0.00045058262, 0.00019362733] . Epoch 154/200: loss=105.146; Losses: [105.14586, 0.0004505078, 0.00018350873] . Epoch 155/200: loss=153.239; Losses: [153.23862, 0.00045094165, 0.00018688777] . Epoch 156/200: loss=97.193; Losses: [97.192276, 0.0004497521, 0.0002643012] . Epoch 157/200: loss=116.076; Losses: [116.075356, 0.00044927179, 0.00018770616] . Epoch 158/200: loss=99.644; Losses: [99.64349, 0.00044897772, 0.00019957994] . Epoch 159/200: loss=101.686; Losses: [101.68573, 0.00044913022, 0.0001649716] . Epoch 160/200: loss=114.998; Losses: [114.99737, 0.00044872603, 0.0001598363] . Epoch 161/200: loss=126.449; Losses: [126.44795, 0.00044798924, 0.00017636445] . Epoch 162/200: loss=99.323; Losses: [99.32204, 0.00044971833, 0.0001718228] . Epoch 163/200: loss=118.403; Losses: [118.402115, 0.0004499098, 0.00016231032] . Epoch 164/200: loss=101.217; Losses: [101.21654, 0.00044922353, 0.00015090306] . Epoch 165/200: loss=132.002; Losses: [132.0016, 0.0004491811, 0.00018679435] . Epoch 166/200: loss=103.262; Losses: [103.26103, 0.00044870118, 0.00014671378] . Epoch 167/200: loss=98.593; Losses: [98.592026, 0.0004482735, 0.00017167021] . Epoch 168/200: loss=102.641; Losses: [102.64062, 0.00044823167, 0.0001966377] . Epoch 169/200: loss=110.199; Losses: [110.19867, 0.00044768318, 0.00014072815] . Epoch 170/200: loss=98.456; Losses: [98.45533, 0.00044640992, 0.00013351238] . Epoch 171/200: loss=107.700; Losses: [107.698944, 0.00044608887, 0.000128763] . Epoch 172/200: loss=110.314; Losses: [110.31317, 0.0004454155, 0.00015472547] . Epoch 173/200: loss=101.824; Losses: [101.82384, 0.00044520054, 0.00012376827] . Epoch 174/200: loss=100.615; Losses: [100.61448, 0.00044518447, 0.00012106997] . Epoch 175/200: loss=99.010; Losses: [99.00989, 0.00044499052, 0.0001265088] . Epoch 176/200: loss=104.999; Losses: [104.99841, 0.0004448745, 0.00017745573] . Epoch 177/200: loss=98.012; Losses: [98.0118, 0.0004448767, 0.00016406355] . Epoch 178/200: loss=99.610; Losses: [99.60982, 0.0004446858, 0.00011498472] . Epoch 179/200: loss=108.899; Losses: [108.89821, 0.00044491427, 0.00010847509] . Epoch 180/200: loss=118.136; Losses: [118.13521, 0.00044519745, 0.00010489453] . Epoch 181/200: loss=98.810; Losses: [98.80894, 0.000445671, 0.00012609128] . Epoch 182/200: loss=96.406; Losses: [96.40544, 0.0004462903, 0.00010188887] . Epoch 183/200: loss=98.501; Losses: [98.50068, 0.00044781342, 9.952572e-05] . Epoch 184/200: loss=97.149; Losses: [97.14829, 0.00044934792, 0.00010874969] . Epoch 185/200: loss=100.637; Losses: [100.63678, 0.00044996737, 9.560937e-05] . Epoch 186/200: loss=97.202; Losses: [97.201294, 0.00045056257, 9.1939364e-05] . Epoch 187/200: loss=99.839; Losses: [99.83876, 0.0004511009, 8.99044e-05] . Epoch 188/200: loss=118.457; Losses: [118.45691, 0.00045174357, 9.007633e-05] . Epoch 189/200: loss=102.419; Losses: [102.41877, 0.00045339097, 9.094182e-05] . Epoch 190/200: loss=110.813; Losses: [110.812386, 0.00045527803, 8.5699685e-05] . Epoch 191/200: loss=99.384; Losses: [99.38332, 0.000456504, 8.375079e-05] . Epoch 192/200: loss=103.581; Losses: [103.58075, 0.0004564843, 9.893996e-05] . Epoch 193/200: loss=97.621; Losses: [97.62078, 0.00045904273, 8.179152e-05] . Epoch 194/200: loss=94.909; Losses: [94.90842, 0.00046064917, 7.898855e-05] . Epoch 195/200: loss=100.840; Losses: [100.83898, 0.0004623049, 8.4420986e-05] . Epoch 196/200: loss=98.157; Losses: [98.15616, 0.00046370056, 8.1935854e-05] . Epoch 197/200: loss=95.283; Losses: [95.28199, 0.00046543585, 7.4171934e-05] . Epoch 198/200: loss=98.005; Losses: [98.00419, 0.0004669357, 7.385877e-05] . Epoch 199/200: loss=103.609; Losses: [103.608406, 0.00046879202, 7.1431816e-05] _ , dyn_prior = f_model . dynamic_encoder . sample_dynamic_prior ( 10 ) np . squeeze ( dyn_prior . mean () . numpy ()) array([-1.6796231, -2.5568666, -2.4644861, -2.4013069, -2.3772488, -2.3707619, -2.377105 , -2.332031 , -2.3327737, -2.360692 ], dtype=float32) class RNNMultivariateNormalDiag ( tfd . MultivariateNormalDiag ): def __init__ ( self , cell , n_timesteps = 1 , output_dim = None , name = \"rnn_mvn_diag\" , ** kwargs ): self . cell = cell if output_dim is not None and hasattr ( self . cell , 'output_dim' ): self . cell . output_dim = output_dim if hasattr ( self . cell , 'output_dim' ): output_dim = self . cell . output_dim else : output_dim = output_dim or self . cell . units h0 = tf . zeros ([ 1 , self . cell . units ]) c0 = tf . zeros ([ 1 , self . cell . units ]) input0 = tf . zeros (( 1 , output_dim )) if hasattr ( cell , 'reset_dropout_mask' ): self . cell . reset_dropout_mask () self . cell . reset_recurrent_dropout_mask () input_ = input0 states_ = ( h0 , c0 ) successive_outputs = [] for i in range ( n_timesteps ): input_ , states_ = self . cell ( input_ , states_ ) successive_outputs . append ( input_ ) loc = tf . concat ([ _ . parameters [ \"distribution\" ] . parameters [ \"loc\" ] for _ in successive_outputs ], axis = 0 ) scale_diag = tf . concat ([ _ . parameters [ \"distribution\" ] . parameters [ \"scale_diag\" ] for _ in successive_outputs ], axis = 0 ) super ( RNNMultivariateNormalDiag , self ) . __init__ ( loc = loc , scale_diag = scale_diag , name = name , ** kwargs ) K . clear_session () dynamic_prior = RNNMultivariateNormalDiag ( VariationalLSTMCell ( N_HIDDEN , output_dim = LATENT_SIZE_DYNAMIC ), n_timesteps = N_TIMES , output_dim = LATENT_SIZE_DYNAMIC ) sample = dynamic_prior . sample (( N_SAMPLES , BATCH_SIZE )) print ( sample . shape ) print ( dynamic_prior . mean ()) (2, 6, 10, 1) tf.Tensor( [[ 0. ] [ 0.04328619] [ 0.08498121] [-0.17377347] [-0.09743058] [-0.30255282] [-0.22110605] [-0.36379734] [-0.30933833] [-0.13590682]], shape=(10, 1), dtype=float32)","title":"Latent Dynamic Factor"}]}